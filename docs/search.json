[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Bioinformatics for Malaria Molecular Surveillance",
    "section": "",
    "text": "About\nWelcome to the Bioinformatics Module of the Fighting Malaria Across Borders (FiMAB) International Training Programme, created by the Institute of Tropical Medicine Antwerp (ITM) in collaboration with the University of Antwerp, and supported by VLIR-UOS. Its primary goal is to support the implementation of targeted sequencing assays (in particular, AmpliSeq) to strengthen malaria molecular surveillance and help guide national control programmes. In conjunction with laboratory training, this bioinformatics course is intended to allow young academics around the globe to become familiar with molecular surveillance as a key activity to monitor transmission, sources of epidemics and the emergence and spread of drug resistance mutations in the Plasmodium parasite through sequencing approaches.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#scope-of-the-course",
    "href": "index.html#scope-of-the-course",
    "title": "Introduction to Bioinformatics for Malaria Molecular Surveillance",
    "section": "Scope of the course",
    "text": "Scope of the course\nThis course aims to provide an overview of the key bioinformatics concepts related to performing molecular surveillance in Plasmodium. It is divided into the following sections:\n\nIntroduction to the Unix shell\nIntroduction to R\nOverview of genomics pipeline\nPopulation genetics and molecular surveillance in Plasmodium (separate materials on ITM course page)\n\nSection 1-2 are online self-paced modules, whereas the others will include classroom lectures and practical sessions. Evaluation exercises will be conducted on the ITM course page.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#computational-thinking",
    "href": "index.html#computational-thinking",
    "title": "Introduction to Bioinformatics for Malaria Molecular Surveillance",
    "section": "Computational thinking",
    "text": "Computational thinking\nWe hope that this course can teach you a few computational thinking and problem solving skills that will help you along your bioinformatics journey. The learning curve in computational biology can be quite steep at times and the path is littered with arcane commands and obtuse syntax, but as you practice the concepts introduced in this course on your own, your command-line efficiency will improve and you will start to spot similarities across different types of environments and languages. Through this course, we hope to arm you with the necessary skills to make tasks like running custom analysis scripts or installing bioinformatics software seem a little less daunting, and enough knowledge and experience to be able to explore more advanced topics on your own.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "content/unix/1-unix-intro.html",
    "href": "content/unix/1-unix-intro.html",
    "title": "1  What is a CLI?",
    "section": "",
    "text": "1.1 Learning objectives\nThis section of the course will introduce you to the general concept of the Unix command line interface (CLI) - as opposed to the graphical user interface (GUI) that you are familiar with - and Bash, one of the most ubiquitous Unix shells.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a CLI?</span>"
    ]
  },
  {
    "objectID": "content/unix/1-unix-intro.html#learning-objectives",
    "href": "content/unix/1-unix-intro.html#learning-objectives",
    "title": "1  What is a CLI?",
    "section": "",
    "text": "Knowledge of what a Unix shell and the CLI are and why/when they can be useful.\nSetting up your own Unix environment.\nFamiliarity with basic bash commands for e.g., navigation, moving/copying and creating/deleting/modifying files and directories.\nIntroduction of a few more advanced commands and concepts like redirection, piping and loops.\nFirst look at scripts and how they can be used in the context of DNA sequencing pipelines for variant calling.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a CLI?</span>"
    ]
  },
  {
    "objectID": "content/unix/1-unix-intro.html#resources",
    "href": "content/unix/1-unix-intro.html#resources",
    "title": "1  What is a CLI?",
    "section": "Resources",
    "text": "Resources\nThis section of the course draws inspiration from the following resources:\n\nConor Meehan’s UNIX shell tutorial (CC BY-NC-SA 4.0)\nMike Lee’s Unix Crash Course (https://doi.org/10.21105/jose.00053)\nData Carpentry’s Introduction to the Command Line for Genomics (https://doi.org/10.5281/zenodo.3260560 CC-BY 4.0)\nRonan Harrington’s Bioinformatics Notebook (MIT)\nA Primer for Computational Biology by Shawn T. O’Neil (CC BY-NC-SA)\nEric C. Anderson’s Bioinformatics Handbook (Chapter 4) (MIT)\nCourse on UNIX and Genomic Data\nECA’s Bioinformatics Handbook\nRonor Harrington’s Bioinformatics Notebook\nDuke HTS Summ Course 2018",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a CLI?</span>"
    ]
  },
  {
    "objectID": "content/unix/1-unix-intro.html#what-is-unix",
    "href": "content/unix/1-unix-intro.html#what-is-unix",
    "title": "1  What is a CLI?",
    "section": "1.2 What is Unix?",
    "text": "1.2 What is Unix?\nUnix is a family of operating systems, with one of their defining features being the Unix shell, which is both a command line interface and scripting language.\nIn simpler terms, shells look like what you see in the figure below and they are used to talk to computers using a CLI - i.e., through written text commands - instead of via a graphical user interface (GUI) where you primarily use a mouse cursor.\n\n\n\nBash shell in WSL\n\n\nThere exist many different flavours of Unix, collectively termed “Unix-like”, but the ones you will most likely encounter yourself are Linux (which itself comes in many different varieties we call distributions, e.g. Debian, Ubuntu, Fedora, Arch, etc.) and MacOS. These operating systems come with a built-in Unix shell. While Windows also comes with a command line interface (Command Prompt and PowerShell), it is not a Unix shell and thus uses different syntax and commands. We’ll dig into how you can get your hands on a Unix shell on a Windows machine in a later section. The most ubiquitous Unix shell is Bash, which comes as the default on most Linux distributions.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a CLI?</span>"
    ]
  },
  {
    "objectID": "content/unix/1-unix-intro.html#why-bother-learning-the-unix-shell-as-a-bioinformatician",
    "href": "content/unix/1-unix-intro.html#why-bother-learning-the-unix-shell-as-a-bioinformatician",
    "title": "1  What is a CLI?",
    "section": "1.3 Why bother learning the Unix shell as a bioinformatician?",
    "text": "1.3 Why bother learning the Unix shell as a bioinformatician?\nEven if you are primarily a wet lab scientist, learning the basics of working with CLIs offers a number of advantages:\n\nAutomation: CLIs and scripting excel at performing repetitive tasks, saving not only time, but also lowering the risk of mistakes. Have you ever tried manually renaming hundreds of files? Or adding an extra column to an Excel spreadsheet with millions of rows?\nReproducibility: reproducibility is key in science and by using scripts (and other tools like git, package managers and workflow systems) you can ensure that your analyses can be repeated more readily. This is in stark contrast to the point-and-click nature of GUIs.\nBuilt-in tools: the Unix shell offers a plethora of tools for manipulating and inspecting large (text) files, which we often deal with in bioinformatics. E.g., DNA sequences are often stored as plain text files.\nAvailability of software: many bioinformatics tools are exclusively built for Unix-like environments.\nAccess to remote servers: Unix shells (usually bash) are the native language of most remote servers, High Performance Computing (HPC) clusters and cloud compute systems.\nProgrammatic access: CLIs and scripts allows you to interact in various ways (e.g., via APIs) with data that is stored in large on-line databases, like those hosted by NCBI or EBI.\n\nAs a concrete example of what we will be using the shell for, consider the task of processing hundreds of Plasmodium DNA sequencing reads with the goal of determining the genetic variation in these samples (e.g., the presence of SNPs). Suppose we were to do this in a GUI program, where we would open each individual sample and subject it to a number of analyses steps. Even if each step were to only require a few seconds (in reality, minutes or even hours…), this would take quite a long time and be prone to errors (and quite boring!). With shell scripting, we can automate these repetitive steps and run the analysis without requiring human input at every step. Some of the key techniques we will use for this are:\n\nNavigating to directories and moving files around.\nLooping over a set of files, calling a piece of software on each of them.\nExtracting information from a particular location in a text file.\nCompressing and extracting files.\nChaining commands: passing the output of one tool to another one. E.g., after aligning reads to a reference genome, the resulting output can be fed to the next step of the pipeline, the variant caller.\nEtc.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a CLI?</span>"
    ]
  },
  {
    "objectID": "content/unix/1-unix-intro.html#dont-get-discouraged",
    "href": "content/unix/1-unix-intro.html#dont-get-discouraged",
    "title": "1  What is a CLI?",
    "section": "1.4 Don’t get discouraged",
    "text": "1.4 Don’t get discouraged\nLearning to use the shell, or learning programming languages and bioinformatics skills in general, can be daunting if you have had little experience with these types of tasks in the past. Don’t worry though, just take your time and things will become easier over time as you gain more experience.\nWe do not expect you to be able to memorize every single command and all of its option. Instead, it is more important to be aware of the existence of commands to perform particular tasks, and to be able to independently retrieve information on how to use them when the need arises.\nFinally, the appendix (Section A.1) of the course also contains a bunch of tips and tricks to keep in mind while learning your way around the shell.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a CLI?</span>"
    ]
  },
  {
    "objectID": "content/unix/1-unix-intro.html#a-note-on-terminology",
    "href": "content/unix/1-unix-intro.html#a-note-on-terminology",
    "title": "1  What is a CLI?",
    "section": "1.5 A note on terminology",
    "text": "1.5 A note on terminology\nYou will often see the terms command line (interface), terminal, shell, bash, unix (or unix-like) being thrown around more or less interchangeably (including in this course). Most of the time, it is not terribly important to know all the minute differences between them, but you can find an overview here if you are curious: https://astrobiomike.github.io/unix/unix-intro.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a CLI?</span>"
    ]
  },
  {
    "objectID": "content/unix/2-unix-setup.html",
    "href": "content/unix/2-unix-setup.html",
    "title": "2  Setting up your own Unix shell",
    "section": "",
    "text": "2.1 Online Unix environment\nIn order to get started, you need to get access to a Unix environment of your own. You can either work locally on your own computer or you can use the online environment that we have created. The latter comes with a bash shell and will immediately give you access to a bunch of files that we will use throughout the course and exercises.\nThere are also online playgrounds/simulations available to try out the Unix shell, for example https://sandbox.bio/playground. These are great to learn and we highly recommend checking out some of the tutorials on there, but the downside of course is that they are not true true environments and you cannot interact with your own files. You can use this site to follow along while learning some of the basic unix commands, but you will need to switch to a different option for the exercises eventually.\nWe have provided two different options for getting access to an online unix environment: through Binder (free, but less powerful) or through GitHub Codespaces (free for 60 hours per month).\nBoth options will launch an environment containing all relevant training files, based on this GitHub repository.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting up your own Unix shell</span>"
    ]
  },
  {
    "objectID": "content/unix/2-unix-setup.html#online-unix-environment",
    "href": "content/unix/2-unix-setup.html#online-unix-environment",
    "title": "2  Setting up your own Unix shell",
    "section": "",
    "text": "2.1.1 GitHub Codespaces\nTo access Codespaces, you will first need to create a GitHub account via https://github.com/signup. Just follow the instructions and be sure to enable one of the two-factor authentication options (via a TOTP app like Authy, Google Authenticator or Microsoft Authenticator, or via text messages), otherwise you might not receive access to Codespaces.\nAfterwards, you can click this link, optionally change the region to the one closest to you under change options (but leave the machine type on 2-core to remain eligble for 60 free hours!), and then press the Create codespace button.\n\n\n\nCreating a new codespace\n\n\n\n\n\nLaunching a new codespace\n\n\nSetting up the codespace can take a while, but eventually you will be greeted by a VSCode environment. The terminal (a bash shell) is accessible at the bottom (or by pressing the hamburger icon in the top left and selecting “new terminal”). This is where you will be able to run the Unix commands introduced in the next chapters.\n\n\n\nTerminal inside VSCode editor in GitHub Codespace\n\n\nYou will only receive 60 hours of free usage of Codespaces per month. This means you should manually shutdown your codespace whenever you are done with it. Otherwise it will keep running for 30 more minutes (by default). Just closing your browser will not shut down the workspace. Instead, you need to manually shut it down from within the codespace (by clicking the &gt;&lt; button in the bottom left corner and selecting stop current codespace) or by browsing to https://github.com/codespaces and shutting it down from that page.\n\n\n\n\n\n\nWhat are GitHub and git?\n\n\n\nGitHub is a place to host code and software via a tool named git, which is a version control system. It allows you to keep track of the history of your code, easily revert changes and allows for collaborating with multiple people on the same project. We will not go into further detail on using version control, but for now just remember that it can play an important role in scientific reproducibility.\nIf you want to learn more about git already, you can have a look at the following resources:\n\nhttps://happygitwithr.com/\nhttps://hwheeler01.github.io/CompBio/github/\nhttps://pmoris.github.io/git-workshop/ (self-promotion)\n\n\n\n\n\n\n\n\n\nWhat is GitHub CodeSpaces\n\n\n\nSimilar to Binder, Codespaces are development environments that are hosted in the cloud. This is a paid service provided by GitHub/Microsoft, which offers 60 hours of free usage per individual per month. Instead of Jupyter notebooks, Codespaces use code editors, like VSCode and Jetbrains IDEs, which come bundled with a bash terminal too.\nYou can find more info in the GitHub Codespaces docs.\n\n\n\n\n2.1.2 Binder\nAs an alternative to CodeSpaces, we have also created a Binder environment. It will operate similar to CodeSpaces and provide you with an online environment containing all of the required files as well as a Unix shell (bash).\n\n\n\n\n\n\nWhat is Binder?\n\n\n\nBinder is a service that allows people to share a customized compute environment based on a Git repository. It is mainly aimed at sharing Jupyter Notebooks (Python), but it also supports RStudio, Shiny and fortunately for us, a plain bash terminal too.\nYou can find more info on the Binder website.\n\n\nTo access the remote bash shell on Binder, browse to https://mybinder.org/v2/gh/pmoris/FiMAB-bioinformatics/HEAD and wait for the launcher to start. This process can take quite a while, so be patient.\n\n\n\nLaunching the Binder environment\n\n\nEventually, you should be greeted by a screen (Jupyter Lab) with a number of launchers. Simply select the one labelled “Terminal” (a black square with a white $) and you should be all set.\n\n\n\nStarting a new Bash shell",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting up your own Unix shell</span>"
    ]
  },
  {
    "objectID": "content/unix/2-unix-setup.html#local-unix-environment",
    "href": "content/unix/2-unix-setup.html#local-unix-environment",
    "title": "2  Setting up your own Unix shell",
    "section": "2.2 Local Unix environment",
    "text": "2.2 Local Unix environment\nIf you are using MacOS or Linux, then you will already have access to a Unix shell (either bash or zsh, which will mostly behave identical for our purposes). To access it, simply search for a program called Terminal (or search for anything resembling “command”, “prompt” or “shell”).\nIn case you are using a Windows machine, things are slightly more complex and different methods exist, each with their own pros and cons. You could use a fully-fledged virtual machine like VirtualBox to emulate a Linux machine within Windows. Or you could rely on the minimal bash emulator that comes bundled with git for windows. However, nowadays we recommend that you use the Windows Subsystem for Linux (WSL), which was developed by Microsoft itself. In our opinion, it is one of the most polished methods to get access to a (nearly) full-featured Linux environment from within Windows, without the overhead of a full virtual machine or dual boot setup (dual boot means you install two different operating systems on your machine, and you switch between them when booting). For instructions on how to set it up, you can refer to this section.\n\n2.2.1 Download the course files\nRegardless of what type of local Unix environment you use, you will need to download the files that we will be using in our examples and exercises. You can do this directly on the command line or by manually downloading the files in the correct location.\n\nOpen your terminal and cd to a location where you want to place the training files.\nEnter the command git clone https://github.com/pmoris/FiMAB-bioinformatics.git.\nAfterwards, a new directory named FiMAB-bioinformatics will have been created.\n\nIt should look similar to this:\n$ git clone https://github.com/pmoris/FiMAB-bioinformatics.git\nCloning into 'FiMAB-bioinformatics'...\nremote: Enumerating objects: 7, done.\nremote: Counting objects: 100% (7/7), done.\nremote: Compressing objects: 100% (7/7), done.\nremote: Total 7 (delta 0), reused 7 (delta 0), pack-reused 0\nReceiving objects: 100% (7/7), done.\nAlternatively,\n\nBrowse to https://codeload.github.com/pmoris/FiMAB-bioinformatics/zip/refs/heads/main\nSave the .zip file in a directory accessible by your Unix environment. For Windows/WSL, the easiest option is to choose the Linux file system (e.g., \\\\wsl.localhost\\Ubuntu\\home\\pmoris), which is accessible by clicking the Linux/WSL entry in your explorer.\nExtract/unzip the file.\n\n\n\n\nLinux file system inside Windows File Explorer\n\n\n\n\n2.2.2 WSL installation\nIf you are using an updated version of Windows 10 (or 11), you should meet all the requirements and can simply follow the installation instructions listed here: https://learn.microsoft.com/en-us/windows/wsl/install. We recommend that you follow the instructions for WSL 2 (default), rather than the older WSL 1, and use the default Ubuntu 22.04 distribution (Linux comes in many different flavours, Ubuntu being one of the more popular ones).\nBriefly:\n\nOpen Windows PowerShell as administrator by right clicking your Windows Start Menu or searching for it in your list of applications.\nType wsl --install and press enter.\nAfterwards, restart your PC.\nYou can then launch WSL by searching for wsl or Ubuntu in your start menu.\nThe first time you launch WSL, you will need to configure it.\n\nIf you use software like RStudio or VSCode, you can tell these programs to use WSL as their built-in terminal from now on, instead of Command Prompt.\n\n\n\n\n\n\nWSL1 vs WSL2\n\n\n\nWSL 2 is the newer version of WSL 1. For most tasks, WSL 2 tends to be much faster, hence why we (and Microsoft) recommend using it in favour of the previous version. However, WSL 2 is only faster when you interact with files that are stored directly on the WSL file system, rather than working directly on the Windows file system. More info on the distinction between these file systems can be found further below and in Microsoft’s WSL documentation.\nYou can switch between WSL1 and WSL2 on the fly by just calling wsl --set-version &lt;distro_name&gt; 2 (or 1) in PowerShell, so feel free to experiment for yourself.\nFor a full overview of the differences, check out: https://docs.microsoft.com/en-us/windows/wsl/compare-versions.\n\n\n\n2.2.2.1 Configuring WSL\nMicrosoft also provides an excellent tutorial on setting up your WLS environment, which you can find here.\n\nAfter installing WSL and a Linux distribution, you will have access to it via its own built-in terminal emulator. It should be located in your Windows Start Menu with a name corresponding to the distribution that you installed, e.g. Ubuntu 20.04 LTS, or simply wsl.\nThe first time you run WSL, you will need to setup a Linux username and password. Note that while you are entering a password, nothing will appear on the screen, but this is intended (blind typing). The username will determine, among other things, the name of your home folder, whereas the password will grant you administrator rights (referred to as super users or admins in Linux land; the sudo command is used to invoke these rights, see Section A.3).\nYou will also need to upgrade the packages by running the following command: sudo apt update && sudo apt upgrade, followed by your password.\nFor more information, check the docs.\n\n\n2.2.2.2 Accessing files across the Windows and WSL file systems\n\n\n\n\n\n\nNote\n\n\n\nSome of the information below might be a bit confusing at this point, but things should become more clear after working your way through the Unix section of this course.\n\n\nNewer versions of WSL will automatically add a shortcut to the WSL file system in your Windows File Explorer (look for Tux, Linux’ penguin mascot). The file path will look similar to \\\\wsl$\\Ubuntu\\home\\&lt;user name&gt;\\Project, indicating that Windows treats the WSL file system as a sort of network drive. You can also open a file location in Windows File Explorer from within a WSL terminal (e.g. after you browse to a particular directory cd ~/my-project) by simply using the command explorer.exe . (don’t forget the dot!).\nVice versa, you can also access the Windows file system from within WSL because it is mounted under /mnt/c. So, you could for example do something like cp /mnt/c/Users/&lt;user name&gt;/Downloads/file-downloaded-via-webbrowser ~/projects/filename.\nMore information can be found in the WSL documentation.\n\n\n2.2.2.3 Windows Terminal\nEven though WSL comes with its own terminal application, it is rather bare-bones and can make some operations like copying and pasting via CTRL+C/CTRL+V a bit tricky (you will need to use CTRL+SHIFT+C to copy and right mouse click to paste). Fortunately, Microsoft has also been working on a new terminal emulator that is much nicer to work with. Meet the Windows Terminal.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting up your own Unix shell</span>"
    ]
  },
  {
    "objectID": "content/unix/3-unix-enter-the-shell.html",
    "href": "content/unix/3-unix-enter-the-shell.html",
    "title": "3  Using the shell",
    "section": "",
    "text": "3.1 Interacting with the shell\nWhen you launch your (Bash) shell, you will be greeted by what is called a shell prompt: a short snippet of text followed by a cursor, which indicates that the shell is waiting for input. The prompt can look different on different systems, but it often consists of your linux username followed by the name of your machine (like in the picture below) or sometimes just a single $ symbol. When you see the prompt, you can enter commands interactively and execute them by pressing enter.\nAlready note that you cannot use your mouse cursor to move around your terminal. You will need to use your arrow keys (or shortcuts) to move around while typing commands.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using the shell</span>"
    ]
  },
  {
    "objectID": "content/unix/3-unix-enter-the-shell.html#interacting-with-the-shell",
    "href": "content/unix/3-unix-enter-the-shell.html#interacting-with-the-shell",
    "title": "3  Using the shell",
    "section": "",
    "text": "A bash shell prompt waiting for user input",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using the shell</span>"
    ]
  },
  {
    "objectID": "content/unix/3-unix-enter-the-shell.html#command-syntax",
    "href": "content/unix/3-unix-enter-the-shell.html#command-syntax",
    "title": "3  Using the shell",
    "section": "3.2 Command syntax",
    "text": "3.2 Command syntax\nUnix commands generally follow the format:\ncommand [OPTIONS] argument\nwhere,\n\ncommand is the name of the (usually built-in) command that you want to execute.\n[OPTIONS] is a list of optional flags to modify the behaviour of the command. They are often preceded by a single (-) or double (--) dash.\nargument is a thing that your command can use. E.g., it can be a file name, a short piece of text (or string)\n\nTry it yourself with the following command:\necho “Hello world!”\n\n\n\n\n\n\nWhat did that do? (Click me to expand!)\n\n\n\n\n\necho is a command that simply prints a message to your screen (technically, to the standard output stream (stdout) of the terminal). echo is the command, teling the shell what we want to do. \"Hello world!\" is the target, in this case the message we want to print.\nWe place the message between quotes (\") because it contains spaces, and as you will see, spaces (and certain other special characters) can cause confusions. For now, just note that the message that gets printed, is whatever was written between the quotes, but not the quotes themselves.\n$ echo \"Hello world!\"\nHello world!\n$",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using the shell</span>"
    ]
  },
  {
    "objectID": "content/unix/3-unix-enter-the-shell.html#tips-and-hints",
    "href": "content/unix/3-unix-enter-the-shell.html#tips-and-hints",
    "title": "3  Using the shell",
    "section": "3.3 Tips and hints",
    "text": "3.3 Tips and hints\nWe have compiled a number of helpful tips in the appendix of this course (Section A.1), some of which will hopefully be helpful on your journey towards mastering the unix shell. For now, we recommend at the very least checking out the section on tab-completion and your command history. In fact, you can give it a try already. Just press the up arrow and see if you can recall your previous command! Next, try to type out ec, and press &lt;tab&gt;, to see auto-complete in action.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using the shell</span>"
    ]
  },
  {
    "objectID": "content/unix/4-unix-navigation.html",
    "href": "content/unix/4-unix-navigation.html",
    "title": "4  Navigating the Unix file system",
    "section": "",
    "text": "4.1 Layout of the Unix file system\nAll files and directories (or folders) in Unix are stored in a hierarchical tree-like structure, similar to what you might be used to on Windows or Mac (cf. File Explorer). The base or foundation of the directory layout in Unix is the root (/) (like the root of a tree). All other files and directories are built on top of this root location. When navigating the file system, it is also important to be aware of your current location. This is called the working directory.\nThe address of a particular file or directory is provided by its filepath: this is a sequence of location names separated by a forward slash (/), like /home/user1. Note that this differs from the convention in Windows, where backslashes (\\) are used in file paths instead.\nThere are two types of file paths: absolute and relative paths.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Navigating the Unix file system</span>"
    ]
  },
  {
    "objectID": "content/unix/4-unix-navigation.html#sec-linux-paths",
    "href": "content/unix/4-unix-navigation.html#sec-linux-paths",
    "title": "4  Navigating the Unix file system",
    "section": "",
    "text": "Absolute file path: this is the exact location of a file and is always built up from the root location. E.g., /home/user1/projects/document.txt.\nRelative file path: this is the relative address of a file compared to some other path. E.g., from the perspective of /home/user1, the file document.txt is located in projects/document.txt.\n\n\n\n\nOverview of the Unix file system or directory layout\n\n\n\n4.1.1 Home sweet home: ~\nAnother important location is the home directory. In general, every user has their own home directory, found in /home/username. A frequently used shortcut for this is the tilde symbol (~). Depending on the current user, this will refer to a particular directory under /home/..\n\n\n\n\n\n\nHow can user1 write the file path to document.txt using the ~ shortcut?\n\n\n\n\n\n~/projects/document.txt\n\n\n\n\n\n4.1.2 Where am I? . shortcuts\nThe dot (.) also has an important function in file paths:\n\n. represents the directory you are currently in, i.e. the working directory.\n\nE.g., while inside the projects directory, any files inside can be accessed using either filename or ./filename.\n\n.. represents the parent directory of the working directory.\n\nE.g., from /home/user1/Desktop, the relative path to file document.txt can be written as ../projects/document.txt.\nThese expressions can be nested; while inside the projects directory, ../../user2 can be used to access the user2 home directory.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Navigating the Unix file system</span>"
    ]
  },
  {
    "objectID": "content/unix/4-unix-navigation.html#moving-around-the-file-system",
    "href": "content/unix/4-unix-navigation.html#moving-around-the-file-system",
    "title": "4  Navigating the Unix file system",
    "section": "4.2 Moving around the file system",
    "text": "4.2 Moving around the file system\nIn this section we will introduce a few essential commands that allow you to navigate the file system: pwd, cd and ls.\n\n4.2.1 pwd: avoid getting lost\npwd stands for print working directory and it does exactly that: it allows you to figure out where you are in the file system. For example, in the figure above, user1 would generally find themselves in their home directory upon login:\n$ pwd\n/home/user1\n\n\n4.2.2 cd: on the move\nNext, there is the cd command. This is used to move between directories (the name derives from change directory). Simply follow the command name by a file path to navigate there: cd &lt;filepath&gt;. To move from user1’s home directory to the projects directory:\ncd projects\nNote that you can use the special symbols we saw earlier as navigational shortcuts:\n\n\n\n\n\n\n\nCommand\nResult\n\n\n\n\ncd ~\nChange to home directory (/home/username)\n\n\ncd ..\nChange to parent directory (e.g., go up 1 directory)\n\n\ncd /\nChange to the root location\n\n\n\n\n\n4.2.3 ls: show me what you got\nFinally, we have the ls command. Its name stands for listing and it will list the names of the files and directories in the current working directory. The basic structure of the command ls [OPTIONS] &lt;target&gt;, with &lt;target&gt; being an optional path to a directory.\nTo continue upon our previous example, from inside /home/user1/projects we would see:\n$ ls\nDRX333466_1.fastq.gz    DRX333466_2.fastq.gz    document.txt\nNote that we did not specify a path, in which case ls will just list the contents of the current working directory. If we do specify a path, we will of course be shown the contents of that particular location:\n$ ls /home\nuser1   user2\nBy default, the files and directories are listed in alphabetically order and depending on your terminal settings, files and directories might even be colour-coded differently.\nls also comes with a few handy optional flags to modify its behaviour:\n\n\n\n\n\n\n\nCommand\nResult\n\n\n\n\nls -l\nShow detailed list view\n\n\nls -hl\nShow detailed list view and print file sizes in a human readable format\n\n\nls -a\nList all files and directories, including hidden ones\n\n\nls -lha\nCombine all options into one command\n\n\nls --help\nShow more information on the ls command and its options\n\n\nls\n\n\n\n\n\n\n\n\n\n\nWhat are hidden files?\n\n\n\nEarlier, we mentioned that . is used to refer to the current working directory, but it actually has a second function as well. Any file or directory name that starts with a dot (like /home/user1/.ssh) will be hidden and not displayed by default when using ls, hence the need for the -a flag.\nLinux often hides system or configuration files to avoid cluttering up your (home) directory. We will not deal with hidden files directly in this course, but one of the situations where you might encounter them are when modifying your .bashrc file (e.g., when creating custom functions, aliases or tweaking your PATH Section A.4) or when managing SSH keys for remote server access Section A.6).\n\n\nThe ls -l command is particularly useful, because it shows all types of additional information.\n\n\n\n\n\n\nWhat do the different columns in the output of ls -l represent?\n\n\n\n\n\n$ ls -l\ntotal 83764\n-rw-r--r-- 1 pmoris pmoris 14367565 Dec  7 09:39 3B207-2_S92_L001_R1_001.fastq.gz\n-rw-r--r-- 1 pmoris pmoris 16622378 Dec  7 09:39 3B207-2_S92_L001_R2_001.fastq.gz\n-rw-r--r-- 1 pmoris pmoris 13592342 Dec  7 09:39 MRA1242_S28_L001_R1_001.fastq.gz\n-rw-r--r-- 1 pmoris pmoris 15821981 Dec  7 09:39 MRA1242_S28_L001_R2_001.fastq.gz\n-rw-r--r-- 1 pmoris pmoris 12131772 Dec  7 09:39 NK6_S57_L001_R1_001.fastq.gz\n-rw-r--r-- 1 pmoris pmoris 13226198 Dec  7 09:39 NK6_S57_L001_R2_001.fastq.gz\nThe first column represents the permissions of the files/folders. In a nutshell, these determine things like who can read or write (= modify, including deletion) particular files. There is a column for the owner, a group of users and everyone else. There is more info in the appendix (Section A.5). The next column showing a 1 for each entry, you can ignore for now (they represent hard links, a concept we will not dive into). The two names in the following columns are the user and the group owner of the file. Next is the size of the file in bytes. If we had used the -h flag, the size would have been shown in KB, MB or GB instead. Next we have the time of the last modification and finally the name of the file/directory.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Navigating the Unix file system</span>"
    ]
  },
  {
    "objectID": "content/unix/4-unix-navigation.html#exercises",
    "href": "content/unix/4-unix-navigation.html#exercises",
    "title": "4  Navigating the Unix file system",
    "section": "4.3 Exercises",
    "text": "4.3 Exercises\n\nNavigate to your home directory and list all the files and folders there. Try typing the path with and without using the ~. Rely on tab-completion to assist you and avoid typos (Section A.1).\nPrint the name of the current working directory to your screen.\nList the contents of the ./training/data/fastq/ directory of the course files, without first moving there. Experiment with absolute and relative paths.\nWhat is the most recent modification date of the file Homo_sapiens.GRCh38.dna.chromosome.Y.truncated.fa found in the ./training/unix-demo/ directory?\nTry to search for the file penguins.csv: what is the absolute path to it on your machine?\nNavigate to ./training/data/fastq/ to make it your working directory (double check using pwd!). What is the relative path to the penguins.csv file from here?\nSuppose your working directory is still ./training/data/fastq/. What will the result of pwd be after running each of the following commands in succession?\n\n\ncd ../\ncd ../unix-demo/\ncd files_to_loop_through/../../data/..\ncd /\ncd ~",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Navigating the Unix file system</span>"
    ]
  },
  {
    "objectID": "content/unix/4-unix-navigation.html#summary",
    "href": "content/unix/4-unix-navigation.html#summary",
    "title": "4  Navigating the Unix file system",
    "section": "4.4 Summary",
    "text": "4.4 Summary\n\n\n\n\n\n\nOverview of concepts and commands\n\n\n\n\n\n\nAbsolute versus relative file paths\nRoot (/) and home directory (~)\n. represents the current working directory\n.. represents the parent directory\npwd: print the path of the current working directory\ncd &lt;path&gt;: navigate to the given directory\nls &lt;path&gt;: list files and directories in the given location\nHidden files contain a . at the start of their name and are not visible by default",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Navigating the Unix file system</span>"
    ]
  },
  {
    "objectID": "content/unix/5-unix-files-and-dirs.html",
    "href": "content/unix/5-unix-files-and-dirs.html",
    "title": "5  Working with files and directories",
    "section": "",
    "text": "5.1 Examining files",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working with files and directories</span>"
    ]
  },
  {
    "objectID": "content/unix/5-unix-files-and-dirs.html#examining-files",
    "href": "content/unix/5-unix-files-and-dirs.html#examining-files",
    "title": "5  Working with files and directories",
    "section": "",
    "text": "5.1.1 cat: viewing short files\nThe most basic command for viewing a file is the cat &lt;file&gt; command. It simply prints all of the contents of a file to the screen (= standard output).\n$ cd training/unix-demo\n$ cat short.txt\nOn the Origin of Species\n\nBY MEANS OF NATURAL SELECTION,\n\nOR THE PRESERVATION OF FAVOURED RACES IN THE STRUGGLE FOR LIFE.\n\nBy Charles Darwin, M.A., F.R.S.,\n\nAuthor of “The Descent of Man,” etc., etc.\n\nSixth London Edition, with all Additions and Corrections.\n\n\n\n\n\n\nTry using cat on the file named long.txt and see what happens (Click me to expand!)\n\n\n\n\n\nThe entire file (in this case, the entirety of the Origin of Species by Charles Darwin) is printed to the screen. This works, but is not very easy to navigate. Especially if you consider the fact that this text is still just tiny compared to some of the files that we deal with in bioinformatics; it is only ~0.03% of the size of the (rather short) human Y chromosome (~60 Mbp) that we will look at next.\n\n\n\nWhile cat is very useful, it is clearly not suitable for large text files. Since long files are very prevalent - and not only in bioinformatics - we need an alternative. Enter the less command.\n\n\n5.1.2 less: viewing large files\nThis tool is suitable for streaming very large files which would otherwise crash a normal text editor or program like Excel. less will open the contents of the file in a dedicated viewer, i.e. your terminal and prompt will be replaced by a unique interface for the less tool. You can exit this interface by pressing q.\nUsing less, we can have a look at the (truncated) version of the human Y chromosome (in FASTA format):\n$ less Homo_sapiens.GRCh38.dna.chromosome.Y.truncated.fa\n\n\n\nOpening a FASTA file in less\n\n\n\n\n\n\n\n\nNavigating inside less\n\n\n\n\n\n\nUse arrow keys to navigate. space and b can also be used to go forward and backwards, and page up/page down work as well.\nPress g to jump to the start of the file\nPress G (shift + g) to jump to the end of the file\nType / followed by a string to search forward (?[string] for backwards search) and n/N for the previous/next match\nTo exit, press Q\nUse the help command for more info: less --help\n\n\n\n\n\n5.1.2.1 FASTA file format\n\n\n\n\n\n\nDNA sequence file formats: FASTA\n\n\n\nThe FASTA file format (usually denoted by a .fa or .fasta file extension) is very common in bioinformatics. As you can see, the FASTA files contain a long stretch of nucleotides, which in our case represent the sequence of the human Y chromosome (or at least the first ~6,000,000 basepairs). The sequence itself is usually broken up over multiple lines. At the very top of the file there is a header or identifier, which always starts with the &gt; symbol, followed by a short description. FASTA files can store one or multiple sequences, each with their own header.\nFASTA files are a type of text-based or plain text files, meaning that we can simply read them using a tool like cat or less. This seems obvious, but we will later encounter another file type, namely binary files, where this is not the case.\nFASTA files are commonly used in genomics to store the reference genome of the organism we are studying. A reference genome can be used as a template to which we map (or align) new DNA sequence reads we have generated.\n&gt;Pf3D7_01_v3 | organism=Plasmodium_falciparum_3D7 | version=2020-09-01 | length=640851 | SO=chromosome\nTGAACCCTAAAACCTAAACCCTAAACCCTAAACCCTGAACCCTAAACCCTGAACCCTAAA\nCCCTAAACCCTGAACCCTAAACCCTAAACCCTGAACCCTAAACCCTGAAACCTAAACCCT\nGAACCCTAAACCCTGAACCCTGAACCCTAACCCTAAACCCTAAACCTAAAACCCTGAACC\nCTAAACCCTGAACCCTGAACCCTAAACCCTGAACCCTAAACCCTAAACCCTGAACCCTAA\nACCCTGAACCCTAAACCCTAAACCCTGAACCCTGAACCCTAAAACCTAAACCCTAAACCC\nTAAACCCTAAACCCTGAACCTAAACCTAAAACCTAAAACCTAAAACCCTGAACCCTTACT\nTTTCATTTCTTCTTCTTATCTTCTTACTTTTCATTCTTTACTCTTACTTACTTAGTCTTA\nCTTACTTACTCTTACTTACTTACTCTTATCTTCTTACTTTTCATTTCTTAGTCTTACTTA\n...\n\n\n\n\n\n5.1.3 head and tail: viewing the start or end of files\nSometimes we are not interested in viewing the entire file, but just the first few or last lines. The commands head and tail were created for exactly this use case. The basic usage is simply head &lt;filepath&gt;, but there again are a few optional flags that can alter the default behaviour.\n\n\n\n\n\n\n\nCommand\nResult\n\n\n\n\nhead file\nPrint the first 10 lines of a file\n\n\ntail file\nPrint the last 10 lines of a file\n\n\nhead -n # file\nDisplay the first # lines of a file\n\n\ntail -n # file\nDisplay the final # lines of a file\n\n\ntail -n +2 file\nDisplay all lines except for the first one (i.e., perform tail, but start at line 2)\n\n\n\nLet us inspect the first lines of one of the (uncompressed) FASTQ files in the unix-demo directory:\n$ head PF0512_S47_L001_R1_001.fastq\n@M05795:43:000000000-CFLMP:1:1101:16134:1717 1:N:0:47\nTTGGTCAAGATCTTTTACATTCCATGCACACAAAAGAATTCTTCTACTTGTCTGATCCTTTTTCATTATATTTATTATCTTTTTTTATTTTTCCTCTCCTTTATTTTCATAATTATCATACATATTTTTATATTCATCACCTAAATGTCTCCATTTAAAACCATAAATAGTTCCTAATTCGTTAACTTCTCTATGACATAATTTTCTATTATCTCTTTCTCTTATCCACATCTCCGAGCCCACGAGACGTCGCACCCTCTCCTATGCCCTCTTCTGCTTGAAACACAAACGCCACCACCT\n+\n-8B&lt;CGGFGGGDGGGGGEFGGGAFFEFCFGGDC888C,CF@@FFG,6FC,C,,,,&lt;&lt;,&lt;&lt;CCE,,&lt;C,,,&lt;6B,;B,&lt;,&lt;A@EEC+,:@,,94,,5549@?F,@@?D&lt;,C,,9@,A;,C,9,A,:A=,&gt;,9,9@;,,,,94,,,,9,9499=,9&gt;,,,,,9+,9,,,9,,,493@,,,99+60++6==93=C+C++++6++4&lt;=;=D+?=+42+43+33*?=;+**5*1*)0*108))18=):))))0)185)))1))--:)1*/*16)0/(01//8***()**((((((,(((/((,(,\n@M05795:43:000000000-CFLMP:1:1101:20605:1731 1:N:0:47\nGAAAAAGGAAGAGAATTGAACTTTTGGCAGCAAACTCAAACATTATAAGTGAAATTAAGATGCCCAAGTCTGTGCTCAATCTCATTTTTTGTTTTTGTGTTTTTCCTTCAATCTCTTCATGTATTCAGTTATTTTTAAT\n+\n-@CCCGGGGGGGGCGFG@&lt;EFGGGGAFFFEGGC8FEF9,,;,CE,C,,,&lt;,,,,&lt;C,,,,&lt;,,6;,,,;;C,&lt;,6;C,,&lt;,,:,:@,5,8+,,9:@+:,,9,9,99AE?,,,959AA?,9,,,994,,9=,9&gt;ED,,,9\n@M05795:43:000000000-CFLMP:1:1101:9135:1768 1:N:0:47\nCGTTAAAATCTTGCTCCTCATCACTACTAACCTTTTGTTCATTCTCATCACAAATATTATCCTTATCTTCATTATCTACTTCATCTACATTATTTTTTAT\n\n5.1.3.1 FASTQ file format\n\n\n\n\n\n\nDNA sequence file formats: FASTQ\n\n\n\nAside from FASTA files, another typical DNA sequence format is FASTQ (extension .fastq or .fq), which is used to store the raw output of high-throughput sequencing (like AmpliSeq) in the form of short read fragments. Like FASTA, it is text-based format, but instead of just identifiers and sequences, it also contains quality scores associated with each nucleotide. Each read is described by four lines of text. A single read might look like this:\n@SEQ_ID\nACTACTAGGATTGAGGACGTCCTCCCAACAGGGAGTTGGTTGGGCGCCCGGTGCCGTCATGTCCGATCGCTATCTACGTCTAGTACTAGAGAATTATACA\n+\n\"H85&lt;EI4A533D;E1A56C@@GHI=BFGIIH6;F=3::HGF8C;9/&gt;;EI?E4I(F?FID&lt;CBAFFD69E:BB&gt;+#&lt;58H:/&lt;&gt;IE;881&'D':F&lt;&lt;H(\"\n\n\n\n\n\n\n\nLine\nDescription\n\n\n\n\n1\nidentifier: always starts with ‘@’ and contains information about the read (e.g., instrument, lane, multiplex tag, coordinates, etc.)\n\n\n2\nThe sequence of nucleotides making up the read\n\n\n3\nAlways begins with a ‘+’ and sometimes repeats the identifier\n\n\n4\nContains a string of ASCII characters that represent the quality score for each base (i.e., it has the exact same length as line 2)\n\n\n\nFASTQ files often come in pairs, which are usually named the same with a slightly different suffix (e.g., sample_1_R1.fastq and sample_1_R2.fastq). These pairs are reads of the same fragment in the opposite direction. In a nutshell, paired-end sequencing is used because the additional information provided by reads being paired can help with mapping repetitive regions of the genome.\n\n\n\n\n\n\n\n\nTry inspecting the contents of one of the .fastq.gz files in the training/data/fastq directory (Click me to expand!)\n\n\n\n\n\nThe less command most likely behaves as we expect it to, but if we were to try cat, head or tail, you would see a lot of gibberish being printed to your screen.\nThe reason is that these FASTQ files are compressed using gzip (which is why the file extension ends in .gz). Because of this, they are no longer plain text files, but compressed binary versions. We will learn more about compressed files and how to deal with them in a later section of this course. In a nutshell though, compressed files either need to be unpacked or they require a tool that was designed to handle them (e.g., zcat, zgrep, zless. In most linux distributions, zless is called automatically when you try to less a file with the .gz extension, which is why the text seemed normal.\n\n\n\n\n\n\n5.1.4 wc: counting lines\nSometimes we’re not interested in the specific contents of a file, but only in how long it is in terms of text (not file size). For this we can use the wc command: it can count the number of lines, words and characters in a text file. By default, it prints all of this information, but by providing the -l flag, you can tell the command to only return the number of lines. Taking the example of our FASTQ file again, we see:\n# number of lines, words and characters\n$ wc PF0512_S47_L001_R1_001.fastq\n582940   728675 69598721 PF0512_S47_L001_R1_001.fastq\n\n# number of lines only\n$ wc -l PF0512_S47_L001_R1_001.fastq\n582940 PF0512_S47_L001_R1_001.fastq\n\n\n\n\n\n\nHow many reads are there in this FASTQ file? (Click me to expand!)\n\n\n\n\n\nEach read in a FASTQ file consists of four lines (see Section 5.1.3.1). Therefore, we can simply divide the output of wc -l by four to figure out the number of reads. In this case:  {582,940 \\over 4} = 145,735 reads.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working with files and directories</span>"
    ]
  },
  {
    "objectID": "content/unix/5-unix-files-and-dirs.html#editing-files",
    "href": "content/unix/5-unix-files-and-dirs.html#editing-files",
    "title": "5  Working with files and directories",
    "section": "5.2 Editing files",
    "text": "5.2 Editing files\nYou can edit files directly on the command line, i.e. without opening them in a text editor like Notepad(++) or VSCode, by using the nano command. This can come in quite handy in a variety of situations, like fixing small errors in your code before running it or to editing configuration files. Similar to less, nano will open a special editor interface where you can edit text files.\n\n\n\nThe nano text editor\n\n\n\n\n\n\n\n\nNavigating inside nano\n\n\n\n\nYour mouse pointer won’t work. Use arrow keys to move instead.\nTo save, press ctrl+o, followed by return/enter.\nTo exit, press ctrl+x, followed by return/enter.\n\n\n\nThere exist many other editors, one of the most beloved, yet notorious ones, being vim. It is quite a bit more powerful, but also more complex. Even closing vim has become somewhat of a meme because it can be difficult to figure out (it’s &lt;escape&gt; followed by :q and `enter/return``).",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working with files and directories</span>"
    ]
  },
  {
    "objectID": "content/unix/5-unix-files-and-dirs.html#moving-things-around",
    "href": "content/unix/5-unix-files-and-dirs.html#moving-things-around",
    "title": "5  Working with files and directories",
    "section": "5.3 Moving things around",
    "text": "5.3 Moving things around\nNow that we have spent some time on inspecting files, let us move on to moving them around.\n\n5.3.1 cp: copying files and directories\ncp stands for copy and it does exactly what it says on the tin. It can copy files, as well as directories to a new location. For files, the syntax is as follows:\ncp path/to/source_file path/to/destination\nWhere source is the original file that you want to copy and destination is the new path where you want to place the copy. If the destination is a directory, the file will be placed inside of it with the same name as the original file. If the destination does not exist yet, it will be used as the new name for the copy.\nWhen we want to move around directories instead of files, we need to add the -r flag (short for --recursive).\ncp -r path/to/source_directory path/to/destination_file\nYou can even copy multiple files at the same time!\n$ cp file_1 file_2 file_3 /path/to/destination\n\n$ ls /path/to/destination\nfile_1 file_2 file_3\n\n\n5.3.2 Intermezzo: globbing and wildcards\nNow seems like a good time to introduce the concept of the globbing and wildcards. Globbing allows you to perform operations on multiple files. By providing specific patterns, the shell will be able to expand them into a list of matching file names. The patterns are built using wildcards, one of the most common ones being the asterisk *.\nHow does this work? Well, * can represent any number of other characters. For example, the string *.txt can match all file names ending with .txt in your directory. Let’s look at a concrete example, using the ls command we saw earlier:\n$ ls\nHomo_sapiens.GRCh38.dna.chromosome.Y.truncated.fa  PF0512_S47_L001_R1_001.fastq  files_to_copy  files_to_delete  files_to_move  long.txt  short.txt  penguins.csv\n\n$ ls *.txt\nlong.txt  short.txt\nAs you can see, we can make ls list only those files that match a particular pattern, instead of showing all the files in the directory. What happens behind the scenes is that *.txt is expanded to long.txt short.txt. This means that the command that the shell eventually sees is actually ls long.txt short.txt.\nSimilarly, we can combine wildcards with the new cp command.\n$ cp *.txt ..\n\n$ ls ..\n\n\n\n\n\n\nWhat do you think this previous command will do? (Click me to expand!)\n\n\n\n\n\n*.txt will be expanded to a list of all .txt files in the current working directory. The cp command will then try to copy each of those files to the destination, which is .. in this case. As we saw before, .. represents the parent directory of the current directory (see Section A.2).\nThis means that the command is equivalent to cp long.txt short.txt /absolute/path/to/parent_directory and will move all the .txt files in the current directory to its parent directory.\n\n\n\nAnother type of wildcard is [...]. This is used to supply a list of possible character matches. For example, the glob pattern [bcr]at would match bat, cat and rat.\nThere are a number of other wildcards, but even * alone will prove to be very useful. If you’d like to find out more, have a look at this resource. Also note that globbing looks similar to regular expressions, but while related, these two concepts behave slightly differently. We will not dive into regular expressions here though, but we will mention them again when we talk about the search tool grep.\nTo summarise, globbing is an extremely powerful tool that will allow you to more easily target multiple files. We will rely on the power of globbing a lot going forward.\n\n\n5.3.3 mv Moving or renaming files and directories\nThe mv (move) command behaves very similar to the cp command, the main difference being that the former allows you to move rather than copy files and directories. Also note that mv is used to rename files as well.\n# move around/rename a particular file\nmv &lt;source_file&gt; &lt;destination_file&gt;\n\n# move a directory\nmv &lt;path/to/source_directory&gt; &lt;path/to/destination_directory&gt;",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working with files and directories</span>"
    ]
  },
  {
    "objectID": "content/unix/5-unix-files-and-dirs.html#creation-and-destruction",
    "href": "content/unix/5-unix-files-and-dirs.html#creation-and-destruction",
    "title": "5  Working with files and directories",
    "section": "5.4 Creation and destruction",
    "text": "5.4 Creation and destruction\nWe will end this section by teaching you how to create and delete files or directories.\n\n5.4.1 Creating files\nThere are several ways of creating new files in Unix, but one of them is the nano command that we already introduced earlier. If you provide a file name that does not yet exist, nano will create the file for you.\n$ ls\n\n$ nano new_file.txt\n# inside nano, use ctrl+x to save the file and then close the editor via ctrl+x\n\n$ ls\nnew_file.txt\nAnother option is to use the touch /path/to/file command. This will just create a new empty file at the specified location.\n\n\n5.4.2 mkdir: creating directories\nmkdir stands for make directory and it does just that:\n$ mkdir new_dir\n\n$ ls\nnew_dir\nOne useful optional flag is -p/--parents: this allows you to create multiple nested (parent) directories in one go. For example, if we’re inside an empty directory, we could call:\nmkdir -p my/new/multi/level/directory\nAnd all the intermediate directories would be automatically created.\n\n\n5.4.3 rm: removing things\n\n\n\n\n\n\nWatch out…\n\n\n\nBe careful while learning your way around the command-line. The Unix shell will do exactly what you tell it to, often without hesitation or asking for confirmation. This means that you might accidentally move, overwrite or delete files without intending to do so. For example, when creating, copying or moving files, they can overwrite existing ones if you give them the same name. Similarly, when a file is deleted, it will be removed completely, without first passing by a recycle bin.\nNo matter how much experience you have, it is a good idea to remain cautious when performing these types of operations.\nFor the purposes of learning, if you are using your own device instead of a cloud environment, we recommend that you work in a dedicated playground directory or even create a new user profile to be extra safe. And like always, backups of your important files are invaluable regardless of what you are doing.\n\n\nThe rm command (remove) is used to delete files and directories. Be warned though, once deleted, things are really gone. There is no recycle bin or trash folder where you can restore deleted items!\n# for files:\nrm &lt;file path&gt;\n\n# for directories\nrm -r &lt;directory path&gt;\nFor files, this works as expected, but for directories you need to provide the -r flag (or --recursive). This tells Unix to remove the directory recursively, i.e. all of its contents need to be removed as well. If you don’t use this option, you will see the following warning:\nrm directory\nrm: cannot remove 'test/': Is a directory\n\n\n\n\n\n\nProtected files\n\n\n\n\n\nSometimes, files will be protected and you will get another warning message when you try to remove them. If you are really sure that you want to delete them, you can type y and press enter. Alternatively, you can cancel the operation (by entering n or by pressing ctrl+c) and try again, but this time providing the -f/--force option.\n# create a new empty file\n$ touch protected-file\n# change its permissions so that it is protected against writing and deleting (see appendix for more info on file permissions)\n$ chmod a-w protected-file\n# try to remove it\n$ rm protected-file\nrm: remove write-protected regular empty file 'protected-file'? n\n# use the --force flag\n$ rm -f protected-file",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working with files and directories</span>"
    ]
  },
  {
    "objectID": "content/unix/5-unix-files-and-dirs.html#exercises",
    "href": "content/unix/5-unix-files-and-dirs.html#exercises",
    "title": "5  Working with files and directories",
    "section": "5.5 Exercises",
    "text": "5.5 Exercises\n\nCreate a new directory named “my_dir” inside the ./training/unix-demo directory. Next, without using cd first, create another directory named my_sub_dir inside of it. Finally, again without using cd, create a final directory named my_sub_sub_dir inside of that one.\nRead the last 20 lines of the FASTA file in the ./training/unix-demo directory.\nCreate a new text file named lines inside my_sub_dir using nano. Store the number of lines of the file long.txt inside. Then read it using cat and less.\nNavigate to the files_to_copy directory and copy its contents to the my_sub_dir directory. What is the relative path of the destination to use?\nMove the file under files_to_move to its parent directory.\nRemove all the files under files_to_delete using a glob pattern.\nRename the directory files_to_delete to empty_dir.\nList the contents of the ./training/unix-demo directory.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working with files and directories</span>"
    ]
  },
  {
    "objectID": "content/unix/5-unix-files-and-dirs.html#summary",
    "href": "content/unix/5-unix-files-and-dirs.html#summary",
    "title": "5  Working with files and directories",
    "section": "5.6 Summary",
    "text": "5.6 Summary\n\n\n\n\n\n\nOverview of concepts and commands\n\n\n\n\n\n\nFASTA file format is used to store DNA sequences\nFASTQ file format is used to store sequence reads and their quality scores\nCompressed files (e.g., .gz) are smaller in file size, but are no longer plain text files and require special tools.\nThe permission of files can be set to prevent users from reading, writing or (re)moving them.\n\n\n\n\n\n\n\n\nCommand\nResult\n\n\n\n\ncat &lt;path/to/file&gt;\nprint the content of files\n\n\nless &lt;path/to/file&gt;\nread the contents of (large) files in a special viewer\n\n\nhead/tail &lt;path/to/file&gt;\nview the first or last lines of a file\n\n\nwc &lt;path/to/file\ndisplay the line/word/character count of a file\n\n\nnano &lt;path/to/file&gt;\nopen a file (or create a new file) in the nano text editor\n\n\ncp [-r] &lt;source&gt; &lt;destination&gt;\ncopy a file/directory to a new location\n\n\nmv [-r] &lt;source&gt; &lt;destination&gt;\nmove a file/directory to a new location (or rename it)\n\n\nrm [-r] &lt;path/to/file_or_directory&gt;\npermanently remove a file/directory\n\n\nmkdir &lt;path/to/directory&gt;\ncreate a new directory",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working with files and directories</span>"
    ]
  },
  {
    "objectID": "content/unix/6-unix-more-commands.html",
    "href": "content/unix/6-unix-more-commands.html",
    "title": "6  More advanced commands",
    "section": "",
    "text": "6.1 Searching in files: grep\nBeing able to search through (long) text files is incredibly useful in a wide range of scenarios. For this, we make use of the grep command. It is a very powerful and complex command, with many different options to tweak its behaviour, but even just the basic version can already be a lifesaver. The basic syntax is\nFor example, we can look for the word “evolution” in the Origin of Species:\nIt might not be obvious from the above snippet, but inside your own terminal, the matching words in the search results will be highlighted. As you can see, grep will return each line that contains a match. Also note how partial matches like revolution are returned as well.\nAside from the -i option explained in the box above, there exist several other flags to improve your search results.\nThe -# option is particularly useful to learn more about the context around you search result. You can supply any number of lines here, which will get printed both before and after each match. The example below, you can see how it helps us find the identifier of a DNA read that contains a particular sequence (AACCGGGGT):\nThe -c options makes grep return the total number of matches it found. This method of counting is useful to complement the wc command, in case you are presented with a file that does not have such an orderly structure as the FASTQ format we saw earlier. To demonstrate, consider the penguins.csv file, which contains morphological data on three different penguin species.1 We can count the number of Adelie penguin records via:\nIn some situations we want to search through multiple files simultaneously. This is where the -r/--recursive flag comes in. It allows us to target a directory and search through all of its contents (including subdirectories). Let us try searching for the same DNA sequence as before, but this time targeting all the files in the unix-demo directory:\nWe already mentioned regular expressions in the previous section: they allow you to search for particular patterns that can match more than one exact string of text. This is tremendously useful, but we will not dive deeply into how they work during this course. If you are interested, you can check out an excellent tutorial here.\nOne special pattern that we will introduce is the start or end of line anchor. You can search for patterns that start at the beginning of a line by prefixing the pattern with a ^ symbol. Similarly, you can search for patterns that occur at the end of a line by using a $ symbol. For example, if we search for grep \"&gt;@\" read.fastq in a FASTQ file, we will only retrieve lines that start with the @ symbol, ignoring any @ symbols that are used elsewhere on a line.\nWe will see more elaborate use cases for grep when we introduce the Unix concepts of piping and redirection.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More advanced commands</span>"
    ]
  },
  {
    "objectID": "content/unix/6-unix-more-commands.html#searching-in-files-grep",
    "href": "content/unix/6-unix-more-commands.html#searching-in-files-grep",
    "title": "6  More advanced commands",
    "section": "",
    "text": "grep \"PATTERN\" &lt;path/to/target_file&gt;\n\n$ grep \"evolution\" long.txt\nevolutionists that mammals are descended from a marsupial form; and if\nAt the present day almost all naturalists admit evolution under some\nevolutionists; but there is no need, as it seems to me, to invoke any\nEveryone who believes in slow and gradual evolution, will of course\nof gradual evolution, through the preservation of a large number of\na strong disbeliever in evolution, but he appears to have been so much\nhistorian will recognise as having produced a revolution in natural\nthe fact would be fatal to the theory of evolution through natural\nthe revolution in our palæontological knowledge effected by the\nopposed to the admission of such prodigious geographical revolutions\nhas thus been arrived at; and the belief in the revolution of the earth\nsubject of evolution, and never once met with any sympathetic\nagreement. It is probable that some did then believe in evolution, but\nevolution. There are, however, some who still think that species have\nwe can dimly foresee that there will be a considerable revolution in\n\n\n\n\n\n\n\nTry searching for the string “species” instead. Do you think these are all the hits? (Click me to expand!)\n\n\n\n\n\nWhen you run grep \"species\" long.txt, you will indeed find many occurrences of this word. However, we are missing all the occurrences of “Species”. Try running grep \"Species\" long.txt to compare the results. Lastly, try running the command grep -i \"species\" long.txt. This option will\nRemember, capitalization matters to grep (and to Unix as a whole)!\n\n\n\n\n\n\n\n\n\n\n\nOption\nEffect\n\n\n\n\n-i\nCase insensitive (i.e., ACTG = actg)\n\n\n-n\nAlso print the line number of the result\n\n\n-c\nCount the number of matches\n\n\n-5\nPrint 5 lines surrounding each match\n\n\n-e/-E/-P\nUse regular or extended regular expressions\n\n\n-r\nRecursive search through all files in a folder\n\n\n\n\n$ grep -3 AACCGGGGT PF0512_S47_L001_R1_001.fastq\n+\nCCCCCGGCFGGGGFGGGGDGCEFGGGGGGGGGGGGGGGGGGGGGGFGGGGGGGGFFGGGFGGGGGGGGGGGGGGFGGGGGGGGGGGGGGGGGGGGGGGGGD+,A=FG,:&lt;?F5FGGGD&gt;F9BDBDE9FEDAFGAFGGB=EDF,,&gt;@FCCF;@;D;CF;,5;DEGG84,@@,3@ED&lt;@,,@7,;@,7EC2=E&gt;C977=\n@M05795:43:000000000-CFLMP:1:2106:16840:21815 1:N:0:47\nAGCCATACCAAGACCACAATTCTGAAGAGGAAACAAAACAAAAAAAAAAAAATAATTAAAAAAAAAAAAATTTAAAATTAAAAAAAAAATTTTTTTATTAAAATAATAAATATTAATTTTTATAATATAAATAAAATCCTATTTTACCCCACCAACCGGGGTTCATCCCCGGGCTCTTATACACATTTCCTAACCCACAAAAAGTACGAACAACACGCAACCCCCCTTCTGCCTTAAAAAAAAAACAAATCAAAATACACAAATATATCGAACATACAGCAACTACAAATGAAGATGTGGT\n+\nCCCCCGGFGGGFGGGGCFG9@@C&lt;FGDG8EGGGFGGGGGGGGGGGGGGGGGD,:C,96CFD,8&gt;FG7FGG,,,,B&lt;B9,9CFFGGCG+3,,8BF@@,=8,8,=,C,,3@,,,3,3@,DGGFC,,,,,,7,,,,7&gt;C,,3,,,7@@@:9,6**5,,**4*1*=8,,+5&gt;=:****3/+&gt;2;;92++2+++4++29*/:**9*1**3*0*/*/95)1*1))))29*05)202.:96*/-@=&lt;(7:(.)00()))))))).-,)(())--)-(((.().,.:)8..(,).).))-4))..)))(\n@M05795:43:000000000-CFLMP:1:2106:14940:21820 1:N:0:47\n\n1 Gorman KB, Williams TD, Fraser WR (2014) Ecological Sexual Dimorphism and Environmental Variability within a Community of Antarctic Penguins (Genus Pygoscelis). PLoS ONE 9(3): e90081. doi:10.1371/journal.pone.0090081grep -c \"Adelie\" penguins.csv\n152\n\n$ grep -r \"AACCGGGGT\" .\n./PF0512_S47_L001_R1_001.fastq:AGCCATACCAAGACCACAATTCTGAAGAGGAAACAAAACAAAAAAAAAAAAATAATTAAAAAAAAAAAAATTTAAAATTAAAAAAAAAATTTTTTTATTAAAATAATAAATATTAATTTTTATAATATAAATAAAATCCTATTTTACCCCACCAACCGGGGTTCATCCCCGGGCTCTTATACACATTTCCTAACCCACAAAAAGTACGAACAACACGCAACCCCCCTTCTGCCTTAAAAAAAAAACAAATCAAAATACACAAATATATCGAACATACAGCAACTACAAATGAAGATGTGGT\n./Homo_sapiens.GRCh38.dna.chromosome.Y.truncated.fa:AATAAAACCGGGGTTGATACCACCACTTCCAGGTTCCCACATTCCAAGTCCCCTCAGCCA\n./Homo_sapiens.GRCh38.dna.chromosome.Y.truncated.fa:CTGGAGTCAGGACGTGAGCCGACTTGCTTAAAAATAAATCCACATGGCTGAACCGGGGTT\n./Homo_sapiens.GRCh38.dna.chromosome.Y.truncated.fa:ACAGCTAACCGGGGTTTTAGTATATGTGCCACATCTCTGTAAATGTTCACTTCCTAGGCA\n./Homo_sapiens.GRCh38.dna.chromosome.Y.truncated.fa:TATGATCGTGCCACTGCACTTCAACCGGGGTGACAAAGCGAAAACCGTGTCTCTAAAAAA\n\n\n\n\n\n\nHow would you search through .txt files only? (Click me to expand!)\n\n\n\n\n\nInstead of using the -r flag, we can also rely on globbing again (see Section 5.3.2). To search for the string “needle” in all .txt files in a particular folder, we can do the following:\ngrep \"needle\" path/to/directory_with_txt_files/*.txt\n\n\n\n\n\n\n\n\n\n\n\nThe above command could still return matches that are not read headers. Why? (Click me to expand!)\n\n\n\n\n\nAs we saw in Section 5.1.3.1, every read in a FASTQ file is represented by four lines, the first of which is the header and which always starts with an @ symbol. So far so good, but the fourth line, which contains the quality score of each base in the sequence, can also start with an @ symbol, because @ is just another score value that could happen to be occur for the first base in the sequence.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More advanced commands</span>"
    ]
  },
  {
    "objectID": "content/unix/6-unix-more-commands.html#tabular-data-cut",
    "href": "content/unix/6-unix-more-commands.html#tabular-data-cut",
    "title": "6  More advanced commands",
    "section": "6.2 Tabular data: cut",
    "text": "6.2 Tabular data: cut\nWe already encountered tabular data (the penguin dataset in .csv format) when talking about grep. Tabular data files like .csv are a very common format, and not just in bioinformatics.\n\n\n\n\n\n\nTabular data and .csv files\n\n\n\nTabular data files are usually plain text files, where each row corresponds to a record (e.g., an individual penguin), and each column represents a particular field (e.g., species, flipper length, body mass, etc.). The columns can be separated by different field delimiters or separators. In .csv files, these are usually commas (comma separated values), but they can also be TABS (.tsv) or semicolons (;).\n\n\nA particularly useful Unix tool for manipulating tabular data files, is cut. It allows us to extract particular columns from these files. The syntax is as follows:\ncut [OPTIONS] target_file\n\n\n\n\n\n\n\nOption\nEffect\n\n\n\n\n-d \",\"/--delimiter \";\"\nChange the default delimiter (TAB) to another character like ,\n\n\n-f 1\nSelect the first column\n\n\n-f 2,3\nSelect the second and third column\n\n\n-f 1-3,6\nSelect columns one through three and columns six\n\n\n--complement -f 1\nSelect all columns except for the first one\n\n\n-r\nRecursive search through all files in a folder\n\n\n\n\n6.2.1 SAM file format\nAside from .csv and .tsv files, there are many bioinformatics file formats that also follow a tabular lay-out. One of these is the SAM file format.\n\n\n\n\n\n\nSAM: Sequence Alignment/Map Format\n\n\n\nThe SAM file format is a tab-delimited text file that stores information about the alignment of sequence reads to a reference genome.\nWhen considering the steps that are taken during the variant calling analysis of sequence reads, SAM files result when the reads inside a FASTQ file are processed by an alignment tool like bwa and minimap. These tools take each individual read corresponding to a particular sample and try to map them to their most likely position in a reference genome.\nSAM files consists of an optional header section followed by an alignment section.\nThe header lines always start with an @ symbol and contain information about e.g., the reference genome that was used and the sorting of the alignments. This section is not yet tab-delimited.\nThe alignment section contains a tab-delimited line for each sequence read that was aligned to the reference. There are 11 mandatory fields, containing information on the position of the read in the reference genome (i.e., where it was mapped), the sequence itself, its quality score ( = the quality of each base pair of the sequence during sequence calling, cf. FASTQ format), its mapping score ( = how well the read aligned to the reference genome), etc.\n\n\n\nCol\nField\nType\nBrief description\n\n\n\n\n1\nQNAME\nString\nQuery template NAME\n\n\n2\nFLAG\nInt\nbitwise FLAG\n\n\n3\nRNAME\nString\nReferences sequence NAME\n\n\n4\nPOS\nInt\n1- based leftmost mapping POSition\n\n\n5\nMAPQ\nInt\nMAPping Quality\n\n\n6\nCIGAR\nString\nCIGAR string\n\n\n7\nRNEXT\nString\nRef. name of the mate/next read\n\n\n8\nPNEXT\nInt\nPosition of the mate/next read\n\n\n9\nTLEN\nInt\nobserved Template LENgth\n\n\n10\nSEQ\nString\nsegment SEQuence\n\n\n11\nQUAL\nString\nASCII of Phred-scaled base QUALity+33\n\n\n\nWe will not dive into the details of each of these fields for now, but if you are interested you can check out the official documentation or this useful write-up.\nA typical SAM file will look something like this:\n@SQ     SN:ref  LN:45\n@SQ     SN:ref2 LN:40\nr001    163     ref     7       30      8M4I4M1D3M      =       37      39      TTAGATAAAGAGGATACTG     *       XX:B:S,12561,2,20,112\nr002    0       ref     9       30      1S2I6M1P1I1P1I4M2I      *       0       0       AAAAGATAAGGGATAAA       *\nr003    0       ref     9       30      5H6M    *       0       0       AGCTAA  *\nr004    0       ref     16      30      6M14N1I5M       *       0       0       ATAGCTCTCAGC    *\nr003    16      ref     29      30      6H5M    *       0       0       TAGGC   *\nr001    83      ref     37      30      9M      =       7       -39     CAGCGCCAT       *\nx1      0       ref2    1       30      20M     *       0       0       aggttttataaaacaaataa    ????????????????????\nx2      0       ref2    2       30      21M     *       0       0       ggttttataaaacaaataatt   ?????????????????????\nx3      0       ref2    6       30      9M4I13M *       0       0       ttataaaacAAATaattaagtctaca      ??????????????????????????\nx4      0       ref2    10      30      25M     *       0       0       CaaaTaattaagtctacagagcaac       ?????????????????????????\nx5      0       ref2    12      30      24M     *       0       0       aaTaattaagtctacagagcaact        ????????????????????????\nx6      0       ref2    14      30      23M     *       0       0       Taattaagtctacagagcaacta ???????????????????????\nThis example has two header lines denoting the reference genome contigs and their lengths, followed by 12 aligned reads.\nNow inspect the SAM file in ./training/unix-demo/PF0302_S20.sort.sam and try to identify the different sections that make up the file.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More advanced commands</span>"
    ]
  },
  {
    "objectID": "content/unix/6-unix-more-commands.html#file-sizes-du",
    "href": "content/unix/6-unix-more-commands.html#file-sizes-du",
    "title": "6  More advanced commands",
    "section": "6.3 File sizes: du",
    "text": "6.3 File sizes: du\nWe already saw that the ls -lh can be used to figure out the file size of files in a particular directory. du is another tool to do this, but it operates on individual files or directories directly. Like ls, it also provides the -h/--human-readable option to return file sizes in KB/MB/GB, so it is generally recommended to always use this option. When used on a file, it will simply return its size, but when used on a directory, it will output information for all files, as well as the total file size of the entire directory (the final line of the output).\n# targetting an individual file\n$ du -h Homo_sapiens.GRCh38.dna.chromosome.Y.truncated.fa\n5.9M    Homo_sapiens.GRCh38.dna.chromosome.Y.truncated.fa\n\n# targetting a directory\n$ du -h training/data/\n284M    training/data/fastq\n62M     training/data/reference\n346M    training/data/",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More advanced commands</span>"
    ]
  },
  {
    "objectID": "content/unix/6-unix-more-commands.html#sec-compression",
    "href": "content/unix/6-unix-more-commands.html#sec-compression",
    "title": "6  More advanced commands",
    "section": "6.4 Compressed files: gzip",
    "text": "6.4 Compressed files: gzip\nWe already introduced the concept of file compression when talking about the FASTQ files in the training/data/fastq directory. As a reminder, compressed files are binary files (as opposed to human-readable plain text files) that are used to reduce the file size for more efficient storage. Many of the files that we use in bioinformatics tend to be compressed. Some of the tools we use, will not work on compressed files (e.g., try to cat a compressed file and see what happens), so we either need to 1) use specialized tools that expect compressed files as their input, or 2) decompress or extract the files first.\nFor gzipped (.gz) files specifically, we can do this via the gzip and gunzip commands. The former allows us to create a gzip-compressed version of the file, whereas the latter will extract one back to a plain text file. The basic syntax is gzip/gunzip &lt;path/to/file&gt;, but a very useful option is the -k/--keep flag. Without it, compressing a file would replace the uncompressed file with the new compressed one (vice versa: extracting would replace the compressed version with the extracted one), but when using the flag both files will be retained.\n\n\n\n\n\n\nTry compressing the FASTQ file in the unix-demo directory. By how much does its file size change? (Click me to expand!)\n\n\n\n\n\n$ du -h PF0512_S47_L001_R1_001.fastq\n67M     PF0512_S47_L001_R1_001.fastq\n$ gzip --keep PF0512_S47_L001_R1_001.fastq\n$ du -h PF0512_S47_L001_R1_001.fastq.gz\n17M     PF0512_S47_L001_R1_001.fastq.gz\nAfter compressing the FASTQ file with gzip, it shrunk to less than a third of its original size.\n\n\n\nDo note that there exist other types of file compression besides gzip, like .zip/.7zip. In unix we also often make use of tar (which technically is not a compression tool, but a file archiver). File compression and tar can even be combined, leading to files with suffixes like .tar.gz. This allows us to compress entire directories, instead of only individual files.\nTo extract these so called tarballs, we need to use the tar command:\n# extract .tar.gz archive\n$ tar -xzvf tar_archive.tar.gz\ntar_archive/\ntar_archive/3\ntar_archive/2\ntar_archive/1\n\n$ ls tar_archive\n1  2  3\n\n# create .tar.gz archive\n$ tar -czvf new_archive.tar.gz &lt;path/to/target_directory&gt;\nThis command is notorious for how arcane its option flags are, but you can either try to remember it using a mnemonic (“eXtract/Compress Ze Vucking Files”, pronounced like a B-movie vampire) or the meaning of the individual flags (z tells tar that we are using gzip compression, -v stands for --verbose to make the command show more information and output, -c/x switches between compression and extraction mode, -f is the last option and points to the tar file) . And of course, the correct syntax is only a google/tldr search or tar --help call away.\n\n6.4.1 BAM file format\nA final example of a binary file that you might encounter in the bioinformatic analysis of AmpliSeq sequencing is the BAM file format:\n\n\n\n\n\n\nBAM: Binary Alignment Map\n\n\n\nA BAM file is nothing more than a compressed, binary representation of a SAM file. It is used to reduce the file size of SAM files and also improve the speed of specific operations like sorting or retrieving information from the file.\nMany bioinformatics tools can handle BAM file natively. However, similar to gzipped files, BAM files are binary and we can no longer preview them using tools like cat and less.\nOne of the tools that is often used to convert between the two types of alignment file formats is samtools, which is maintained by the same group of people who manage the format specification for SAM/BAM files.\n\n\nAside from BAM, you might also encounter the CRAM file format. This is a more recent and even more highly optimized compressed alternative for sequence alignments, but it is not as commonly used (yet).\nsamtools is a commonly used program to manage SAM/BAM files, which we will introduce later when discussing the structure of variant calling pipelines.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More advanced commands</span>"
    ]
  },
  {
    "objectID": "content/unix/6-unix-more-commands.html#sec-plasmodb",
    "href": "content/unix/6-unix-more-commands.html#sec-plasmodb",
    "title": "6  More advanced commands",
    "section": "6.5 Downloading files: wget",
    "text": "6.5 Downloading files: wget\nwget is a command that allows you to download files from a particular web address or URL and place them in your working directory. While there are several optional flags, in its most basic form the syntax is simply: wget URL. This command is not only useful when automating certain tasks, but also crucial if you ever find yourself in a Unix environment that does not have a GUI at all (e.g., compute clusters or cloud servers).\n\n\n\n\n\n\nTry downloading the Plasmodium falciparum 3D7 reference genome in FASTA format from PlasmoDB and store it in ./training/data/results. (Click me to expand!)\n\n\n\n\n\n\nAt the top of the page, click on Data -&gt; Download data files.\nSearch for falciparum 3D7 and then narrow down your search by selecting the most recent release and the FASTA file format. Alternatively, you can click on the Download Archive link in the top and navigate the file directory to the current release.\nThe file name of the 3D7 reference genome in FASTA format is PlasmoDB-66_Pfalciparum3D7_Genome.fasta.\nRight click the file and copy its URL to your clipboard: https://plasmodb.org/common/downloads/release-66/Pfalciparum3D7/fasta/data/PlasmoDB-66_Pfalciparum3D7_Genome.fasta.\nCreate and navigate to the output directory (mkdir -p ./training/data/results and cd ./training/data/results)\nDownload the file here using the command: wget https://plasmodb.org/common/downloads/release-66/Pfalciparum3D7/fasta/data/PlasmoDB-66_Pfalciparum3D7_Genome.fasta.\n\n\n\n\nTwo optional flags that you might find useful are: 1) -o allows you to rename the download file, and 2) -P &lt;path/to/directory saves the file in a directory of your choice instead of the current working directory. Of course, these are just small convenient timesavers, since you can always cd to a particular location and use mv to rename the file afterwards.\nLastly, an alternative to wget that you might encounter at some point is curl. On the whole, it acts quite similar to wget for the most part.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More advanced commands</span>"
    ]
  },
  {
    "objectID": "content/unix/6-unix-more-commands.html#retrieving-file-names-basename",
    "href": "content/unix/6-unix-more-commands.html#retrieving-file-names-basename",
    "title": "6  More advanced commands",
    "section": "6.6 Retrieving file names: basename",
    "text": "6.6 Retrieving file names: basename\nbasename is a rather simple command: if you give it a long file path, it will return the final section (i.e., the file name).\n# starting in the `training` directory\n$ pwd\n/home/pmoris/itg/FiMAB-bioinformatics/training\n\n# get the file name for the reference genome we just downloaded\n$ basename data/reference/PlasmoDB-65_Pfalciparum3D7_Genome.fasta\nPlasmoDB-65_Pfalciparum3D7_Genome.fasta\nWe can also use this command to remove a particular suffix from a filename:\n$ basename PF0512_S47_L001_R1_001.fastq .fastq\nPF0512_S47_L001_R1_001\nAt this point in time, it might not seem particularly useful to be able to extract the file name of a file, but when we introduce the concept of for loops and bash scripting, it will become more clear why this can be so useful.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More advanced commands</span>"
    ]
  },
  {
    "objectID": "content/unix/6-unix-more-commands.html#sorting-and-removing-duplicates-sort-and-uniq",
    "href": "content/unix/6-unix-more-commands.html#sorting-and-removing-duplicates-sort-and-uniq",
    "title": "6  More advanced commands",
    "section": "6.7 Sorting and removing duplicates: sort and uniq",
    "text": "6.7 Sorting and removing duplicates: sort and uniq\nThe final two commands that we will introduce are yet again tools to manipulate plain text files. The first is sort, which does exactly that you expect it to. It can sort all the rows in a text file. Its syntax is:\nsort [OPTIONS] &lt;./path/to/file&gt;`\nThere are optional flags that allow you to choose the type of order to use (e.g., numerical -n/--numeric-sort instead of alphabetical ), reverse the order of the sort (-r/--reverse) or ignore capitals (-f/--ignore-case).\nThe second command, uniq, is used to remove duplicate lines in a file. It also offers the option to count the frequency of each unique line.\n# given the following file\n$ cat file_with_duplicates.txt\na\na\nb\nb\nb\nb\nc\n\n$ uniq -c file_with_duplicates.txt\n2 a\n4 b\n1 c\nThese two commands are often used in conjunction, because uniq on its own is not capable of filtering out identical lines that are not adjacent. So to truly remove all duplicate lines in a file, we would first need to sort it: sort unsorted_file.txt | uniq. Also note that the sort command also offers a flag -u, which will make it return only unique lines after sorting.\nIn the next section, we will introduce a method of combining these commands in a more convenient way than running them one by one and without needing to create any intermediary files.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More advanced commands</span>"
    ]
  },
  {
    "objectID": "content/unix/6-unix-more-commands.html#other-commands",
    "href": "content/unix/6-unix-more-commands.html#other-commands",
    "title": "6  More advanced commands",
    "section": "6.8 Other commands",
    "text": "6.8 Other commands\nOf course, there exist many more Unix commands than the ones we introduced here. We will end this section by briefly mentioning two that you might run into at some point awk and sed. Both of them allow you to search and replace patterns in text files, and with awk you can even perform more complex operations including calculations. We will not dive into them here, but keep them in the back of your mind for the future.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More advanced commands</span>"
    ]
  },
  {
    "objectID": "content/unix/6-unix-more-commands.html#exercises",
    "href": "content/unix/6-unix-more-commands.html#exercises",
    "title": "6  More advanced commands",
    "section": "6.9 Exercises",
    "text": "6.9 Exercises\n\nVisit PlasmoDB again and find the download URL for the Plasmodium vivax P01 reference genome sequence in FASTA format. Download it via the command line and store it in ./training/data/reference.\nReport the file size of this reference genome in MBs.\nFind out how many lines of text the file contains.\nSearch through the file for the &gt; character, which is used to denote every chromosome/contig. Use a single command to count them.\nCompress the FASTQ file PF0512_S47_L001_R1_001.fastq in the unix-demo directory using gzip.\nNavigate to the directory ./training/data/fastq and in a single command, extract the forward (PF0097_S43_L001_R1_001.fastq.gz) and reverse (PF0097_S43_L001_R2_001.fastq.gz) of the PF0097_S43 sample, without removing the compressed files. Hint: use globbing!\nSearch both FASTQ files for the read fragment with identifier @M05795:43:000000000-CFLMP:1:1101:21518:5740 2:N:0:43 using a single command.\nCompare the file sizes of the two compressed and uncompressed FASTQ files.\nExtract the columns containing the island and flipper length of each penguin from the ./training/unix-demo/penguins.csv file.\nCount how often the sequence CATCATCATCATCAT occurs in the FASTA file ./training/unix-demo/Homo_sapiens.GRCh38.dna.chromosome.Y.truncated.fa.\nWhich command can be used to extract the name of the SAM file ./training/unix-demo/PF0302_S20.sort.sam without the .sam suffix?\nWhich reference genome was used to create the SAM file?",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More advanced commands</span>"
    ]
  },
  {
    "objectID": "content/unix/6-unix-more-commands.html#summary",
    "href": "content/unix/6-unix-more-commands.html#summary",
    "title": "6  More advanced commands",
    "section": "6.10 Summary",
    "text": "6.10 Summary\n\n\n\n\n\n\nOverview of concepts and commands\n\n\n\n\n\n\nTabular data: .csv, .tsv\nThe SAM/BAM file formats store sequence reads aligned to a reference genome.\n\n\n\n\n\n\n\n\nCommand\nResult\n\n\n\n\ngrep &lt;pattern&gt; &lt;path/to/file&gt;\nSearch through a (very large) file for the supplied pattern\n\n\ndu &lt;-h&gt; &lt;path/to/file_or_directory&gt;\nCheck how much space a file or directory occupies\n\n\ncut -f [--delimiter \",\"] &lt;path/to/file&gt;\nExtract columns from tabular data using the specified delimiter\n\n\ngzip / gunzip (-keep) &lt;path/to/file&gt;\nCompress or extract a gzip compressed file (.gz)\n\n\nwget &lt;url&gt;\nDownloads a file from the URL to the current directory\n\n\nbasename &lt;path/to/file&gt;\nReturns the name of the file without the path prefix\n\n\nbasename &lt;path/to/file&gt; \nReturns the name of the file and remove the provided suffix",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More advanced commands</span>"
    ]
  },
  {
    "objectID": "content/unix/7-unix-redirection.html",
    "href": "content/unix/7-unix-redirection.html",
    "title": "7  Streams, redirection and piping",
    "section": "",
    "text": "7.1 Streams\nUp until now, most of the commands that we have used, printed their output directly to the terminal screen. But what if we want to save that output to a file? Or similarly, if we want to run program X on the output of program Y? This is where redirection and piping come into play. But first, we will have to briefly introduce the concept of streams.\nThe general flow of Unix commands is that we supply a specific input on the terminal, which is supplied to a command, and any output is printed back to the terminal screen. In other words, processes have three different data streams connected to them.\nThe output for most commands, like that of echo, cat and ls, is called the standard output or stdout, and it is printed to our terminal screen by default. However, there exists another output stream in Unix, namely the standard error or stderr. This stream will contain error or warning messages produced by commands, and it is also printed to the terminal screen by default. There also exists an input stream, called standard input or stdin, which provides the data that is fed into a program.\nRedirection and piping allows us to make these data streams go to or come from another file or process, instead of the terminal. Connecting these streams in different combination allows us to perform all kinds of useful operations.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Streams, redirection and piping</span>"
    ]
  },
  {
    "objectID": "content/unix/7-unix-redirection.html#sec-unix-streams",
    "href": "content/unix/7-unix-redirection.html#sec-unix-streams",
    "title": "7  Streams, redirection and piping",
    "section": "",
    "text": "Unix input and output streams\n\n\n\n\n\n7.1.1 Redirecting output\nOne of the most common uses of redirection is redirecting the output to a file. For this, we make use of the greater than operator &gt;:\n$ ls &gt; redirected_output.txt\nsome     files     in   a   directory\n\n$ cat redirected_output.txt\nsome     files     in   a   directory\nIn the example above, the output of ls was not printed to the screen, but redirected to a file named ls_output.txt. Note that if the file does not exist, it will be created for us. However, if the file already exists, it will be overwritten (i.e., its contents will be removed entirely and replaced by our new output).\nA related operator is &gt;&gt;. It will behave similar, with the difference being that &gt;&gt; will instead append its output to existing files, rather than overwriting them.\n$ ls &gt; redirected_output.txt\nsome     files     in   a   directory\n\n$ echo \"A second line!\" &gt;&gt; redirected_output.txt\n\n$ cat redirected_output.txt\nsome     files     in   a   directory\nA second line!\nTechnically, whenever we use redirection, we are targeting a specific stream. Stdout is the default stream, so in the previous examples, &gt; and &gt;&gt; were actually shorthand for 1&gt; and 1&gt;&gt;.\nAs a more concrete example, we can use the redirection operator to store the results of a grep search.\n$ grep \"contig\" ampliseq-variants.vcf &gt; vcf-contigs.txt\n$ cat vcf-contigs.txt\n##contig=&lt;ID=Pf3D7_01_v3,length=640851,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_02_v3,length=947102,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_03_v3,length=1067971,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_04_v3,length=1200490,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_05_v3,length=1343557,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_06_v3,length=1418242,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_07_v3,length=1445207,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_08_v3,length=1472805,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_09_v3,length=1541735,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_10_v3,length=1687656,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_11_v3,length=2038340,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_12_v3,length=2271494,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_13_v3,length=2925236,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_14_v3,length=3291936,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf3D7_API_v3,length=34250,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\n##contig=&lt;ID=Pf_M76611,length=5967,assembly=PlasmoDB-44_Pfalciparum3D7_Genome.fasta&gt;\nIn the example above, we searched for lines containing the word contig in a .vcf file and stored the output in a file called vcf-contigs.txt, instead of just printing the output to the screen. But what is a VCF file anyway?\n\n7.1.1.1 VCF files\n\n\n\n\n\n\nVariant Call Format (VCF)\n\n\n\nVCF is the de facto file format for storing gene sequence variation data. It is a plain text file with tab-delimited columns preceded by header lines with additional metadata (starting with ##), similar to the structure of a BAM file.\nAn example of a VCF file is provided below1:\n\n\n\nDescription of Variant Call Format (VCF)\n\n\nFor now, the most important aspect to remember is that each of the lines in the body of the file store information on the presence of an indel at a particular position in the genome. There are eight mandatory fields for each variant, but the format is flexible and additional fields can be used to store extra information:\n\n\n\n\n\n\n\n\nField\nName\nDescription\n\n\n\n\n1\nCHROM\nThe name of the sequence (typically a chromosome) against which a variant is compared.\n\n\n2\nPOS\nThe reference position.\n\n\n3\nID\nThe identifier of the variant.\n\n\n4\nREF\nThe reference base occurring at this position in the reference sequence.\n\n\n5\nALT\nThe list of alternative alleles found in your samples at this position.\n\n\n6\nQUAL\nA quality score associated with the inference of the given alleles.\n\n\n7\nFILTER\nIndicates which filters the variant has passed. Used as a quality control.\n\n\n8\nINFO\nAdditional info about the variant can be stored here as key-value pairs.\n\n\n\nFor a more in-depth view, we refer to the following excellent resources:\n\nhttps://gatk.broadinstitute.org/hc/en-us/articles/360035531692-VCF-Variant-Call-Format\nhttps://davetang.github.io/learning_vcf_file/\n\nSimilar to SAM/BAM files, there is also a binary version of VCF, namely BCF (Binary variant Call Format). Moreover, like samtools was developed to handle SAM/BAM files, there is dedicated program named bcftools that can be used to work with VCF/BCF files.\n\n\n1 Source: https://vcftools.sourceforge.net/VCF-poster.pdf\n\n\n7.1.2 Redirecting errors\nWithout redirection, most commands print their error and warning messages to the terminal screen. If we use &gt; (or 1&gt;) to redirect the output stream, the stderr will still be printed to the screen (and not be stored in the file that we are redirecting to). To redirect and store the error messages, we need to specify stderr as the stream via 2&lt;.\n# cat on two files normally prints the output of both,\n# but in this case fake_file does not exist\n$ cat real_file fake_file\nfoobar\ncat: fake_file: No such file or directory\n\n# when we redirect stdout to a file\n# stderr is still printed to the screen\n$ cat real_file fake_file &gt; redirected_output.txt\ncat: fake_file: No such file or directory\n\n# when we redirect stderr to a file\n# stdout is still printed to the screen\n$ cat real_file fake_file 2&gt; redirected_errors.txt\nfoobar\nIf we want to redirect both stdout and stderr, we have to use a slightly more complex command:\n# redirect both stdout and stderr to a file\n$ cat real_file fake_file &gt; redirection.txt 2&gt;&1\n\n$ ls redirection.txt\nfoobar\ncat: fake_file: No such file or directory\n\n\n7.1.3 Input redirection\nThe input stream (stdin) can also be redirected. Most commands like cat can open and process a file, but some commands cannot operate directly on a file. Instead, they need to be supplied with data directly. This is where input redirection (&lt;) comes in. We have not yet encountered any commands that need to work on however, so the example below would work equally well without input redirection.\n$ cat &lt; input_file.txt\nlines in\ninput_file.txt\n\n\n7.1.4 Overview of redirection operators\n\n\n\n\n\n\n\nRedirection operator\nResult\n\n\n\n\ncommand &gt; file\nwrite stdout to file, overwriting if file exists\n\n\ncommand &gt;&gt; file\nwrite stdout to file, appending if file exists\n\n\ncommand 2&gt; file\nwrite stderr to file, overwriting if file exists\n\n\ncommand &gt; file 2&gt;&1\nwrite both stdout and stderr to file, overwriting if file exists\n\n\ncommand &lt; file\nread input from file and pass it to command",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Streams, redirection and piping</span>"
    ]
  },
  {
    "objectID": "content/unix/7-unix-redirection.html#piping",
    "href": "content/unix/7-unix-redirection.html#piping",
    "title": "7  Streams, redirection and piping",
    "section": "7.2 Piping",
    "text": "7.2 Piping\nPiping allows us to redirect the output of one command, to the input of another command. It is the more common way of redirecting the input stream. Pipes can chain multiple commands one after another so that a complex series of steps can be run in one go, without any intermediary output files. In its simplest form, piping looks like this:\ncommand_1 | command_2\nWhere the first command produces some kind of output that can be used by the second one. For example, we could pipe the output of ls to grep to search through a list of directories and files:\n$ ls\nPF0080_S44_L001_R1_001.fastq.gz  PF0157_S55_L001_R2_001.fastq.gz  PF0329_S56_L001_R1_001.fastq.gz\nPF0080_S44_L001_R2_001.fastq.gz  PF0275_S68_L001_R1_001.fastq.gz  PF0329_S56_L001_R2_001.fastq.gz\nPF0097_S43_L001_R1_001.fastq.gz  PF0275_S68_L001_R2_001.fastq.gz  PF0512_S47_L001_R1_001.fastq.gz\nPF0097_S43_L001_R2_001.fastq.gz  PF0302_S20_L001_R1_001.fastq.gz  PF0512_S47_L001_R2_001.fastq.gz\nPF0157_S55_L001_R1_001.fastq.gz  PF0302_S20_L001_R2_001.fastq.gz\n\n# search through the list of files in the\n# current directory for a particular sample name\n$ ls | grep \"PF0275\"\nPF0275_S68_L001_R1_001.fastq.gz\nPF0275_S68_L001_R2_001.fastq.gz\nNote that in this case the syntax of grep \"pattern\" &lt;file&gt; changes slightly: we only supply the pattern, and the target file is now replaced by the stdout stream of ls. Piping makes it a lot more convenient to manipulate and chain commands in this manner. Doing the same thing without making use of piping takes a lot more work:\n# write the output of ls to a file\n$ ls &gt; ls_output.txt\n\n$ grep \"PF0275\" ls_output.txt\n\n$ rm ls_output.txt\nClearly, the above approach is not very convenient. Especially if you consider the fact that you can chain as many pipes and redirections as you want: command &lt; input.txt | command | command &gt; output.txt. Let’s take a look at a few more examples:\n\n\n\n\n\n\nTry counting the number of grep matches using a pipe instead of the -c flag. (Click me to expand!)\n\n\n\n\n\n# count the number of matches in grep search results\n$ grep \"pattern\" file | wc -l\ngrep will return a single line for each match that it finds. These lines are passed to the stdin of wc -l, which will count the number of lines.\n\n\n\n\n\n\n\n\n\nHow can we remove non-consecutive duplicate lines from a file? (Click me to expand!)\n\n\n\n\n\n# consider a file with non-consecutive duplicate lines\n$ cat file.txt\na\na\nb\nc\nb\n\n# using uniq will only remove the consecutive duplicate line\n$ uniq file.txt\na\nb\nc\nb\n\n# if we first sort and then run uniq, we get the desired output\n$ sort file | uniq\na\nb\nc\nsort will sort the file alphabetically, which results in all duplicates on consecutive lines. If we then pipe this output into uniq, all duplicates will be removed.\n\n\n\nHere is another example of how to use pipes, this time applied to the SAM files that we saw earlier. SAM files sometimes contain a few lines of additional information - called the header and starting with an @ symbol - before the start of the tab-delimited alignments (1 read per line). If we were to use the cut command to extract a particular column from such a file, the first few lines matching the header would cause problems, because these lines do not correspond to the tabular structure of the rest of the file.\n\n\n\n\n\n\nHow can we use a pipe to only apply the cut -f10 command to only the lines after the header? (Click me to expand!)\n\n\n\n\n\n# count the number of lines in the header\n$ grep -c \"@\" alignment-with-header.sam\n2\n\n# use tail -n +3 to print the output of the file starting\n# the 3rd line, then pipe this output into the cut command\n$ tail -n +3 alignment-with-header.sam | cut -f10\nTTAGATAAAGAGGATACTG\nAAAAGATAAGGGATAAA\nAGCTAA\nATAGCTCTCAGC\nTAGGC\nCAGCGCCAT\naggttttataaaacaaataa\nggttttataaaacaaataatt\nttataaaacAAATaattaagtctaca\nCaaaTaattaagtctacagagcaac\naaTaattaagtctacagagcaact\nTaattaagtctacagagcaacta\nWe use the tail command to first extract the parts of the file that we are interested in, and then feed this output into the cut command to select a particular column (in this case, the 10th column corresponds to the sequence).\n\n\n\nAs a final tip on the usages of piping, consider that you can pipe the output of any command to | less. This is extremely convenient whenever the output of a particular command is too long and does not fit on your terminal screen. Of course, in some situations you are probably better of storing the output in a file using a stdout &gt; redirection.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Streams, redirection and piping</span>"
    ]
  },
  {
    "objectID": "content/unix/7-unix-redirection.html#further-reading",
    "href": "content/unix/7-unix-redirection.html#further-reading",
    "title": "7  Streams, redirection and piping",
    "section": "7.3 Further reading",
    "text": "7.3 Further reading\nRedirection operations and pipes can be combined in many more complex ways than what we saw here. For example, in case we want to redirect output to both a file and the terminal, we can make use of the tee command, as described here. It is even possible to create more complex nested processes, where you feed the output of multiple different commands into a single command: diff &lt;(ls old) &lt;(ls new); this is called process substitution.\nYou do not need to concern yourself with learning these more advanced concepts for the time being, but just keep in mind that whatever you want to do, the Unix shell likely offers a way of doing it.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Streams, redirection and piping</span>"
    ]
  },
  {
    "objectID": "content/unix/7-unix-redirection.html#exercises",
    "href": "content/unix/7-unix-redirection.html#exercises",
    "title": "7  Streams, redirection and piping",
    "section": "7.4 Exercises",
    "text": "7.4 Exercises\n\nSearch for the DNA sequence “aacct” in the truncated human Y chromosome FASTA file and store the output in a file called aacct-hits.txt.\nCount the number of chromosomes in the P. falciparum reference genome fasta file.\nStore the chromosome identifiers of the P. falciparum reference genome fasta file in a file.\nStore the last 40 lines of PF0512_S47_L001_R1_001.fastq in a file named PF0512_S47_L001_R1_001.subset.fastq.\nHow many penguin records are there for each island in penguins.csv? Hint: Try to do it in one go, without grep, by combining multiple pipes (cut, sort and uniq).\nHow can you count the number of unique commands in your command history?\nExtract the header information from ampliseq-variants.vcf, sort it alphabetically, and store it in a file named vcf-header.txt.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Streams, redirection and piping</span>"
    ]
  },
  {
    "objectID": "content/unix/7-unix-redirection.html#summary",
    "href": "content/unix/7-unix-redirection.html#summary",
    "title": "7  Streams, redirection and piping",
    "section": "7.5 Summary",
    "text": "7.5 Summary\n\n\n\n\n\n\nOverview of concepts and commands\n\n\n\n\n\n\nVCF files are used to store genetic variant information\nData streams: stdin, stdout and stderr\nRedirecting output to a file to replace (&gt;) or append (&gt;&gt;)\nRedirecting errors to a file (2&gt;)\nPiping the output of one command to the input of another command via |",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Streams, redirection and piping</span>"
    ]
  },
  {
    "objectID": "content/unix/8-unix-variables-loops-scripts.html",
    "href": "content/unix/8-unix-variables-loops-scripts.html",
    "title": "8  Variables, loops and scripts",
    "section": "",
    "text": "8.1 Variables\nVariables are placeholder names to refer to specific values. You can use them as shortcuts to refer back to a specific value or file path. Moreover, they are easy to set and re-use in bash scripts, which we’ll introduce later. To set a variable, we assign a value to a name using the equals sign =. Afterwards, we can always recall the value via the variable’s name, prefixed with a dollar sign $. While not strictly necessary, it is good practice to also enclose the name of the variable in {}, because it makes creating new variable names a lot easier during scripting.\nAs before, we use spaces around the value that we are assigned to the variable. This allows us to use spaces and other special characters inside our value. The variable in our example was a piece of text (a string), but we can also store things like integers (x=101) or booleans (true/false).\nThe main reason we introduced the concept of variables is because they play an important role in (for) loops, so let us move on to that topic now.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables, loops and scripts</span>"
    ]
  },
  {
    "objectID": "content/unix/8-unix-variables-loops-scripts.html#variables",
    "href": "content/unix/8-unix-variables-loops-scripts.html#variables",
    "title": "8  Variables, loops and scripts",
    "section": "",
    "text": "$ my_value=\"Plasmodium falciparum\"\n\n$ echo ${my_value}\nPlasmodium falciparum",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables, loops and scripts</span>"
    ]
  },
  {
    "objectID": "content/unix/8-unix-variables-loops-scripts.html#loops",
    "href": "content/unix/8-unix-variables-loops-scripts.html#loops",
    "title": "8  Variables, loops and scripts",
    "section": "8.2 Loops",
    "text": "8.2 Loops\nLoops provide a powerful method of repeating a set of operations multiple times. They are integral to automation and being able to process large numbers of samples in bioinformatics pipelines, but also very convenient for performing other tasks like renaming a bunch of files. The idea is a bit similar to the concept of globbing, but loops offer a lot more flexibility and control over the process.\nThe most common type of loops are probably for loops:\n$ for nucleotide in A C T G \\\n&gt; do echo ${nucleotide} \\\n&gt; done\nA\nC\nT\nG\nThere are a number of different things going on here:\n\nThe for loop consists out of three different sections:\n\nfor nucleotide in A C T G: this tells bash that we want to start a for loop. It also defines the range of values that our loop will iterate over, in this case the characters A, C, T, and G. Finally, it creates a new variable called nucleotide. During every pass or round of the loop, its value will change to one of the values defined in the loop’s range.\ndo echo $i: this is the body of the loop. It always starts with do and is then followed by one or more commands. In the body, you can make use of the loop variable $nucleotide.\ndone: this notifies bash that the body and loop definition end here.\n\nThis is the first time that we see a multi-line bash command, where we split across new lines using a backslash symbol (\\). We could have just as well written this statement on a single line (for i in a c t g; do echo $i; done), using colons (;) to mark the end of each section of the loop.\n\nInstead of looping over a set of words, we can also loop over a range of values:\n$ for i in {1..3} \\\n&gt; do echo ${i} &gt;&gt; loop.txt \\\n&gt; done\n\n$ cat for_loop.txt\n1\n2\n3\nAlso note that we used a different name for our loop variable this time around; you can use any name you like, but i is a very common placeholder. To make your scripts easier to read, it is good to stick with a reasonable short, but informative name.\n\n\n\n\n\n\nAs a reminder, what would have happened if we had used &gt; instead of &gt;&gt;? (Click me to expand!)\n\n\n\n\n\nThe for loop for i in {1..3}; do echo ${i} &gt; loop.txt; done is basically equivalent to running the following three commands one after another:\necho 1 &gt; loop.txt\necho 2 &gt; loop.txt\necho 3 &gt; loop.txt\nAs we saw in the previous sections, the redirection &gt; will always overwrite the contents of its destination file. So in this case, the file would only contain the final number of the loop, namely 3. Loops always run in the order defined in their range.\n\n\n\nAnother common for loop pattern is the following one, which is used to loop over a set of files. It combines the for loop syntax with glob patterns (Section 5.3.2):\n$ ls ./directory\nsample_1_R1.fastq   sample_1_R2.fastq   text_file.txt\n\n$ for fq in ./directory/*.fastq; do wc -l ${fq}; du -h ${fq}; done\n582940 ./directory/sample_1_R1.fastq\n67M     ./directory/sample_1_R1.fastq\n462334 ./directory/sample_1_R2.fastq\n54M     ./directory/sample_1_R2.fastq\nThere are a few important things to note here:\n\nThe glob pattern ./directory/*.fastq will be expanded by the shell to a list of all files ending with .fastq. Consequently, the for loop will only iterate of the FASTQ files and the .txt file is ignored.\nInside the execution statement of the loop, the current file is referred to via the variable ${fq}.\nWe used a semicolon (;) to write the loop statement on a single line.\nUnlike the previous example, the body of this for loop contains multiple commands: first the filename is printed to the screen using echo, then the number of lines in the file is printed to the screen.\n\n\nLoops, combined with scripting, are incredibly useful when performing more advanced operations, like performing the bioinformatics analysis of DNA sequencing reads. For example, to process the DNA reads generated by an AmpliSeq assay and identify the genetic variants (variant calling analysis), the following steps would be performed by looping over the FASTQ file corresponding to each sample:\nfor every FASTQ file:\n    1. Perform a quality control step\n    2. Map the reads to the reference genome\n    3. Call the variants in the alignment\n\nThere actually exists another type of loop, namely the while loop. These behave similar, but instead of going through a list of items or a range of numbers, the loop will continue for as long as a certain condition is met. You can find more information here in case you are interested.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables, loops and scripts</span>"
    ]
  },
  {
    "objectID": "content/unix/8-unix-variables-loops-scripts.html#shell-scripts",
    "href": "content/unix/8-unix-variables-loops-scripts.html#shell-scripts",
    "title": "8  Variables, loops and scripts",
    "section": "8.3 Shell scripts",
    "text": "8.3 Shell scripts\nAll the topics that we have covered so far, were performed interactively on the command line. However, we can also write scripts that contain a series of commands, loops and variables, which can be executed in one go. That way, you can queue up a bunch of long-running processes and don’t need to stick around to start up each next step in the process. Moreover, it allows you to reuse the same set of operations in the future. Scripts can even be written in such a way that they can be called using different options, similar to how we can provide different optional arguments to bash commands.\n\n8.3.1 Creating a bash script\nShell scripts are nothing more than executable text files written in a specific scripting language, in our case bash. We can write bash scripts in any type of text editor (like Notepad or VS Code), but we can also do it directly on the command line, by making use of an editor like nano or vim.\nThe only requirements for bash scripts is that the first line contains a shebang directive, like #!/usr/bin/env bash. When the script is executed, this line tells your machine to run the script using the bash interpreter (i.e., that it is a bash script and needs to be treated accordingly). By convention, shell scripts are saved with the .sh extension.\n\n\n8.3.2 Running scripts\nA very simple script might look a bit like this:\n#!/usr/bin/env bash\n\necho \"My first script!\"\nInside a script, you can use any valid bash statement that would work on the command line. This includes all the commands we introduced up until now, structures like for loops, output redirection, pipes, etc.\nTo run a bash script, you can simply execute the bash command and point it to a script. If we save the two lines above to script.sh file and then executing it by running bash script.sh, the single command inside of it will be executed and printed to the screen:\n$ bash script.sh\nMy first script!\nBelow is a slightly more complex example:\n#!/usr/bin/env bash\n\necho \"This is an example script.\"\n\necho \"The script was executed from the directory: \"\npwd\n\n# this is a comment\n\necho \"Running a for loop\"\n\nfor i in {1..5}\n        do echo i\ndone\n\necho \"This is a grep command\"\n\n# long commands can be split over multiple lines\n# this grep command counts the number of times \"tttataaaaaaac\"\n# occurs in the current directory and all of its subdirectories\n# while ignoring case\ngrep -i \\\n        -r \\\n        \"tttataaaaaaac\" \\\n        .\n\n\n\n\n\n\nTip\n\n\n\nNote that the indentation that we used is not strictly necessary for loops to work, but it does help with legibility and it is common practice to do this, especially in scripts.\nYou can use hashes (“#”) to comment out a line. Use this to describe what your code is doing. Your future you will be grateful!\nYou can also use backslash (“”) to split a long command over multiple lines, making it easier to read your script. E.g., for listing -options on consecutive lines.\n\n\nIf we save and run the script above, we get the following output.\nThis is an example script.\nThe script was executed from the directory:\n/home/pmoris/itg/FiMAB-bioinformatics/training/unix-demo\nThis is a for loop\ni\ni\ni\ni\ni\nThis is a grep command\n./PF0512_S47_L001_R1_001.fastq:CTAACTACAATGAAGACAAAAATATTATGTATATGTACCCAAATGAACCAAATTATAAGGATTCCAAAAAAGTATTATCTCAAAAAAAAAAAAAAAAAATCCACCATACATCATTTTCATCGTATTAATTCCCATGGACCACCTACACATGTGCAATTTATAAAAAAACAACAATCCCACTATCTCTAATACACATCTCCGAACCCACGAGACGCCGGACAATACCGTTTGCCAGCTCCCCGTACAATAAACCAATACTAAGATCATTGCCTCACTCTGAATCGCAGAACTCTGACGTATA\n./PF0512_S47_L001_R1_001.fastq:CTAACTACAATGAAGACAAAAATATTATGTATATGTACCCAAATGAACCAAATTATAAGGATTCCAAAAAAGTATTATCTCAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAATTAAAAAAAAAAAATAAAAAAAAAAAAAATAAAAAAAAAAAAAAAAAAAAAAAAAATAAAAAAAAAAAAATAAAAAAAAAAAAAAAAAAAAAACAAAAACACAACAAAAACAAAAAATAAAATATATATTTATAAAAAAACAAAAAAAAGAAAACAACACAGACACTCAAACAACAACACACCA\n./Homo_sapiens.GRCh38.dna.chromosome.Y.truncated.fa:TGAGATTGGATTTTTAAACATTAATATGGCGTGTTACATTTATAAAAAAACCCCAAAGAT\n./my_script.sh:# this grep command counts the number of times \"tttataaaaaaac\"\n./my_script.sh: \"tttataaaaaaac\" \\\nExiting script\n\n\n\n\n\n\nCan you spot what is wrong with this script? (Click me to expand!)\n\n\n\n\n\nThe output of the for loop section of the script is five lines of the character “i”, which is not what we wanted. The problem is that inside of the body of the for loop, we used the character “i”, instead of referencing the for loop variable using ${i}.\n\n\n\nLastly, note that not all scripts are bash scripts, you can also create R, Python or other types of scripts.\n\n\n8.3.3 Making scripts executable\nIn the previous examples we ran scripts by invoking them using the bash command. However, we can also make the file executable by using the chmod command. Once a file has been marked as executable for a given user, it can be run by simply typing the file name, without any other command in front.\n$ ls -l my_script.sh\n-rw-r--r-- 1 pmoris pmoris 488 Jan  4 14:35 my_script.sh\n\n$ chmod +x my_script.sh\n\n$ ls -l my_script.sh\n-rwxr-xr-x 1 pmoris pmoris 488 Jan  4 14:35 my_script.sh\n\n$ my_script.sh\n&lt;script output&gt;\nYou can read more about file permissions in Section A.5.\n\n\n\n\n\n\nDifferent methods of running script\n\n\n\nThere are actually a few different ways to execute a shell script:\n\nPrefixing the interpreter: bash path/to/script.sh. For this method, we explicitly execute the bash command and point it to the location of a script.\nDirectly running a script in the current working directory: ./script.sh. This option has two requirements: first, the file needs to be made executable (as we saw above). Furthermore, as a safety precaution, we need to prefix the name of the file with ./, to let the system know that we are trying to run a script that resides in the current working directory, as opposed to one that is globally accessible.\nDirectly running a globally accessible script: script.sh. This option also requires the file to be executable, but on top of that it will only work for files that are found in the list of directories making up your PATH (see Section A.4). These are pre-defined (or custom) directories like /usr/bin that usually store all the common commands that we have used until now, as well as any other software that you might install yourself.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables, loops and scripts</span>"
    ]
  },
  {
    "objectID": "content/unix/8-unix-variables-loops-scripts.html#overview-of-variant-calling-pipeline",
    "href": "content/unix/8-unix-variables-loops-scripts.html#overview-of-variant-calling-pipeline",
    "title": "8  Variables, loops and scripts",
    "section": "8.4 Overview of variant calling pipeline",
    "text": "8.4 Overview of variant calling pipeline\nThe chart below provides a high-level overview of the steps involved in a basic variant calling pipeline. Throughout the course, we have already introduced some of the file formats that are used (green). We will explore these steps and the associated tools, in more detail during the in-person courses later on.\n\n\n\nVariant calling pipeline\n\n\nFor now though, we have included a few example scripts that run through the above steps, to showcase how the building blocks that we have learned so far can be used to orchestrate a more complex analysis.\n\n\n\n\n\n\nNote\n\n\n\nInspect the scripts stored in ./training/scripts and try to make sense of the general steps that they describe. You can gloss over the specifics of what the specialized commands like fastqc or bwa mem do; instead, try to focus on the general structure and syntax of the scripts, the use of patterns like for loops, directory navigation, how arguments are provided to commands, etc.\n\n\n\n\n\n\n\n\nTip 8.1: What does the following line do? sample_name=$(basename ${read_1} _R1_001.fastq.gz)  Hint: $(command) provided a method to run a command inside another statement, so try figuring out what the command between the brackets does first.\n\n\n\n\n\nThis line is present in most of the example scripts inside of a for loop that iterates over a set of FASTQ files, each time processing a pair of R1/R2 files.\n# first navigate to the directory containing fastq files\ncd ./training/data/fastq/\n\n# loop over pairs of fastq files\nfor read_1 in *_R1_001.fastq.gz\ndo\n    sample_name=$(basename ${read_1} _R1_001.fastq.gz)\n...\nDuring each iteration of the loop ${read_1} will correspond to the file path of a specific FASTQ file.\nWhen we call basename on it with the extra argument _R1_001.fastq.gz, we will receive back the name of the file with that suffix removed. E.g.:\n$ basename PF0080_S44_L001_R1_001.fastq.gz _R1_001.fastq.gz\nPF0080_S44_L001\nThe last step we do is running this command inside a command substitution: $(command). When using a command substitution, the output of the command inside the brackets will just be passed along to the command line. In our case, we try to assign the value $(basename ${read_1} _1.fastq.gz) to a new variable named sample_name. The value will then contain the output of its command substitution, namely PF0080_S44_L001.\nAfter running the above statement, we now are able to more easily construct the name of the first and second read pair:\n$ echo ${read_1} ${sample_name}_R2_001.fastq.gz\nPF0080_S44_L001_R1_001.fastq.gz PF0080_S44_L001_R2_001.fastq.gz\n\nThe first read we already had, but to create the second one we concatenate the sample name with the new suffix _R2_001.fastq.gz.\nPaired sequence data is usually named in such a way that it allows to access pairs of files in this way.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables, loops and scripts</span>"
    ]
  },
  {
    "objectID": "content/unix/8-unix-variables-loops-scripts.html#a-few-final-useful-scripting-concepts",
    "href": "content/unix/8-unix-variables-loops-scripts.html#a-few-final-useful-scripting-concepts",
    "title": "8  Variables, loops and scripts",
    "section": "8.5 A few final useful scripting concepts",
    "text": "8.5 A few final useful scripting concepts\n\n8.5.1 Command substitution\nIn the callout box above we already introduced the concept of using command substitutions and the basename command to manipulate filenames and paths during a for loop, in order to be able to iterate over pairs of FASTQ files with similar, but predictable file names (e.g., read_R1_001.fastq and read_R2_001.fastq). To reiterate, the general concept is that we only loop over R1 of each pair, use basename to extract the filename without the R1_001.fastq.gz suffix and without the preceding file path prefix, and then use this to construct the known file path to R2.\nfor read_1 in *_R1_001.fastq.gz\ndo\n    sample_name=$(basename ${read_1} _R1_001.fastq.gz)\n\n    command ${read_1} \"/file/path/to/${sample_name}_R2_001.fast.qz\"\n...\nA simpler example would be letting echo output the result of another command, like date:\necho \"Today is $(date).\"\nYou can also use it to store the output of a command in a variable, like we did for basename:\ncount=$(grep -c \"#\" variants.vcf)\n\n\n8.5.2 Parameter expansion - manipulating strings\nBash contains all sorts of tricks to manipulate strings of text. These can be used in a similar way to basename, in order to strip of file extensions, parts of names or file paths and many other more complex tasks. For more details, we refer to https://mywiki.wooledge.org/BashFAQ/073.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables, loops and scripts</span>"
    ]
  },
  {
    "objectID": "content/unix/8-unix-variables-loops-scripts.html#exercises",
    "href": "content/unix/8-unix-variables-loops-scripts.html#exercises",
    "title": "8  Variables, loops and scripts",
    "section": "8.6 Exercises",
    "text": "8.6 Exercises\n\nCreate a for loop over the files in ./training/unix-demo/files_to_loop_through that prints the first line of each file to the screen.\nCreate a bash script that does the same thing.\nOn a single-line, run the script, but sort its output in reverse order (check sort --help to check how) and store the new output in a file called loop_sort.txt.\nCreate a bash script with a for loop that prints the name of each read file in the ./training/data/fastq directory.\nModify the previous bash script so that it also 1) creates a single new directory named fastq_meta and 2) creates a new file in that directory, one for each FASTQ file, which contains two lines: the first with the number of lines in the FASTQ file and the second with its file size.\nCreate a bash script with a for loop that prints the sample name for each pair of reads in the ./training/data/fastq directory (i.e., half as many names as in the previous exercise).\nCreate a bash script that:\n\nCounts the number of header lines in ./training/unix-demo/ampliseq-variants.vcf and store this number as a variable.\nExtracts the contents of the VCF file after the header lines (i.e., the tabular section) and store it in a separate file.\nCreates a for loop to extract the first eight columns and store them each in a separate file named vcf_column_#.txt (where # is the column number).",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables, loops and scripts</span>"
    ]
  },
  {
    "objectID": "content/unix/8-unix-variables-loops-scripts.html#summary",
    "href": "content/unix/8-unix-variables-loops-scripts.html#summary",
    "title": "8  Variables, loops and scripts",
    "section": "8.7 Summary",
    "text": "8.7 Summary\n\n\n\n\n\n\nOverview of concepts and commands\n\n\n\n\n\n\nVariables can be assigned via name=value and referenced via ${name}\nFor loops are used to iterate over a list of items or files\nScripts can be used to combine multiple commands into a single set of instructions that can be re-used.\nManipulating file names during a loop can be done using basename or parameter expansion.\nCommand substitutions ($(command)) can be used to insert the output of a particular command (e.g., basename) into the middle of other commands or during the construction of new strings of text.",
    "crumbs": [
      "Introduction to the Unix shell",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables, loops and scripts</span>"
    ]
  },
  {
    "objectID": "content/r/r-intro.html",
    "href": "content/r/r-intro.html",
    "title": "9  R introduction",
    "section": "",
    "text": "As your first introduction to R, we will make use of Data Carpentry’s Intro to R and RStudio for Genomics course. This self-paced workshop walks through the basics of R - and its most popular IDE, RStudio - in the context of genomics. The focus lies on the basic syntax of R, wrangling tabular data using the tidyverse - where we will encounter the VCF file format once again (Section 7.1.1.1) - and visualization using ggplot2.\nThe Data Carpentry workshop will sometimes refer to running RStudio in a cloud environment with pre-installed packages and files, but when going through these materials on your own, we recommend installing R and RStudio on your own machine. You can do so by following the instructions listed here:https://rstudio-education.github.io/hopr/starting.html. You will also need to download the following two files, which are used throughout the workshop: combined_tidy_vcf.csv and Ecoli_metadata.xlsx.\nIf you do run into trouble installing R and RStudio on your own computer, you could instead:\n\nmake use of the free version of Posit Cloud (formerly known as RStudio Server): https://posit.cloud/plans/free. However, note that you only receive a limited number of hours of use as a free user.\nAlternatively, you can again make use of GitHub codespace, but you will need to select a different variant this time: RStudio Server.\nAs a final option, you could make use of a binder RStudio instance, such as this one.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R introduction</span>"
    ]
  },
  {
    "objectID": "content/r/r-resources.html",
    "href": "content/r/r-resources.html",
    "title": "10  Other R resources",
    "section": "",
    "text": "10.1 Cheatsheets\nHere is a collection of other useful resources for learning R.\nEveryone loves cheatsheets!",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Other R resources</span>"
    ]
  },
  {
    "objectID": "content/r/r-resources.html#cheatsheets",
    "href": "content/r/r-resources.html#cheatsheets",
    "title": "10  Other R resources",
    "section": "",
    "text": "ggplot2\ndplyr data transformations\ndata tidying using tidyr and tibble",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Other R resources</span>"
    ]
  },
  {
    "objectID": "content/genomics/setup.html",
    "href": "content/genomics/setup.html",
    "title": "Setup and preparations",
    "section": "",
    "text": "For this hands-on practical on processing genomics data for variant calling, we will make use of the same GitHub Codespaces that were introduced in 2  Setting up your own Unix shell.\nDue to the large file size of the training data, not all files were included directly in the repository. When you launch your codespace for the first time (or whenever you create a new one), you will need to run the download scripts that are stored in ./training/data/, as described in the README.md.\nIf you want to follow along on your own machine instead, make sure to install all the required software tools. The easiest option is to use conda/mamba via the miniforge installer, and then installing all the packages described in the environment.yml file. For R, simply install the list of packages described in the devcontainer.json file (or renv file if it is available).\nOn Linux:\n# download and install miniforge\nwget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\nbash Miniforge3-Linux-x86_64.sh\n\n# install relevant packages into a new environment\nmamba env create -n fimab -f ./FiMAB-bioinformatics/environment.yml\n\n# activate environment\nmamba activate fimab\nOn Windows:\n\nDownload and run the latest Windows installer\nOn your command line, run:\n# install relevant packages into a new environment\nmamba env create -n fimab -f ./FiMAB-bioinformatics/environment.yml\n\n# activate environment\nmamba activate fimab",
    "crumbs": [
      "Processing genomic data",
      "Setup and preparations"
    ]
  },
  {
    "objectID": "content/genomics/pipeline-overview.html",
    "href": "content/genomics/pipeline-overview.html",
    "title": "11  Pipeline overview",
    "section": "",
    "text": "11.1 Overview of variant calling pipeline\nThe chart below provides a high-level overview of the different steps involved in the processing of genomic data. In the following chapter we will focus on the variant calling pipeline, which deals with:\nIn the diagram below the various tools (e.g., bwa, samtools, gatk) and data formats (fastq, bam, vcf, etc.) are depicted as well. Some of these we have already encountered in the previous chapters. You might also recall the pipeline scripts (stored in ./training/scripts) that were mentioned in the chapter on Unix scripting. These already showcased a number of the different steps involved in the pipeline.",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Pipeline overview</span>"
    ]
  },
  {
    "objectID": "content/genomics/pipeline-overview.html#overview-of-variant-calling-pipeline",
    "href": "content/genomics/pipeline-overview.html#overview-of-variant-calling-pipeline",
    "title": "11  Pipeline overview",
    "section": "",
    "text": "Assessing the quality of sequence reads and filtering if necessary.\nAligning the reads to a reference genome - this is also referred to as read mapping.\nIdentify variants, i.e. where do my samples differ from the reference genome - this is known as _variant calling.\n\n\n\n\n\nVariant calling pipeline\n\n\n\n\nA more simplified overview of the pipeline can be seen here:\n\n\n\n\nVariant calling pipeline - simplified\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe will mostly focus on the AmpliSeq pipeline available here as it was introduced by Kattenberg et al. (2022) and Kattenberg et al. (2023).\nFor the whole-genome sequencing parts, while largely similar, we try to adhere to standard conventions established by examples like the GATK best practices and MalariaGEN’s genomic databases (2023).",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Pipeline overview</span>"
    ]
  },
  {
    "objectID": "content/genomics/pipeline-overview.html#additional-resources",
    "href": "content/genomics/pipeline-overview.html#additional-resources",
    "title": "11  Pipeline overview",
    "section": "11.2 Additional resources",
    "text": "11.2 Additional resources\nWe have collected a number of related resources below, which have served as an inspiration while preparing our own course:\n\nhttps://genomics.sschmeier.com/\nhttps://training.galaxyproject.org/training-material/topics/sequence-analysis/\nhttps://www.melbournebioinformatics.org.au/tutorials/tutorials/variant_calling_gatk1/variant_calling_gatk1/\nhttps://speciationgenomics.github.io/\nhttps://ngs-course.readthedocs.io/en/praha-november-2023/\nhttps://people.duke.edu/~ccc14/duke-hts-2018/index.html\nhttps://biomedicalhub.github.io/genomics/\nhttps://hbctraining.github.io/variant_analysis/schedule/self-learning.html\nhttps://datacarpentry.org/wrangling-genomics/\nhttps://mmbdtp.github.io/modules/sequencing/week-2-programme/\nhttps://eriqande.github.io/eca-bioinf-handbook/overview-of-bioinformatic-analyses.html\nhttps://rnnh.github.io/bioinfo-notebook/#contents\nhttps://learn.gencore.bio.nyu.edu/\nhttps://conmeehan.github.io/PathogenDataCourse/PathogenCourse.html\nhttps://bio-courses1.rsb.anu.edu.au/biol3161-data/ReadsToVariants.html\n\nThe video below by Tobias Rausch @ EMBL-EBI (Rausch 2022) also provides an excellent overview of various topics that come up in genomic variant calling, but it is broader in scope than just AmpliSeq or molecular surveillance of parasites:\n\n\n\n\n\nKattenberg, Johanna Helena, Carlos Fernandez-Miñope, Norbert J. Van Dijk, Lidia Llacsahuanga Allcca, Pieter Guetens, Hugo O. Valdivia, Jean-Pierre Van Geertruyden, et al. 2023. “Malaria Molecular Surveillance in the Peruvian Amazon with a Novel Highly Multiplexed Plasmodium Falciparum AmpliSeq Assay.” Edited by Gemma Moncunill. Microbiology Spectrum 11 (2): e00960–22. https://doi.org/10.1128/spectrum.00960-22.\n\n\nKattenberg, Johanna Helena, Hong Van Nguyen, Hieu Luong Nguyen, Erin Sauve, Ngoc Thi Hong Nguyen, Ana Chopo-Pizarro, Hidayat Trimarsanto, et al. 2022. “Novel Highly-Multiplexed AmpliSeq Targeted Assay for Plasmodium Vivax Genetic Surveillance Use Cases at Multiple Geographical Scales.” Frontiers in Cellular and Infection Microbiology 12 (August): 953187. https://doi.org/10.3389/fcimb.2022.953187.\n\n\nMalariaGEN, Muzamil Mahdi Abdel Hamid, Mohamed Hassan Abdelraheem, Desmond Omane Acheampong, Ambroise Ahouidi, Mozam Ali, Jacob Almagro-Garcia, et al. 2023. “Pf7: An Open Dataset of Plasmodium Falciparum Genome Variation in 20,000 Worldwide Samples.” Wellcome Open Research 8 (January): 22. https://doi.org/10.12688/wellcomeopenres.18681.1.\n\n\nRausch, Tobias. 2022. “Methods in Genomic Variant Calling.” EMBL-EBI. https://doi.org/10.6019/TOL.GenomicVariantCalling-w.2022.00001.1 .",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Pipeline overview</span>"
    ]
  },
  {
    "objectID": "content/genomics/tools-and-formats.html",
    "href": "content/genomics/tools-and-formats.html",
    "title": "12  Bioinformatics tools and file formats",
    "section": "",
    "text": "12.0.1 FASTA: Biological sequences\nBelow we have created a list of the various tools that are used for the steps in this pipeline. As a disclaimer: a common occurrence in bioinformatics is that several different approaches have been developed for a particular task or analysis, leading to a large (and sometimes confusing) ecosystem of different software tools, each with their own advantages, disadvantages and learning curve. In the course we will try to focus on one representative tool for each task, but be aware that these are not the only ones and there might exist more suitable options depending on your data and use cases.",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bioinformatics tools and file formats</span>"
    ]
  },
  {
    "objectID": "content/genomics/tools-and-formats.html#tools",
    "href": "content/genomics/tools-and-formats.html#tools",
    "title": "12  Bioinformatics tools and file formats",
    "section": "12.1 Tools",
    "text": "12.1 Tools\n\nFastQC: Section 13.3\nFastQ Screen: Section 13.4\nMultiQC: Tip 13.3\nTrimmomatic: Section 13.5\nBWA: Section 14.3\nsamtools: Section 14.4, Section 14.6.1 and Section 14.6\npicard: Section 14.7 and Section 14.8\nGATK: Chapter 15\nIGV: Section 14.9 and Section 15.4",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bioinformatics tools and file formats</span>"
    ]
  },
  {
    "objectID": "content/genomics/qc.html",
    "href": "content/genomics/qc.html",
    "title": "13  Quality control and trimming",
    "section": "",
    "text": "13.1 FASTQ file format revisited\nThe first step in our pipeline deals with assessing the quality of our sequence reads and when necessary cleaning them. The reads are provided to us by the sequencer in the form of FASTQ (or fastq) files. We already introduced this file format in a previous chapter (Section 5.1.3.1), but we will dive in a bit deeper this time around.\nThe FASTQ file format holds the raw sequencing reads generated by next-generation sequencing technologies. For the Illumina platform, they contain the sequence data from the clusters on the flow cell (after some initial filtering steps) and these are the types of files that you would receive after a sequencing run. FASTQ files look superficially similar to FASTA files, in the sense that they hold sequence of nucleotides (strings of ACTG characters) preceded by an identifying header (&gt; ...), but they also contain additional information that encodes the base quality scores.\nFASTQ files are plain-text files consisting of the following four lines for each sequence read:\nFASTQ files generally contain many sequences, each of which might look something like this:\nThe header line always starts with an @ and usually carries information on the sequencing run, cluster on the flow cell or any other metadata. Note however that this format is not entirely fixed and depending on how and where your sequences were generated, the headers might look slightly different. In this case, the read was sequenced on an Illumina 1.8 sequencer, and we can identify the following types of metadata, separated by colons : and forward slashes / as field separators:\nThe second line contains the sequence as a string of characters. The third line is basically a historical artifact that we’re now stuck with. The fourth line, containing the quality scores, will be explored in more detail in the next section (Section 13.2).\nFASTQ files are typically named using the following convention:\nAlso, note that there is no standard file extension for FASTQ files, both .fq and .fastq are common. Keep in mind that file extensions in general are arbitrary; it is the file content that determines the file format, not the extension (although both Windows and Mac might try to convince you otherwise). However, using standard and descriptive file extensions makes everyone’s life much easier.\nFASTQ files can typically contain up to millions of reads (depending on the depth of sequencing), resulting in file sizes in the range of hundreds of MBs to multiple GBs. However, the files are usually compressed using gzip (receiving the .fastq.gz extension) to reduce their file size.\nThe raw FASTQ files forms the basis on which your analysis builds. No matter what analysis you might perform, you should be able to reproduce it if you can start from the same raw reads. It is therefor of utmost importance to properly manage these files and store them in a secure location, preferably backed up.\nA sequencing run will generally produce either a single FASTQ file per sample (for single-end runs) or a pair of files per sample (for paired-end sequencing). For the latter, the files will bear identical file names, except for the suffix to distinguish them easily (and programmatically), usually either _1/_2 or R1/R2. E.g., the two files might be called SampleName_S1_L001_R1_001.fastq and SampleName_S1_L001_R1_001.fastq. In some circumstances you might end up with a third file (either without a suffix or tagged as _3), e.g., containg unpaired reads.\nFor deep sequencing, you might receive multiple files for each sample split across different lanes of the sequencer, in which case you might see files such as SampleName_S1_L001_R1_001.fastq.gz, SampleName_S1_L002_R1_001.fastq.gz. The lane descriptor is also important in the case of multiplexed samples (see Section 13.1.1).",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quality control and trimming</span>"
    ]
  },
  {
    "objectID": "content/genomics/qc.html#sec-fastq-in-depth",
    "href": "content/genomics/qc.html#sec-fastq-in-depth",
    "title": "13  Quality control and trimming",
    "section": "",
    "text": "FASTQ files and Illumina sequencing\n\n\n\nA more in-depth explanation on FASTQ files can be found on the Illumina website.\nIf you need a refresh on the Illumina sequencing technology, i.e. cluster generation on flow cells and sequencing-by-synthesis, you can also check out the following resources:\n\nShort video by ClevaLab (ClevaLab 2022b) - also available as a blog post (ClevaLab 2022a)\nNext-Generation Sequencing Glossary on the Illumina website (n.d.)\n\nthis glossary is especially helpful to brush up on terms like cluster, insert, adapters and multiplexing. \n\nIn-Depth NGS Introduction by Illumina (2017)\nLonger video on Illumina platforms (40 min) by Illumina (2021a)\nReviews on NGS technologies:\n\nDNA sequencing at 40: past, present and future (Shendure et al. 2017)\nThe chemistry of next-generation sequencing (Rodriguez and Krishnan 2023) \n\nExcellent blogpost by Loren Launen (accompanying video lectures are available too) (Launen 2017)\nShort course on MiSeq system by Illumina (focus on imaging and base calling)\n\n\n\n\n\n\n\n\nLibrary preparation and bridge amplification on flow cell.\n\n\n\n\n\nSequencing-by-synthesis and fluorophore imaging or base calling.\n\n\n\n\nFigure 13.1: Overview of Illumina sequencing chemistry (Source: Rodriguez and Krishnan 2023).\n\n\n\n\n\n\n\n\n\n\n\n\n\nLine\nDescription\n\n\n\n\n1\nIdentifier or header: always starts with ‘@’ and contains information about the read (free format)\n\n\n2\nThe sequence of nucleotides making up the read\n\n\n3\nAlways begins with a ‘+’ and sometimes repeats the identifier\n\n\n4\nContains a string of ASCII characters that represent the quality score for each base (i.e., it has the exact same length as line 2)\n\n\n\n\n@M05795:43:000000000-CFLMP:1:1101:21302:1790 1:N:0:44\nCCTCACCATGCAACCGATGCTCATCATGCAGCCGATGCTCATCATTCTCATCATGCACCCGCTGTCTCTTTTACACTTCTCCTTTCCCACGTCTCCCTTCTTTTTCTCCTATCCCTTCTTCTTCTTTCCCTCTTTTTTCTTTTTTCTTTTTCTCTTTTTTTCTTTTTTTCCCCTTTCTCTCTCTCTTTTCCTCTTCTTCTTCTTCTCTTCCTTCCCCCCCCCTTCCCTCTCCCCTCTCTCTTCCTCTTTCCCCCTTCTTTCCTTCCCCCCTCCCCCTTCCCTCCCCTTTCATCTCTTCCT\n+\n-AACCFGGGGGGGGFEGGCFGGCCFGGGGGGFDCCFGGEF9FA9F,6;C,CC,E,6,,;,++6+B6,&lt;CE,6,6,9,95,,5,,,,,5,4+,,,,,9,,,,,5,5,9,,9,9,45,944,,5,4,9,,,,,,,,,,,+,,,,,,+,,,,,,,,,,,,,,++,,3,,,++,,,,,,,,,,,+++++++++++++1+++++++++++++++++++0****/())()/(()()(((((((())))))))))))))((((,))))))-),(((((((,((((((()(((((())-)-)))))))\n\n\n@&lt;instrument&gt;:&lt;run number&gt;:&lt;flowcell ID&gt;:&lt;lane&gt;:&lt;tile&gt;:&lt;x-pos&gt;:&lt;y-pos&gt; &lt;read&gt;:&lt;is filtered&gt;:&lt;control number&gt;:&lt;barcode sequence&gt;\n\n@: start of the header\nM05795: unique sequencing instrument ID\n43: run number\n000000000-CFLMP: unique flow cell ID\n1: lane number (see below for more info on lanes)\n1101: tile number (= section of the lane)\n21302: x-coordinate of the cluster\n1790: y-coordinate of the cluster\n1: read number - 1 or 2 for paired-end sequencing\nN: not filtered\n0: control number\n44: index/barcode sequence (see below for more info on barcoding and multiplexing)\n\n\n\n\n\n\nSampleName_S1_L001_R1_001.fastq.gz\nwhere:\n\nSampleName is the name of the sample as it was provided in the samplesheet during submission, or a unique sample ID otherwise.\nS1 is the sample number, again based on the samplesheet.\nL001 is the lane number on the flow cell.\nR1 is the read pair - 1 or 2 for paired-end sequencing (or I1/I2 for index reads).\n001is always the last part of the name.\n\nPutting it all together, we end up with:\n{sample_name}_S{sample number}_L{lane number}_{R/I}{read or index number}_001.fastq.gz\n\n\n\n\n\n\n\nWarning\n\n\n\nKeep in mind that these are conventions, not strict rules, so you will likely encounter file names that differ from the above.\nSimilar caveats apply to the header lines inside FASTQ files.\n\n\n\n\n\n\n\n\n\n\n\nCompress your files!\n\n\n\nThroughout this pipeline (and in many other bioinformatics analyses) you should avoid using and storing uncompressed files, because they waste disk space. Moreover, many of the formats we will talk about are also faster to analyse in binary compressed form compared to the plain-text versions (e.g., BAM vs SAM, see Section 6.2.1 and Section 6.4.1).\nRefer to Section 6.4 or this overview for a refresher on file compression.\n\n\n\n\n\n\n\n\n\n\n\nTip 13.1: Paired-end sequencing\n\n\n\nIn a nutshell: in paired-end sequencing, both ends of the DNA fragments in the library are sequenced inwards (5'-F===&gt;_______&lt;===R-3') one after the other (the Forward and Reverse read).\nThe forward and the reverse reads do not necessarily overlap; this depends on the length of the insert and the number of sequencing cycles used for each round of sequencing. Indeed, in many cases the forward and reverse reads are a couple of hundred bases apart, pointing towards each other, because the DNA insert is longer (e.g., ~300 bp) than the forward or reverse read (e.g., 150 bp each, including read primers). So to emphasize, we do not expect the forward and reverse reads to cover identical sections of the fragment.\nWhat is the point of paired-end sequencing? Well, because we know what the average distance between the forward and reverse read is 1, alignment algorithms can make use this information to more accurately map the reads across repetitive regions in the genome (see Figure 13.2), which would otherwise be even more problematic.\n\n\n\n\n\n\nFigure 13.2: Paired-end sequencing and alignment. (Source: Illumina 2017)\n\n\n\nThese paired reads are usually results in two separate files; read 1 and read 2 R1/R2 (not to be confused with mate-pairs).\nThe forward and reverse read pairs are split across these two files and they always appear in the same order (or at least this is what most downstream tools expect).\n\n\n1 During library construction, we target a specific size distribution or range of DNA inserts, either through the fragmentation process, or in the case of AmpliSeq, by directly generating amplicons of known specific sizes.\n\n\n\n\n\nInspect the first lines of two paired FASTQ files\n\n\n\n\nCan you tell by the headers that the sequences are paired and appear in the same order?\nCheck the sequence length of each pair and explain why they are (not) the same based on the sequencing process.\nDo the sequences appear to overlap?\n\nYou can find paired FASTQ files in the data directory of the training environment.\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\nThe following bash snippets prints the first reads from two paired FASTQ files:\n# print the first 4 lines (= 1 read) of read 1 and read 2 to the console\nfor fq in ./data/fastq/PF0097_S43_L001_R*_001.fastq.gz; do\n    echo \"Read ${fq}\"\n    zcat ${fq} | head -n4\n    echo\ndone\n\n# note that a command like the one below will not work! Why?\nzcat training/data/fastq/PF0097_S43_L001_R*_001.fastq.gz | head\nOutput:\nRead ./data/fastq/PF0097_S43_L001_R1_001.fastq.gz\n@M05795:43:000000000-CFLMP:1:1101:15977:1738 1:N:0:43\nCACAATCAATATCATTATTATTTTTTTTTTTTTTTTTTTTTTTTTTTTTTCTTTTTTTTTCTTTTTCCTTTTTTCTTTTTTTCTTTTTTTCTTTTCCTTTTTTTTTTTCTTTTTCCAATTTATTTTTTTTTTTTTTTTTTTTTTTATTCTTTTTTTTTTCCTTTTTTTTTTTTTTCTTATTTTTTTTTACTTCTTTCTATATTTTATTTCCTTTTCCTCTTTTCCTTTTTCCAATTTTTTTTTCCCCTTTTCCTTTTCCCCTTTTCCTTATTCCTTCATTTTTTTTTTTACCTCCCACCT\n+\nA8BCCGGFFGFGGCGGFFG&lt;FGGGGGGGGGGD77+@7+@::CC+=@+@:+,&lt;,&lt;A97+++,8&lt;5&lt;,3,,3,:=7,,3E9,++,,3388:1,,3@;,,,33?DBC77**,,,7&lt;,2,,,,7,,,,,,,::&gt;C?5**/5*8:C558**++++0+&lt;&lt;CEEE88+++3&lt;&lt;5*:5:?CE5+++++*2*2975)******84********************/**)**)*.)).49())))-.64((-4())(.48)).)6:7--)(,.4)-).)))-)).))))))))--3(((()))),(2(((\n\nRead ./data/fastq/PF0097_S43_L001_R2_001.fastq.gz\n@M05795:43:000000000-CFLMP:1:1101:15977:1738 2:N:0:43\nCATACTTTTGTTTATTTTCCTCTTCTTCTTTTTCTTGTTATTTCTTCTTTTTAATATTTACTTCTTACTTTTTCTTTTTTTGTTTACGTTTCATTTTATTTTCTTATACTATCTATATATATTCTATTTTTTAATTATTTTACAATCTTTATGACAAACATAATATTTTATATAAATATGATTTATACTATACTCTCAACATATATTCCTTTGCCCAGCGCTATCAAAAATGCAAATGACAATCTGTCTAAAGTAAATATGGATAACGATATAAATCTTAATAATCACCATAATATAAAAA\n+\n-,,,8=66@,;,,,=,,666C&lt;CE,,&lt;,&lt;,6C,,;,,6;,;,,,,&lt;,&lt;,&lt;6,,,,,,66,,;,,,,,,,,,,,,;,5,,+++,,,,,,,,,,,:A,,,,,,94,,,9,,9,,,9,9,,,9,9;DD,C;===8,,9,,934=94,,91@=A,9,,++++6+6++++3++++++++++2*?**4+4+0****2*5;DDD**)*0***33;*000***))))0)158)****)1*****0**9***)**002***52*0*3*2*6*:***1)0)1*2*27:A*****2**0)*:*5*9?5*5**\nWe can see that the header of the first sequence in each FASTQ file is identical, except for the 1/2 near the end, indicating that the reads are derived from the forward and reverse pass of the same cluster on the flow cell.\nThe two sequences are exactly the same length, and this is what we expect in most cases when the forward sequencing cycle has the same length as the reverse one. Looking ahead at Figure 13.3, we know that the library fragment is sequenced from both ends, for a fixed amount of cycles (in this case, 300 bp).\nDepending on the size of the insert, there may or may not be overlap between the two sequences. To be certain, one of the reads will need to be reverse complemented (because they were synthesised based on complementary template strands). In this case, there does not appear to be any. We could check later to see which amplicon this sequence aligns to; if it is longer than 600 bp (= twice the length of each 300 bp fragment), we would not expect to see any overlap.\nBelow is a code snippet that takes the reverse complement, but you could use an online converter too.\nzcat ./data/fastq/PF0097_S43_L001_R2_001.fastq.gz | head -n4 | seqkit seq --seq-type dna --reverse --complement\n[INFO] when flag -t (--seq-type) given, flag -v (--validate-seq) is automatically switched on\n@M05795:43:000000000-CFLMP:1:1101:15977:1738 2:N:0:43\nTTTTTATATTATGGTGATTATTAAGATTTATATCGTTATCCATATTTACTTTAGACAGATTGTCATTTGCATTTTTGATAGCGCTGGGCAAAGGAATATATGTTGAGAGTATAGTATAAATCATATTTATATAAAATATTATGTTTGTCATAAAGATTGTAAAATAATTAAAAAATAGAATATATATAGATAGTATAAGAAAATAAAATGAAACGTAAACAAAAAAAGAAAAAGTAAGAAGTAAATATTAAAAAGAAGAAATAACAAGAAAAAGAAGAAGAGGAAAATAAACAAAAGTATG\n+\n**5*5?9*5*:*)0**2*****A:72*2*1)0)1***:*6*2*3*0*25***200**)***9**0*****1)****)851)0))))***000*;33***0*)**DDD;5*2****0+4+4**?*2++++++++++3++++6+6++++,,9,A=@19,,49=439,,9,,8===;C,DD;9,9,,,9,9,,,9,,9,,,49,,,,,,A:,,,,,,,,,,,+++,,5,;,,,,,,,,,,,,;,,66,,,,,,6&lt;,&lt;,&lt;,,,,;,;6,,;,,C6,&lt;,&lt;,,EC&lt;C666,,=,,,;,@66=8,,,-\n\n\n\n\n\n\n13.1.1 Multiplexed samples\nIt is often beneficial to pool samples from multiple individuals or projects together and then sequence them in a single run. It saves on costs, makes more efficient use of reagents and it also helps guard against batch effects. This last part is explained by the fact that every sequencing run, and even all the different lanes of a single run, will introduce some technical variation into your experiment. By pooling samples and loading them in the same lane of a single run (or across multiple lanes if more sequencing depth is required), this batch effect or bias can be reduced. When targeting specific genomic areas, like we do for AmpliSeq, or when working with small genomes in general, pooling also allows us to analyse a large amount samples in a single run.\nHowever, if we were to just throw together the DNA from all those different samples and load this mixture onto the sequencer, we would not be able to tell which reads came from which sample afterwards. In order to be able to distinguish reads from different samples, the library fragments need to be tagged with an index (or barcode, naming conventions are messy 2) that is unique to each sample, prior to pooling. During sequencing, the indices of each cluster are read out (in a separate reaction3 from the target DNA of interest, referred to as the insert), and afterwards this information is used to disentangle the different samples and generate (pairs of) FASTQ files per sample.\n2 in practice, the terms index and barcodes are often used interchangeable for Illumina sequencing, see Illumina (2017). However, sometimes two different types of indices are defined. 1) Multiplex indices are the ones shown in Figure 13.3 which are used for associating reads with samples. They are positioned outside of the insert and have their own sequencing primer (i.e. they are read out during a separate sequencing step and do not show up (at the start) of the FASTQ reads). 2) Inline indices on the other are part of the DNA insert and are read out during the same sequencing step as the DNA insert. Consequently, these do show up in the FASTQ reads and they take up some of the available read length (i.e., the read length available for sequencing of the actual insert is reduced by the length of the inline index). Lastly, the term barcode is also used in other sequencing technologies, like 10X single-cell sequencing, where they are used to uniquely identify beads (~ single cells), rather than multiplexed samples. For Illumina sequencing, a related approach is that of unique molecular identifiers (UMIs), which label each each molecule in a sample in order to reduce the impact of PCR duplicates and sequencing errors.3 Sequencing the index in a separate step avoids issues with base calling quality degradation, avoids consuming part of the read length available for the insert, and ensures that the index sequence is always complete.The demultiplexing step is usually handled by the sequencer itself, so the FASTQ files that we end up with are already cleaned and representing a single sample-lane combination.\n\nThe index is a unique oligonucleotide sequence that is read separately from the target DNA of interest (called the insert). Depending on the technology, there might even be dual indices 4 (on both the 5’ and 3’ ends), placed between the insert (and its sequencing primers) and the adapters (P5/P7, used to anneal the entire DNA fragment to the flow cell) (Figure 13.3).\n4 sometimes dual indices (UDIs) are used on either end of the library fragment, instead of a single index. See https://knowledge.illumina.com/library-preparation/general/library-preparation-general-reference_material-list/000002344]).\n\n\n\n\n\n\nFigure 13.3: Construction of a sequencing library fragment through adapter ligation: P5 (red) / P7 (green) = sequences for anchoring to flow cell; (Rd1 SP (orange) / Rd2 SP (blue) = read 1/2 sequencing primer; index/barcode 1/2 = sample identifiers for multiplexing (Source: Illumina 2020).\n\n\n\n\n(More information on the molecular technologies used for indexing can be found in Illumina 2021b.)\n(More information on multiplexing and the different sequencing read steps involved can be found in Illumina 2008.)",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quality control and trimming</span>"
    ]
  },
  {
    "objectID": "content/genomics/qc.html#sec-fastq-quality-score",
    "href": "content/genomics/qc.html#sec-fastq-quality-score",
    "title": "13  Quality control and trimming",
    "section": "13.2 Quality control (QC)",
    "text": "13.2 Quality control (QC)\nAs mentioned above, FASTQ files contain information about:\n\nthe raw sequence of base calls\nthe quality scores (per base!)\nthe location of the cluster on the flow cell\nindirect information on the overall nucleotide composition\nindirect information on the location of the base in the sequence\n\nThese different types of data can all be used during the QC of the reads.\n\n13.2.1 Sources of errors\nIn Illumina sequencing, sequences reads are constructed by detecting the nucleotides present in each particular cluster ( = group of identical elongated fragments) on a flow cell through a fluorescent signal. The intensity and purity of this signal is used to measure the quality of a particular base call, or in other words, how confident we can be that the base call at a particular position in the read is accurate.\nAccurately inferring which type of nucleotide got incorporated by each fragment making up a single cluster, for all the millions of clusters on a tiny flow cell simultaneously, for hundreds of cycles in a row, is no easy feat. And as with all (sequencing) technologies, there are a few technical limitations. It is these issues inherent to the sequencing-by-synthesis process that can result in quality degradation, but poor quality scores could also be indicative of more serious problems during the sequencing process or sample preparation.\nSome examples of problems that can occur are (University of Exeter DNA sequencing service; Piper et al. 2022; Ledergerber and Dessimoz 2011):\n\nSignal decay due to degrading fluorophores and some strands in a cluster not being elongated; quality is reduced with each successive cycle (expected, technology limitation).\nFrequency cross-talk caused by frequency overlap of the fluorophores (expected, technology limitation).\nPhasing and pre-phasing: signal noise or ambiguity introduced by lag between the different strands in a cluster. I.e., individual strands fall out of sync, blurring and reducing the true base signal over the remainder of the run.5 This problem typically occurs towards the ends of the reads and results in poorer quality with each successive cycle (expected, technology limitation).\nLow complexity sequences (= high AT- or GC-content). A well-balanced nucleotide diversity is required to accurately identify the location of clusters.\nOver-clustering leads to signals from adjacent clusters bleeding through one another. Choosing an optimal cluster-density is the best way to avoid this.\nInstrument breakdown: usually an obvious and sudden drop in quality across many reads. In these cases re-sequencing by the sequencing facility might be required.\n\n5 Phasing can be caused by the 3’ terminators not being removed from some fragments in a cluster, in between sequencing cycles. Pre-phasing can happen when two nucleotides get incorporated in one cycle, instead of a single one (e.g., because the 3’ terminators were not effective).\nFor non-patterned flow cells6, there can be close to a million clusters per square millimeter! No wonder the machine can have trouble distinguishing the signal from each of those.\n6 Newer Illumina instruments use patterned flow cells, where clusters are created inside nanowells at fixed locations on the slide.\n\n\nFluorescent signals from clusters on flow cell (Source: Illumina 2016).\n\n\n\n\n13.2.2 Quality scores\nThe fourth line of each sequence in a FASTQ file contains the so called Phred Quality Scores Q. This line consists of a string of characters, one for each base in the sequence, that encodes the score for that base. The higher the score, the higher the probability that the base was called correctly (or the lower the probability of a misidentification). Specifically, the score is defined as follows:\n Q_{PHRED} = -10 \\times \\log_{10}(P_e) \nWhere P_e is the probability of an incorrect base call. The following table gives an indication of how to interpret various ranges of quality scores:\n\n\n\n\n\n\n\n\nPhred Quality Score\nProbability of incorrect base call\nBase call accuracy\n\n\n\n\n10\n1 in 10\n90%\n\n\n20\n1 in 100\n99%\n\n\n30\n1 in 1000\n99.9%\n\n\n40\n1 in 10,000\n99.99%\n\n\n50\n1 in 100,000\n99.999%\n\n\n60\n1 in 1,000,000\n99.9999%\n\n\n\nIn general, Q30 is considered a good benchmark for reliable base calls.\nHowever, when you look at what is actually in the fourth line of a read in a FASTQ file you will see a rather strange collection of characters, including letters, numbers and other symbols, instead of, well, human-readable scores. What does a score of @ even mean? Well, these are ASCII characters and they are used to compress the Q scores (which are often 2 digit numbers) down into a single character. This not only saves space, but allows the score to be aligned with its corresponding base in the sequence.\nThe characters in the ASCII set are tied to a particular Q score, but rather than starting from 0 (the first character in the ASCII set), each sequencing platform uses a particular off-set, i.e. they start at a unique base value and count up from there. For example, the current standard for Illumina platforms (1.8) is the Phred+33 encoding, in which a score of 0 corresponds to the character ! and higher scores step through the range of characters from there. The Phred+33 encoding is visualised in green in Figure 13.4.\n\n\n\n\n\n\nFigure 13.4: FASTQ quality scoring and ASCII offsets. (Source: Batut et al.; Hiltemann et al. 2023)\n\n\n\n\n\n\n\n\n\nInterpreting Phred Quality Scores - example calculation\n\n\n\n\n\nConsider the following Illumina 1.8 read:\n@SR.1:Pf3D7_01_v3:M:533192-533407\nTCTCATCATCCCTCTCATCATCATCATCACTCTCATCATTATCATCACTCTCATCACTCTCATTACTATCATCACTCTCATCATTATCATTACTATCATC\n+\nCABDAAFCBAHCEGFFA?FEEAFFHDCFCB?GHDECDCABFGBGA@HDGGEAABCDA@I?IIDIIH85:=;&lt;:;;97:78:881//-23200&')$'(#%\nThe first base in the sequence (T) is associated with the encoded Q score C. In Figure 13.5, you can see that the value of this ASCII character is 67.\nIf we subtract the off-set, we get 67 - 33 = 34, which is the Q score.\nLastly, we need to rearrange the quality score equation shown above and plug in Q in order to obtain the probability of an incorrect base call:\n P = 10^{\\frac{Q}{-10}} = 10^{\\frac{34}{-10}} \\approx 0.0004 \nOr a 0.9999% base call accuracy.\n\n\n\n\n\n\nFigure 13.5: ASCII table\n\n\n\n\n\n\nTo summarize, Q scores can be converted into the probability of base calling errors by performing the following steps:\n\nFind the value of the ASCII character.\nConvert into a Q score by subtracting the off-set value (e.g., -33).\nPlug the resulting value into the formula P = 10^{\\frac{Q}{-10}} to obtain the error probability. \n\n\n\n\n\n\n\nExercises\n\n\n\n\n\nGiven the following read from a sequencer using the current Illumina 1.8 Sanger+33 format.\n@M05795:43:000000000-CFLMP:1:1101:6285:5345 2:N:0:68\nGGAGAAACAGGTAGTGGAAAATCAACTTTTATGAATCTCTTATTAAGATGTTAGTCAAGAACCCATGGTAT\n+\nCCCCC8FF8C@FFG;EFFGFCHC,EF9CCFFEFGEFC&lt;,FGGGF6CFC98;*;;3;*;CB@&gt;%5&gt;45)972\n\nWhat is the Phred Quality Score of the first cytosine in the following read?\nWhat is the probability that the last guanine is called correctly?\nWhich ASCII characters correspond to the lowest and highest quality score in this read?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe first C (8th position in the read) was assigned the encoded quality score F. In the ASCII table, we see that the numerical value of F is 70. Our read is in the Phred +33 format where a Q score of 0 is represented by the ASCII value 33 (!). Thus, we need to subtract 33 from our ASCII value to obtain the Q score : Q=70-33=37.\nThe last G in our read (68th position) was assigned the encoded quality score ). The numerical ASCII value of this character is 41. When we subtract 33, we obtain the Q score 41-33=8. To obtain the error probability, we can use the formula P = 10^{\\frac{Q}{-10}} = 10^{\\frac{8}{-10}} = 0.15, or a 15% chance of an incorrect base call.\nThe lowest score in this read is 5 (%), while the highest one is 39 (H).\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nMore information on Q scores can be found here:\n\nhttps://www.illumina.com/documents/products/technotes/technote_Q-Scores.pdf\nhttps://gatk.broadinstitute.org/hc/en-us/articles/360035531872-Phred-scaled-quality-scores\nhttps://zymoresearch.eu/blogs/blog/what-are-phred-scores\n\n\n\n\n\n\n\n\n\nFASTQ + Emoji = FASTQE 🤔\n\n\n\nFASTQE is a simultaneously silly and brilliant tool that allows you to visualize quality scores using emoji. Why we wouldn’t recommend using it for any serious work, it is a rather elegant way of showing the overall quality profile of a read.\nAs an exercise, try using it on one of the .fastq files in your data directory. Use the --bin flag to make the output a bit more structured and easy to understand.\nAre they any trends that you notice? How do these relate back to sources of errors that we mentioned earlier?\n\n\n\n\n\n\nExample output\n\n\n\n\n\n$ fastqe --bin PF0097_S43_L001_R1_001.fastq.gz\nFilename        Statistic       Qualities\ntraining/data/fastq/PF0097_S43_L001_R1_001.fastq.gz     mean (binned)\n😆 😆 😆 😆 😆 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎\n😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎\n😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎\n😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎\n😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎\n😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎\n😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎\n😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎 😎\n😎 😎 😎 😎 😎 😎 😎 😎 😆 😎 😎 😎 😎 😎 😆 😆 😆 😆 😆 😆 😆\n😆 😆 😆 😆 😆 😆 😆 😆 😆 😆 😆 😆 😆 😆 😆 😆 😆 😆 😆 😆 😆\n😆 😆 😆 😆 😆 😆 😆 😆 😆 😆 😆 😆 😆 😆 😄 😄 😆 😆 😆 😆 😄\n😆 😄 😆 😆 😄 😄 😆 😆 😄 😄 😄 😄 😄 😄 😄 😄 😄 😄 😄 😄 😄\n😄 😄 😄 😄 😄 😄 😄 😄 😄 😄 😄 😄 😄 😄 🚨 🚨 🚨 🚨 🚨 🚨 🚨\n🚨 🚨 🚨 🚨 🚨 🚨 🚨 🚨 🚨 🚨 🚨 🚨 🚨 🚨 🚨 🚨 💩 💩 💩 💩 💩\n💩 🚨 💩 💩 🚨 🚨 💩",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quality control and trimming</span>"
    ]
  },
  {
    "objectID": "content/genomics/qc.html#sec-fastqc",
    "href": "content/genomics/qc.html#sec-fastqc",
    "title": "13  Quality control and trimming",
    "section": "13.3 Assessing quality scores programmatically - fastqc",
    "text": "13.3 Assessing quality scores programmatically - fastqc\nOf course, to get some actual work done, we will use a tool like fastqc to assess the quality of our sequence reads, rather than inspecting millions of reads by hand ;)\nFastQC is one of several QC tools for sequence reads7. It can process each one of your FASTQ files and generate a report with several metrics and plots on the overall quality of the reads. One of the most useful plots it creates is the per base sequence quality shown below: it shows box-plot distributions of the quality scores per position, averaged over all the reads in a sample. Quality tapers off towards the end, which is a known issue for Illumina reads that we can address via trimming.\n7 A notable alternative QC program is fastp, which can do not just QC, but also trimming and filtering (more on that later).\n\n\nExample output of FastQC. Note the decreasing scores towards the end of the read.\n\n\nAnother useful metric is the per sequence quality scores, where we can assess the variation in overall quality between the different reads in a sample. Ideally there is a narrow peak around the higher scores. The Read length histogram should ideally show a nice narrow distribution around your expected fragment sizes. Lastly, the per base sequencing content is something we expect to show a similar distribution for each of the four bases unaffected by the position in the read. However, for Plasmodium falciparum we know that there is an extremely strong AT-bias, in which case this plot needs to be interpreted with caution. The same applies for the per sequence GC content metrics.\n\n\n\n\n\n\nNote\n\n\n\nFor more information on FastQC, you can check out their webpage which has example reports for different kind of data (both good and bad).\nThe developers have also created a nice overview video where they go through a report and talk about the interpretation of the different metrics:\n\nLastly, this Galaxy Training offers an excellent overview of using and interpreting FastQC as well (Batut et al.; Hiltemann et al. 2023). For even more examples of how to evaluate FastQC output, you can check this (University of Exeter DNA sequencing service) and this (Piper et al. 2022) resource.\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\n\nTry it yourself:\n\nRead the fastqc help message by calling fastqc --help and figure out the basic syntax to use the tool.\nRun the tool on one of the .fastq files in the P. falciparum AmpliSeq dataset. Does it matter if its .gz compressed or not?\nInspect the output and assess the quality of the reads. Look back at the different sources of base calling errors that we discussed earlier.\n\nInterpreting the output:\n\nDownload the Shiny app from Moodle, inspect the FastQC outputs and answer the questions.\n\nBonus question:\n\nHow would you run the tool on a directory with many .fastqc files? Write a small script that does this.",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quality control and trimming</span>"
    ]
  },
  {
    "objectID": "content/genomics/qc.html#sec-fastqscreen",
    "href": "content/genomics/qc.html#sec-fastqscreen",
    "title": "13  Quality control and trimming",
    "section": "13.4 FastQ Screen",
    "text": "13.4 FastQ Screen\nFastQ Screen is an utility that allows you to screen your FASTQ reads against a panel of genomes or known contaminants (like PhiX), to determine the origin of your sequences. In the case of malaria surveillance via AmpliSeq (or WGS), this allows us to screen for:\n\nmixed infections, common lab contaminants.\nhuman DNA contamination\ncommon lab contaminants\nartificial sequences (adapters, vectors)\nknown problematic sequences (rRNA)\n\nUnder the hood, it uses a sequence aligner like bowtie or bwa (the latter of which we will use in the alignment or mapping chapter) to map the reads against various databases (= reference genomes or known contaminants). It also informs you where your reads map uniquely; both within and across species.\nThe full documentation for FastQ Screen can be found here.\nHere is a short video introduction to the software:\n\nBelow we show an example of the output generated by FastQ Screen.\n\n\n\nExample output of FastQ Screen for a P. vivax sample.\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\nDownload the Shiny app from Moodle, inspect the FastQC outputs and answer the questions.",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quality control and trimming</span>"
    ]
  },
  {
    "objectID": "content/genomics/qc.html#sec-trimming",
    "href": "content/genomics/qc.html#sec-trimming",
    "title": "13  Quality control and trimming",
    "section": "13.5 Trimmining - trimmomatic",
    "text": "13.5 Trimmining - trimmomatic\n\n\n\n\n\n\nNote\n\n\n\nThere exist many alternative tools for trimming and filtering FASTQ files, such as fastp, Cutadapt and Trim Galore (which is a wrapper around Cutadapt and FastQC).\nThis tutorial focuses on fastqc + trimmomatic for historical reasons (being used in the ), but fastp is becoming more popular nowadays.\n\n\nTrimmomatic is a tool that can:\n\nTrim low quality bases at the start and end of sequences\nFilter low quality reads (e.g., too short after trimming)\nRemove adapter sequences\n\n\n\n\n\n\n\nWhy do adapter sequences show up in reads?\n\n\n\nRecall the structure of the library fragments shown in Figure 13.3, in particular the location of the adapters. The sequencing primer where sequencing commences is located downstream from the adapters and indices on the 5’ end. However, if the sequencing extends beyond the length of the DNA insert (i.e. the insert is shorter than the sequencing length), it enters into the adapter sequence on the opposite end of the fragment (on the 3’ end). These adapters need to be trimmed, but things are tricky because often times these are only incomplete partial adapter sequences.\nMore information can be found on page 5 of the Trimmomatic manual.\n\n\nYou can check the usage of trimmomatic by simply invoking the command without any arguments: trimmomatic\nUsage:\n    PE [-version] [-threads &lt;threads&gt;] [-phred33|-phred64] [-trimlog &lt;trimLogFile&gt;] [-summary &lt;statsSummaryFile&gt;] [-quiet] [-validatePairs] [-basein &lt;inputBase&gt; | &lt;inputFile1&gt; &lt;inputFile2&gt;] [-baseout &lt;outputBase&gt; | &lt;outputFile1P&gt; &lt;outputFile1U&gt; &lt;outputFile2P&gt; &lt;outputFile2U&gt;] &lt;trimmer1&gt;...\nor:\n    SE [-version] [-threads &lt;threads&gt;] [-phred33|-phred64] [-trimlog &lt;trimLogFile&gt;] [-summary &lt;statsSummaryFile&gt;] [-quiet] &lt;inputFile&gt; &lt;outputFile&gt; &lt;trimmer1&gt;...\nor:\n    -version\nWe can immediately see that there are two different versions, one for paired-end and one for single-end reads, as well as an option to specify the type of Q score (-phred33, which is not the default!).\nSome of the other important options are trimmomatic:\n\n\n\n\n\n\n\nOption\nMeaning\n\n\n\n\ninputFile1/2\nThe input fastq reads (.gz compression is allowed).\n\n\noutputFile1/2P\nName of output file containing the paired reads from fastq pair 1/2.\n\n\noutputFile1/2U\nName of output file containing the unpaired reads from read pair 1/2 (e.g., when its matching read was discarded).\n\n\nILLUMINACLIP:adapters.fa:2:30:10\nPerform adapter removal. Requires a file containing known Illumina adapter sequences (like Nextera/TruSeq or specific to your protocol like for AmpliSeq).\n\n\nSLIDINGWINDOW\nPerform sliding window trimming, cutting once the average quality within the window falls below a threshold.\n\n\nLEADING\nCut bases off at the start of a read, if the quality is below the threshold.\n\n\nTRAILING\nCut bases off at the end of a read, if the quality is below the threshold.\n\n\nMINLEN\nremove reads that are too short\n\n\n\nFor the AmpliSeq pipelines, we generally stick to the following options:\n\nILLUMINACLIP:adapters.fa:2:30:10: cut off Illumina specific adapters from the read (stored in adapters.fa).8\nLEADING:3: trim bases from the start of the read if they drop below a Q score of 3 (or N).\nTRAILING:3: trim bases from the end of the read if they drop below a Q score of 3 (or N).\nSLIDINGWINDOW:4:15: scan the read with a 4-base wide sliding window, cutting when the average quality per base drops below 15.\nMINLEN:36: remove reads that are shorter than 36 bases.\n\n8 For AmpliSeq, an example can be found here. For generic WGS data, Trimmomatic comes with a few example files. On our workstations, they can be found in ${CONDA_PREFIX}/share/trimmomatic/adapters/NexteraPE-PE.fatrimmomatic PE \\\n    -phred33 \\\n    -trimlog trimreport.txt \\\n    sample_R1_001.fastq.gz \\\n    sample_R2_001.fastq.gz \\\n    sample_R1_001.trim.fastq.gz \\\n    sample_R1_001.trim.unpaired.fastq.gz \\\n    sample_R2_001.trim.fastq.gz \\\n    sample_R2_001.trim.unpaired.fastq.gz \\\n    ILLUMINACLIP:adapters.fa:2:30:10 \\\n    LEADING:3 \\\n    TRAILING:3 \\\n    SLIDINGWINDOW:4:15 \\\n    MINLEN:36\nFor more detailed info on all these commands, check out the trimmomatic manuaul.\n\n\n\n\n\n\nTip\n\n\n\nThe threads option allows you to process multiple samples simultaneously to speed up the analysis. We will see more on parallelization and multi-threading later. In your CodeSpace environment, you can choose 2 or 4 threads, depending on the number of cores you selected.\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\nTry trimming a single pair(!) of reads.\nRun FastQC again on the trimmed reads and compare the results with the original FastQC report.\nWrite a script to trim multiple reads in one go and use it to analyse all the paired read in the Peruvian P. falciparum AmpliSeq dataset. Remember that you can use pseudo-code to start out with.\n\nAfterwards, compare your approach to the trim.sh script in the ./training/scripts directory.\n\nCan you guess what the following line does? base=$(basename ${read_1} _1.fastq.gz)\n\nHint: $(command) is a way to run a command inside another command or statement, so try figuring out what the command between the brackets does first.\n\n\nBonus exercise: trim all the read files for the Vietnam P. vivax WGS data too and compare the FastQC differences.\n\n\n\n\n\n\n\n\nTip 13.2: Looping over paired FASTQ files in bash\n\n\n\nWhen writing scripts that iterate over paired FASTQ files, you can use the concepts that we introduced in Tip 8.1.\nA minimal example is provided below:\n#!/usr/bin/env bash\n\n# loop through the first read of each pair\nfor read_1 in ../data/fastq/*_R1_001.fastq.gz\n\ndo\n\n    # extract the sample name and print it\n    sample_name=$(basename \"${read_1}\" _R1_001.fastq.gz)\n\n    echo \"Read 1 = ${sample_name}_R1_001.fastq.gz\"\n    echo \"Read 2 = ${sample_name}_R2_001.fastq.gz\"\n\ndone\n\n\n\n\n\n\n\n\nTip 13.3: Evaluating many reports at once: MultiQC\n\n\n\nAs a bonus exercise, have a look at MultiQC. This is a tool that allows you to aggregate the output results from many different samples into a single report. It supports many different bioinformatics tools, including FastQC.\nYou can install it on your own environment using the following command: conda install multiqc and play around with it.\n\n\n\n\n\n\n\n\n\nAdditional resources\n\n\n\n\nSequencing library construction, adapters and indexing:\n\nhttps://teichlab.github.io/scg_lib_structs/methods_html/Illumina.html\nhttps://www.umassmed.edu/contentassets/5ea3699998c442bb8c9b1a3cf95dbb24/indexing-and-barcoding-for-illumina-nextgen-sequencing.pdf\nhttps://www.lubio.ch/blog/ngs-adapters\nhttps://www.umassmed.edu/globalassets/deep-sequencing-core/indexing-and-barcoding-for-illumina-nextgen-sequencing-2021.pdf\nhttps://knowledge.illumina.com/library-preparation/general/library-preparation-general-reference_material-list/000003275\nhttps://support.illumina.com/ko-kr/bulletins/2020/12/how-short-inserts-affect-sequencing-performance.html\nhttps://support-docs.illumina.com/SHARE/IndexedSeq/indexed-sequencing.pdf\n\n\n\nDemultiplexing:\n\nhttps://s3-us-west-2.amazonaws.com/oww-files-public/a/a5/IlluminaParsing_v1-2.pdf\n\nPaired-end sequencing:\n\nhttps://www.illumina.com/science/technology/next-generation-sequencing/plan-experiments/paired-end-vs-single-read.html\nOrientation of (index) reads: https://seekdeep.brown.edu/illumina_paired_info.html\n\n\n\n\n\n\n\n\nBatut, Bérénice, Maria Doyle, Alexandre Cormier, Anthony Bretaudeau, Laura Leroi, Erwan Corre, Stéphanie Robin, gallantries, and Cameron Hyde. “Quality Control (Galaxy Training Materials).” https://training.galaxyproject.org/training-material/topics/sequence-analysis/tutorials/quality-control/tutorial.html.\n\n\nClevaLab. 2022a. “A Step-by-Step Guide to NGS.” https://www.clevalab.com/post/a-step-by-step-guide-to-ngs.\n\n\n———. 2022b. “Next Generation Sequencing - a Step-by-Step Guide to DNA Sequencing.” YouTube. https://www.youtube.com/watch?v=WKAUtJQ69n8.\n\n\nHiltemann, Saskia, Helena Rasche, Simon Gladman, Hans-Rudolf Hotz, Delphine Larivière, Daniel Blankenberg, Pratik D. Jagtap, et al. 2023. “Galaxy Training: A Powerful Framework for Teaching!” Edited by Francis Ouellette. PLoS Comput Biol Computational Biology 19 (1): e1010752. https://doi.org/10.1371/journal.pcbi.1010752.\n\n\nIllumina. 2008. “Multiplexed Sequencing with the Illumina Genome Analyzer System.” https://www.illumina.com/documents/products/datasheets/datasheet_sequencing_multiplex.pdf.\n\n\n———. 2016. “Overview of Illumina Sequencing by Synthesis Workflow.” YouTube. https://www.youtube.com/watch?v=fCd6B5HRaZ8.\n\n\n———. 2017. “An Introduction to Next-Generation Sequencing Technology.” https://www.illumina.com/content/dam/illumina-marketing/documents/products/illumina_sequencing_introduction.pdf.\n\n\n———. 2020. “How Short Inserts Affect Sequencing Performance.” https://support.illumina.com/ko-kr/bulletins/2020/12/how-short-inserts-affect-sequencing-performance.html.\n\n\n———. 2021a. “Learn about Illumina’s Next-Generation Sequencing Workflow.” YouTube. https://www.youtube.com/watch?v=oIJaA6h2bFM.\n\n\n———. 2021b. “Indexed Sequencing on Illumina Systems.” https://support-docs.illumina.com/SHARE/IndexedSeq/indexed-sequencing.pdf.\n\n\n———. n.d. “Next-Generation Sequencing Glossary.” NGS Terminology. https://www.illumina.com/science/technology/next-generation-sequencing/beginners/glossary.html.\n\n\nLaunen, Loren. 2017. “Illumina Sequencing (for Dummies).” https://kscbioinformatics.wordpress.com/2017/02/13/illumina-sequencing-for-dummies-samples-are-sequenced/.\n\n\nLedergerber, C., and C. Dessimoz. 2011. “Base-Calling for Next-Generation Sequencing Platforms.” Briefings in Bioinformatics 12 (5): 489–97. https://doi.org/10.1093/bib/bbq077.\n\n\nPiper, Mary E., Meeta Mistry, Jihe Liu, William J. Gammerdinger, and Radhika S. Khetani. 2022. “Hbctraining/Intro-to-Rnaseq-Hpc-Salmon-Flipped: Introduction to RNA-Seq Using Salmon Lessons from HCBC (First Release),” January. https://doi.org/10.5281/ZENODO.5833880.\n\n\nRodriguez, Raphaël, and Yamuna Krishnan. 2023. “The Chemistry of Next-Generation Sequencing.” Nature Biotechnology 41 (12): 1709–15. https://doi.org/10.1038/s41587-023-01986-3.\n\n\nShendure, Jay, Shankar Balasubramanian, George M. Church, Walter Gilbert, Jane Rogers, Jeffery A. Schloss, and Robert H. Waterston. 2017. “DNA Sequencing at 40: Past, Present and Future.” Nature 550 (7676): 345–53. https://doi.org/10.1038/nature24286.\n\n\nUniversity of Exeter DNA sequencing service. “Introduction to Genomics.” https://biomedicalhub.github.io/genomics/01-part1-introduction.html.",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Quality control and trimming</span>"
    ]
  },
  {
    "objectID": "content/genomics/mapping.html",
    "href": "content/genomics/mapping.html",
    "title": "14  Alignment to reference genome",
    "section": "",
    "text": "14.1 Where to find your reference genomes - bwa mem\nThe next step of our pipeline tries to align the filtered and trimmed reads to the reference genome of the organism we’re studying. It starts from the processed FASTQ files and produces SAM and BAM files, which contain the mapped reads. We encountered these file formats before in Section 6.2.1 and Section 6.4.1, but we will go a bit more in-depth now. Two other important aspects in this part of the pipeline are indexing the reference genome and filtering out PCR duplicates from our alignment.\nWe already downloaded a reference genome from PlasmoDB in Section 6.5 (where we introduced the wget command). These reference genomes allow us to compare the samples we have collected with a representative reference type_of the organism that we’re studying2. For some organisms, multiple reference genomes exist (e.g., because the organism exhibits a lot of genetic variability between different geographic locations), and these references also receive updates from time to time, as the assemblies of complex regions improves (e.g., gaps spanned, repetitive regions mapped, or in the case of Plasmodium, the polymorphic subtelomeric regions).",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Alignment to reference genome</span>"
    ]
  },
  {
    "objectID": "content/genomics/mapping.html#where-to-find-your-reference-genomes---bwa-mem",
    "href": "content/genomics/mapping.html#where-to-find-your-reference-genomes---bwa-mem",
    "title": "14  Alignment to reference genome",
    "section": "",
    "text": "2 In practice, reference genomes rarely contain the genetic material of a single individual. Instead, they tend to be a mosaic of DNA sequences from different sources. See https://en.wikipedia.org/wiki/Reference_genome.\n\n\n\n\n\nExercise\n\n\n\n\nDownload the most recent releases of the P. falciparum 3D7 and the P. vivax P01 reference genomes and store them in ./data/references in clearly labelled subdirectories.\nTry to inspect the file using head and less.",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Alignment to reference genome</span>"
    ]
  },
  {
    "objectID": "content/genomics/mapping.html#indexing-a-reference-genome",
    "href": "content/genomics/mapping.html#indexing-a-reference-genome",
    "title": "14  Alignment to reference genome",
    "section": "14.2 Indexing a reference genome",
    "text": "14.2 Indexing a reference genome\nBefore we can map our reads to a reference genome, we need to create an index to allow our mapper to more efficiently search through the (usually rather large) FASTA file3. You can think of it like the index or table of contents of a book (if that particular book contained about 23 million characters in the case of Plasmodium falciparum, or 3.1 billion characters for the human reference genome).\n3 For a refresher on the FASTA file format, check out Section 5.1.2.1.The indexing step itself is usually performed by the same tool as the one that does the actual alignment. The index only needs to be created once, and then it can be re-used when mapping each of our FASTQ files. In this course, we will be using bwa - the Burrows-Wheeler Aligner.\n\n\n\n\n\n\nExercise\n\n\n\n\nUse the bwa index &lt;reference.fasta&gt; command to create an index for both reference genomes.\nWhich files were created by the indexing step?\n\n\n\n\n\n\n\n\n\nTip 14.2: Collaboration and reproducibility\n\n\n\nWe’d like to mention two important related aspects of (computational) scientific workflows here: collaboration and reproducibility.\n\nReference genomes and their indices take up a lot of space. In the case of larger genomes, like the human one, it also takes considerable time to produce the index. Fortunately, these files can be re-used for any future analysis - not just by you, but by others as well. That is why we recommend to store these references and their indices in a shared location on your lab’s workstation, so that they become accessible to others who might benefit from them.\nTo ease collaboration with others, but also with your future self, it is a good idea to keep track of the various steps you’ve performed in your analysis, just like you would do in a lab notebook. Of course the code and scripts that you write should be clearly named (and ideally, elaborated with clear comments), but it also fruitful to note down more nitty-gritty details on how you managed to fix a particular issue or why a specific option was chosen. In the case of reference genomes, we recommend to create a README.md file4 to store alongside your reference genomes, which clearly describes where you obtained them, the version/build/release, and perhaps even a code snippet to re-download and checksum them.\n\nAs a bonus exercise, try to create such a readme file.\n\n\n4 .md stands for markdown, which is a popular plain-text markup language used for these kinds of tasks. You will often see people used markdown or .txt files to add metadata to collections of files in databases and analysis scripts or workflows on places like GitHub.",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Alignment to reference genome</span>"
    ]
  },
  {
    "objectID": "content/genomics/mapping.html#sec-alignment",
    "href": "content/genomics/mapping.html#sec-alignment",
    "title": "14  Alignment to reference genome",
    "section": "14.3 Alignment to the reference genome",
    "text": "14.3 Alignment to the reference genome\nDuring mapping, the aligner tries to find a potential alignment site for each individual read (or query) in the reference genome. After aligning all the reads, we can start looking for variation in our samples compared to the reference genome and each other.\nThere exist many different tools and algorithms for sequence alignment, each with their own strengths and weaknesses. A few examples of popular ones for DNA sequencing are BWA, bowtie2 and minimap2. The differences between these tools mostly lies in speed vs sensitivity, but also preferred application; i.e. long versus short reads5.\n5 You can read the author of BWA and minimap2 thoughts: https://lh3.github.io/2018/04/02/minimap2-and-the-future-of-bwaNote that there is also an entirely different class of aligners that were developed for RNA-sequencing; these need to be “splice-aware” to handle RNA reads without introns (e.g., STAR), and some of them also make use of some clever tricks for faster quantifications - so-called pseudo-aligners (e.g, Kallisto and Salmon).\nWe will use BWA as our mapping tool of choice, as is customary in the field of malaria genomics (MalariaGEN et al. 2023). BWA actually comes with three different flavours of aligners, but the most recent and popular one is bwa mem6.\n6 A second version of bwa mem has also been released, but while it is faster, it is more suited for running on high-performance compute clusters or in the cloud, due to the higher memory requirements.\nFigure 14.1 illustrates the mapping process (Hiltemann et al. 2023; Wolff, Batut, and Rasche). Each individual read is matched against the reference. You can see that read 1 has a few mismatches in its alignment, which eventually might turn out to be single nucleotide polymorphisms (SNP). Similarly, for read 2 only the middle part aligns well; bwa mem performs a local alignment and can clip the ends of the read. The third read shows an example of an insertion and a deletion (indel); the read is longer in one area (GCCA), but shorter in another (AC(A)TA).\n\n\n\n\n\n\nFigure 14.1: Mapping reads to a reference. (Source: Hiltemann et al. 2023; Wolff, Batut, and Rasche)\n\n\n\nAs we discussed in Tip 13.1, when we are working with paired-end sequencing data, there is additional information available for the aligner to work with. The expected distance between forward and reverse reads that can help to span repetitive regions. For this to work, paired reads are always mapped together, which is why they also need to be provided to the mapping tool as a pair; that is why we tend to give them similar file names (*R1.fastq/*_R2.fastq), and the paired reads appear in the same order in each file. The final alignment report usually provides dedicated output on paired reads, letting us know of cases where one of the reads fails to map (unmapped mate) or the pair is not in the expected orientation (i.e., not pointing towards each other, see Tip 14.3) / too far apart (not properly paired) , because this is not something that you’d expect in normal circumstances, since they should be derived from the same fragment of DNA.\n\n\n\n\n\n\nTip 14.3: Orientation of forward and reverse reads during mapping\n\n\n\nThe following is based on the excellent write-up on https://www.cureffi.org/2012/12/19/forward-and-reverse-reads-in-paired-end-sequencing/ and https://seekdeep.brown.edu/illumina_paired_info.html.\nRecall that during sequencing-by-synthesis, the new strand is always synthesized in the 5’-3’ direction (since DNA polymerase walks along the template strand in the 3’-5’ direction). The newly synthesized strand is what is being read out during the sequencing process, so this is what eventually ends up in our FASTQ files.\nFor paired-end sequencing, this means eventually results in the reverse read (read 2) being in the opposite orientation of the first read (read 1); it is read from the complementary strand compared to read 1 and also in the opposite direction. Consequently, the mapping software will need to take the reverse complement of read 2, to put it in the same orientation as read 1, before aligning it to the reference genome (or equivalently, try to map it to the other strand of the reference genome).\n\n\nFor Illumina paired-end sequencing, the double-stranded library fragments (= insert + adapter sequences (= anchor + read primer + index + index primer), see Figure 13.3) are first attached to the flow cell, but then the complementary strands (5’-3’ oriented towards the flow cell) are removed. The template strand (3’-5’ oriented towards the flow cell) is then used to synthesize and sequence the forward read in the 5’-3’ direction, starting from read primer 1 (adjacent to the insert). Next, index 1 (I7) is sequenced in a separate reaction (using the i7 index primer). For the reverse reads, the template strand is first used to regenerate its complementary strand (which gets attached to the flow cell), and is then removed. Lastly, the read 2 primer is used to sequence the reverse read, again in the 5’-3’ direction.\nIf you need a refresher, you can always jump back to one of the resources on the Illumina sequencing chemistry in Chapter 13 like ClevaLab (2022).\n\n\n\nWhen we call bwa mem without any additional arguments, we get an overview of how to use it:\nUsage: bwa mem [options] &lt;reference-genome&gt; &lt;R1.fastq&gt; [R2.fastq]\nAs you can see, the second read pair file is [optional], but when dealing with paired-end read data the two files need to be supplied at the same time. To do this programmatically, it is again important to use concepts like looping over pairs of FASTQ files to do this efficiently (see Tip 13.2).\nBy default, bwa will output its results straight into the terminal’s standard out (see Section 7.1). This is very useful once we start chaining different tools together, but for your first attempts you should provide the -o &lt;output_file&gt; flag to write everything to a file instead.\nbwa mem provides many different options, but we recommend the following ones:\n\n-t &lt;int&gt;: the number of threads to use. See Tip 14.4.\n-R \"@RG\\tID:L001\\tPL:ILLUMINA\\tSM:${sample}\": defines read groups, which are tags to identify which reads belong together across flowcells/lanes/library preparations7. ${sample} is a variable containing the sample name of the current read (pair) in a for loop.\n-Y: use soft clipping for supplementary alignments.\nK 100000000: makes results more deterministic. \n\n7 Reads groups are a confusing subject, and a very deep one as well. In a nutshell, read groups are used to inform downstream tools about which reads were sequenced together in the same flowcell/lane (to differentiate between and account for technical variability between runs, and not just samples) and which reads need to be grouped because they are derived from the same sample (but stored in separate files). Fortunately, they are only required by a few downstream GATK tools, but leaving them out can result in errors. For a nice overview of how to define them, see this example.Taken together, an example would be:\nbwa mem \\\n    -t 2 \\\n    -Y -K 100000000 \\\n    -R \"@RG\\tID:L001\\tPL:ILLUMINA\\tSM:PF0080\" \\\n    -o \"PF0080.sam\" \\\n    \"${ref}\" \\\n    \"${fastq_dir}/PF0080_S44_L001_R1_001.fastq.gz\" \\\n    \"${fastq_dir}/PF0080_S44_L001_R2_001.fastq.gz\"\nOf course, there exist various other options that can be used to do things like tweak the parameters, scores and thresholds used for local alignment.\n\n\n\n\n\n\nExercise: mapping with bwa mem\n\n\n\n\nMap a single pair of reads to the reference genome (use the Peruvian P. falciparum AmpliSeq data).\nOpen the resulting SAM file using less and inspect its contents and structure.\n\n\n\n\n\n\n\n\n\nTip 14.4: Speeding up code through multi-threading or parallelization\n\n\n\nSome of the tasks that we are faced with as bioinformaticians can take a long time to complete, even when using modern hardware. Fortunately, some of these tasks can be parallelized. Which ones? Well, any task that consists of a specific procedure that needs to be applied to many files in the same manner, but for which the different processes are independent of one another. This is sometimes called “embarrassingly parallel” and can be achieved by simply running the different tasks as the same time on multiple threads or compute cores.\nYou could script this yourself using a tool like parallel (see here for an example), and it’s also an important aspect of more advanced workflow managers like Nextflow, but fortunately many of the tools that we use also provide built-in multi-threading: bwa mem, samtools, trimmomatic, etc.\n\n\n\n\n\n\n\n\nHost DNA contamination removal\n\n\n\nWhen analysing WGS data (but not AmpliSeq) it is important to remove human reads from your FASTQ files prior to mapping them to the Plasmodium reference genome. There are a number of different approaches and dedicated tools to this - assigning reads to different organisms is a crucial step in bacterial metagenomics for example - but one of the most common approaches for malaria genomics is to simply do a two-step mapping process with your alignment software of choice: in the first round all reads are mapped against the human reference genome. Anything that “sticks” is discarded. In the second round, the remaining unmapped reads are mapped to Plasmodium.\nWhen depositing genetic data into public sequencing data repository like NCBI’s SRA or EMBL-EBI’s ENA, only filtered FASTQ files should be used, to avoid any potential issues with regards to human privacy and data protection.\nYou could try this for yourself as a bonus exercise. See this guide for more info on how to filter out the host/non-host reads using samtools after the mapping step.",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Alignment to reference genome</span>"
    ]
  },
  {
    "objectID": "content/genomics/mapping.html#sec-samtools",
    "href": "content/genomics/mapping.html#sec-samtools",
    "title": "14  Alignment to reference genome",
    "section": "14.4 SAM and BAM files",
    "text": "14.4 SAM and BAM files\nThe output of the alignment step (using bwa mem) is a SAM file (see Section 6.2.1). This stands for Sequence Alignment Map. These are plain text files delimited with tabs, so you can view them using a text editor or a command like less.\nTo save space and make the alignment quicker to index, SAM files are often converted to BAM files (Section 6.4.1), which are compressed binary versions of the alignment. These can be used by downstream tools directly, so we tend to only store the alignment in this format, even though you can no longer inspect the alignment yourself.\n0080_S44_L001.bam | head -n30\n@HD     VN:1.6  SO:coordinate\n@SQ     SN:Pf3D7_01_v3  LN:640851\n@SQ     SN:Pf3D7_02_v3  LN:947102\n@SQ     SN:Pf3D7_03_v3  LN:1067971\n@SQ     SN:Pf3D7_04_v3  LN:1200490\n@SQ     SN:Pf3D7_05_v3  LN:1343557\n@SQ     SN:Pf3D7_06_v3  LN:1418242\n@SQ     SN:Pf3D7_07_v3  LN:1445207\n@SQ     SN:Pf3D7_08_v3  LN:1472805\n@SQ     SN:Pf3D7_09_v3  LN:1541735\n@SQ     SN:Pf3D7_10_v3  LN:1687656\n@SQ     SN:Pf3D7_11_v3  LN:2038340\n@SQ     SN:Pf3D7_12_v3  LN:2271494\n@SQ     SN:Pf3D7_13_v3  LN:2925236\n@SQ     SN:Pf3D7_14_v3  LN:3291936\n@SQ     SN:Pf3D7_API_v3 LN:34250\n@SQ     SN:Pf3D7_MIT_v3 LN:5967\n@RG     ID:001  SM:001  PL:ILLUMINA\n@PG     ID:bwa  PN:bwa  VN:0.7.17-r1188 CL:bwa mem ../reference/PlasmoDB-67_Pfalciparum3D7_Genome.fasta PF0080_S44_L001_R1_001.fastq.gz PF0080_S44_L001_R2_001.fastq.gz -t 4 -k 30 -R @RG\\tID:001\\tSM:001\\tPL:ILLUMINA\n@PG     ID:samtools     PN:samtools     PP:bwa  VN:1.19.2       CL:samtools sort -@ 4 -o ../../results/bwa/PF0080_S44_L001.bam\n@PG     ID:samtools.1   PN:samtools     PP:samtools     VN:1.19.2       CL:samtools view -h training/results/bwa/PF0080_S44_L001.bam\nM05795:43:000000000-CFLMP:1:2118:16821:2796     2211    Pf3D7_01_v3     35509   0       84H71M144H      =       193095  157838      AAAAAAAAAAAAAAAAAAAAAAAAAAAATTAAAAAAAAAAAAAAAAAAAAACATTAAAAAAAATAAAAAAA FFGFEGGGGGGGGGGGGGGGGGGGGGE33,@FGGBEGGGG**&gt;CE*B**=5*;++23C+9*7C0&lt;CD?+:*     NM:i:5  MD:Z:42T4T0T2A2A16      MC:Z:51H73M2D177M       AS:i:46 XS:i:45 RG:Z:001        SA:Z:Pf3D7_01_v3,193086,+,112M187S,60,1;Pf3D7_13_v3,2049877,-,41S32M226S,0,0;       XA:Z:Pf3D7_05_v3,+529511,87S43M6I19M144S,7;Pf3D7_13_v3,-112651,164S40M95S,0;Pf3D7_12_v3,+497450,90S35M3D19M155S,4;Pf3D7_12_v3,+100906,96S18M1D41M2I11M131S,6;Pf3D7_11_v3,-973273,152S11M1I10M2D34M91S,3;\nM05795:43:000000000-CFLMP:1:2105:5392:16351     2211    Pf3D7_01_v3     35510   12      84H70M147H      =       193086  157856      AAAAAAAAAAAAAAAAAAAAAAAAAAATTAAAAAAAAAATAAAAAAAAAACATAAAAAAAAAAAAAAAAA  AFGGGGGGGGGGGGGGGGGGGGGGG&gt;:,,CFGGCFGGGG,,@DF9EC*&gt;**,,,6&lt;?,**:8/?EEE8=8      NM:i:6  MD:Z:39A1T4T0T2A11T7    MC:Z:80M4D3M1I193M      AS:i:40 XS:i:33 RG:Z:001        SA:Z:Pf3D7_01_v3,193086,+,111M190S,60,1;    XA:Z:Pf3D7_09_v3,-1458764,146S20M1I30M104S,3;\nM05795:43:000000000-CFLMP:1:1114:12861:20875    2179    Pf3D7_01_v3     67176   0       70H30M199H      Pf3D7_07_v3     405766      0       TTTTTATTTAAAACACAAAAAAAAAAAAAA  ,38,,,,,,,,,8,,,&gt;*,,355C7&lt;@@7@  NM:i:0  MD:Z:30 MC:Z:146M155H   AS:i:30 XS:i:0      RG:Z:001        SA:Z:Pf3D7_07_v3,405968,-,242S13M1I43M,0,2;Pf3D7_12_v3,1133014,-,159S38M102S,0,1;\nM05795:43:000000000-CFLMP:1:1119:27790:13377    145     Pf3D7_01_v3     109767  0       174S39M88S      Pf3D7_07_v3     405766      0       TGTCACACACGTAGTGTTGTGACAGTCGTATCAGTGTATTATTGTACGTCTGTTTGTTTATTATATTTTATTATTGTGTTTTTGTTATTATGTGTTGATGTGTTAACGGAGAGTGTAGAGTAAGAGGAAAGTATTTTAGTATTTTTAATATTTTTAATTTTAAAATTAAAAATTTATTTTTTTTTTTTTTTTTTTTTTTTTTTTTAAATAATTAATTTTTTTTTAAATTTGTTTTAAATTAAATTGGTTTTTTGTTTAAAAAAATTTTTATTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT   )-).(-(((((()4.()))6).(((4()444.)))46.)6))-((-((.((,.()))))))))6)6)--)))),((4,(((-.))-)..)((),)),)((-)(2.(((,)()))*-***)***9**/***01**2*0*+2***2*20+3*0++++++3++++0++2*+&lt;300+2+**CGEEGEDGGGGGGECGGEE&gt;GGE@,D3,,,,,3,,,,,*3++,,,,,3,,,,,,,,,,8,,@,,3A+3+CCD8AFC,AB=4+,FC,4,,9,:,:@@EGEGGFGGGGGGGGGGGGGGGGGCCCCC       NM:i:0  MD:Z:39 MC:Z:128M7I119M45S      AS:i:39 XS:i:38 RG:Z:001        SA:Z:Pf3D7_09_v3,683170,-,263S38M,0,0;  XA:Z:Pf3D7_05_v3,+166830,86S38M177S,0;Pf3D7_10_v3,+448221,90S38M173S,0;Pf3D7_01_v3,-519133,169S35M97S,0;Pf3D7_07_v3,-399258,171S35M95S,0;Pf3D7_14_v3,-2709009,170S35M96S,0;\nM05795:43:000000000-CFLMP:1:2109:24931:7426     2145    Pf3D7_01_v3     134097  0       236H16M3I46M    Pf3D7_12_v3     2093554     0       AACAAAACAAAAAAAACATACAAATAAATACAAACAAAAAAAAAAAAAAAAAAAAAAACAAAAAA       .*)(,)0(77),&lt;012(4))*)*.).).)))))5((,.(-(-((27((4(-91-(70&gt;(4.334(   NM:i:7  MD:Z:17G3C3A31C4        MC:Z:25H276M    AS:i:33 XS:i:32 RG:Z:001        SA:Z:Pf3D7_01_v3,193086,+,111M190S,60,2;Pf3D7_06_v3,932942,-,146S29M2I29M95S,8,3;   XA:Z:Pf3D7_09_v3,+1284793,269S32M,0;Pf3D7_12_v3,-659692,4S30M267S,0;\nM05795:43:000000000-CFLMP:1:2116:28002:9063     2211    Pf3D7_01_v3     134127  18      242H33M26H      =       193113  59239       ACAAAAAAAAAAAAAAAAAAAAAAACACAAAAA       .()).(4&gt;B9(((-7(39&gt;91-341(3(,,6(0       NM:i:0  MD:Z:33 MC:Z:27H53M4D3M1I193M       AS:i:33 XS:i:0  RG:Z:001        SA:Z:Pf3D7_01_v3,193086,+,110M191S,60,0;Pf3D7_12_v3,448676,-,138S27M1I22M3I24M86S,0,7;\nM05795:43:000000000-CFLMP:1:1113:17834:11827    2161    Pf3D7_01_v3     137459  4       56H74M171H      =       196019  58732       TTATTGTTTTTTTTTTTATCGTTATTGTTTTTGTTATTATTATTATTGTTTTTGTTATTATTATTATTGTTTTT      9),&lt;:2&gt;EEB@&lt;?;+2(0))*;+*;A987GA=;7;+++31++*7F&lt;;**GGF?*:=,++,,+1=,&gt;5F?8EE@;  NM:i:4  MD:Z:11A2A4T0A53        MC:Z:56H245M    AS:i:54 XS:i:50 RG:Z:001        SA:Z:Pf3D7_08_v3,1374826,+,41M9D29M231S,6,9;        XA:Z:Pf3D7_01_v3,-137459,77S80M144S,6;Pf3D7_01_v3,+196068,123S99M79S,10;\nM05795:43:000000000-CFLMP:1:1111:24375:10578    99      Pf3D7_01_v3     139572  0       84S32M185S      =       139572  32 CTTGAACCCAGGAGGTAGAGGTTGCAGTGAGCCGAGATCGCACCCCTGCACTCCAGCCTGGGGGACAGAGTGAGACTCTGTCTCAAAATAAATAAATAAATAAATAAACAAATAAAAGATAACTGTTTAATTAACAACAACGTAATAGATAGTGTGTGGCACTGATACCACCTGTAAAAATAAAATGAACAACGGTGTAAAGACCAAGAGGAGAGAAATGGAAGTGTCCTATTGTAAGCTTCTCTTACTCTACGAAAAATGGTATATTACTTAAGAATAAACTATGATAATTAAGACGGCA    CCCCCFGGGGGGGGGGGFGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGFGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGCEGGGGGGGGGGGGGGGGGGGGCGGEGGGGGGGGGGDC3;DGGGGGGGGGGGG7FD9BFGGGFFFAA;&gt;FB@FEFFBBCFCEFF&gt;0&gt;B&gt;AEFEEF:5=B59FDAEFFFECEFFFCFF7&lt;64CC4=.75)7797&lt;)9CE))(-4-   NM:i:0  MD:Z:32     MC:Z:80S32M188S AS:i:32 XS:i:32 RG:Z:001        XA:Z:Pf3D7_06_v3,+225165,84S32M185S,0;\nM05795:43:000000000-CFLMP:1:1111:24375:10578    147     Pf3D7_01_v3     139572  0       80S32M188S      =       139572  -32AACCCAGGAGGTAGGGGTTGCAGTGAGCCGAGATCGCACCCCTGCAGTCCAGCCGGGGGGACAGAGTGAGACTCTGTCTCAAAATAAATAAATAAATAAATAAACAAATAAAAGATAACTGTTTAATTAACAACAACGTAATAGATAGTGTGTGGCACTGATACCACCTGTAAAAATAAAATGAACAACGGTGTAAAGACCAAGAGGAGAGAAATGGAAGTGTCCTATTGTAAGCTTCTCTTACTCTAAGAAAAATGGTATATTACTTAAGAATAAACTATGATAATTAAGGCTGCATTT     4,(563?;3@2;:6(:4)E7.);)5(96-0(FD&gt;2==@:252&gt;7)85DA8=1):):DBEED&gt;8:*&gt;;;::D:53:5:)@7FEDEDDEFCDE9&gt;EEEC;7ED:099;9;+&gt;D=C?C?96B?D7D9EBGGGD8GGFFGGCGGGGGEF8@@,GGGGGGGGGDGFFC6GGGGGGGGFGGGEGGGGGGGGCEGGGGGGCGGGGGGGGGF:GGGGGGFCGGGDGFFDGFGFGGGGGGGGGFGGGFGGGFGGGGGGEGFE9GFGGGGGGGGFFFFGGGGGGGGGFFGGFE&lt;GFAGGGGGGGGCCCCC    NM:i:0  MD:Z:32     MC:Z:84S32M185S AS:i:32 XS:i:32 RG:Z:001        XA:Z:Pf3D7_06_v3,-225165,80S32M188S,0;\nAside from the header info, every other line in the SAM file corresponds to a particular read and provides information on where in the reference genome it was mapped and the quality of this mapping8, alongside various other metadata (including the original Phred quality scores). This information is stored in tab-delimited fields (11 mandatory ones, followed by optional ones).\n8 Mapping quality is similar to the per-base quality scores we introduced in the previous chapter, but here they represent the probability of a wrong alignment of the entire read. More info here and here.\n\n\nCol\nField\nType\nBrief description\n\n\n\n\n1\nQNAME\nString\nQuery template NAME\n\n\n2\nFLAG\nInt\nbitwise FLAG\n\n\n3\nRNAME\nString\nReferences sequence NAME\n\n\n4\nPOS\nInt\n1- based leftmost mapping POSition\n\n\n5\nMAPQ\nInt\nMAPping Quality\n\n\n6\nCIGAR\nString\nCIGAR string\n\n\n7\nRNEXT\nString\nRef. name of the mate/next read\n\n\n8\nPNEXT\nInt\nPosition of the mate/next read\n\n\n9\nTLEN\nInt\nobserved Template LENgth\n\n\n10\nSEQ\nString\nsegment SEQuence\n\n\n11\nQUAL\nString\nASCII of Phred-scaled base QUALity+33\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAll details on the SAM format can be found in the format specification: https://samtools.github.io/hts-specs/SAMv1.pdf.9\nOne important column is the bitwise FLAG: it is a lookup code that can tell you (or more commonly, a software tool) whether a read was aligned, how it is oriented, if its pair was aligned, etc.\n\n\n\n\n\n\n\n\n\nBit\nDescription\n\n\n\n\n1\n0x1\ntemplate having multiple segments in sequencing\n\n\n2\n0x2\neach segment properly aligned according to the aligner\n\n\n4\n0x4\nsegment unmapped\n\n\n8\n0x8\nnext segment in the template unmapped\n\n\n16\n0x10\nSEQ being reverse complemented\n\n\n32\n0x20\nSEQ of the next segment in the template being reverse complemented\n\n\n64\n0x40\nthe first segment in the template\n\n\n128\n0x80\nthe last segment in the template\n\n\n256\n0x100\nsecondary alignment\n\n\n512\n0x200\nnot passing filters, such as platform/vendor quality controls\n\n\n1024\n0x400\nPCR or optical duplicate\n\n\n2048\n0x800\nsupplementary alignment\n\n\n\nLastly, we want to share this convenient tool to explain the different bitwise FLAGs. It is invaluable whenever you need to perform (or interpret) a samtools filtering operation.\n\n\n9 A more in-depth exploration of the different type of alignments that can occur (e.g., supplementary, secondary, etc.) is given in this blogpost.10 An amazing guide to several samtools tasks can be found on the Learning the BAM format page by Dave Tang.To convert a SAM to a BAM file, you can use the samtools software. samtools is used to convert between SAM and BAM files, but it can perform many other tasks too, like coordinate sorting, which we will talk about first.10",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Alignment to reference genome</span>"
    ]
  },
  {
    "objectID": "content/genomics/mapping.html#coordinate-sorting-alignment-files",
    "href": "content/genomics/mapping.html#coordinate-sorting-alignment-files",
    "title": "14  Alignment to reference genome",
    "section": "14.5 Coordinate sorting alignment files",
    "text": "14.5 Coordinate sorting alignment files\nThe reads in a SAM/BAM file can be sorted in different ways. For our needs (variant calling using GATK), we will need to sort them according to their location on the reference genome: a coordinate-ordered format.\nThe neat thing is that samtools sort will automatically do the conversion from SAM to BAM for us, so we can perform both steps in one go.\nsamtools sort -@ &lt;number_of_threads&gt; -o &lt;output.bam&gt; &lt;input.sam&gt;\nCan we make things even more streamlined and combine the bwa mapping and samtools sorting steps? Turns out we can!\n\n\n\n\n\n\nChaining commands via pipes\n\n\n\nRecall what you learned in Section 7.1 about piping the output of one command into the input of another using the | operator. We can make use of this to feed the output of bwa mem directly into samtools. This allows us to forego the need to write the output to any intermediate files.11\nThe only difference compared to before is that we now remove the -o output.sam option from the bwa mem command and that we do not provide an explicit input file to samtools (it is read from stdout directly instead).12\nbwa mem &lt;arguments&gt; | samtools sort -o alignment.sort.bam\nFor clarity we use the sorted.bam file suffix for any sorted BAM files.\n\n\n12 One final thing we could do is use &gt; to write the output of samtools to a file directly, instead of using the -o flag.11 This works out of the box, but you might encounter the syntax command_1 | command_2 -, where the hyphen - is a way of explicitly telling the second command to look at stdin for its input. For many tools, including samtools, this is not necessary however.\n\n\n\n\n\nExercise\n\n\n\n\nAlign the same FASTQ file as you did in the previous exercise, but immediately pipe the bwa mem output to samtools for coordinate sorting and conversion to a .bam file.\nCompare the size of the original .sam and the new .bam file (du -sh).\nModify your mapping script to immediately perform compression and sorting using the samtools sort command, without producing any intermediate files.",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Alignment to reference genome</span>"
    ]
  },
  {
    "objectID": "content/genomics/mapping.html#sec-samtools-flagstat",
    "href": "content/genomics/mapping.html#sec-samtools-flagstat",
    "title": "14  Alignment to reference genome",
    "section": "14.6 Inspecting BAM files",
    "text": "14.6 Inspecting BAM files\nYou can read BAM files by using the samtools view command. By default it only outputs the tab-delimited parts of the file, but you can provide the flags -H or-h` to optionally show only the header or everything.\nAnother useful command is samtools flagstat, which reports various mapping statistics on how well the reads aligned to your reference genome:\nsamtools flagstat training/results/bwa/PF0097_S43_L001.bam\n104559 + 0 in total (QC-passed reads + QC-failed reads)\n95962 + 0 primary\n0 + 0 secondary\n8597 + 0 supplementary\n0 + 0 duplicates\n0 + 0 primary duplicates\n103657 + 0 mapped (99.14% : N/A)\n95060 + 0 primary mapped (99.06% : N/A)\n95962 + 0 paired in sequencing\n47981 + 0 read1\n47981 + 0 read2\n89790 + 0 properly paired (93.57% : N/A)\n94736 + 0 with itself and mate mapped\n324 + 0 singletons (0.34% : N/A)\n4246 + 0 with mate mapped to a different chr\n3834 + 0 with mate mapped to a different chr (mapQ&gt;=5)\n\n\n\n\n\n\nExercises\n\n\n\n\nExplore the output of the samtools flagstat command. What do the different statistics mean?\nHow can you store the mapping statistics in a file? What do you need to take into consideration when doing this in a for loop for multiple bam files?\nTry to write a script for processing all FASTQ files belonging to the Peruvian P. falciparum AmpliSeq data in one go. You can inspect the provided scripts map.sh and map-extended.sh for inspiration, especially with regards to file paths and structuring your code.\n\nBonus exercise: do the same for the Vietnam P. vivax WGS data.\n\n\n\n\n\n\n\n\nUpping your scripting game\n\n\n\nWhen you’re just starting out, it is fine to use simple scripts using just the concepts that you know. As you become more experienced though, and as your scripts might need to be used by others and re-use it for a longer time, you should try to focus on making them more readable, robust and efficient. For the former, focus on doing things in a clear and structured way; use code comments, clear file names, split your scripts into logical chunks when necessary, provide READMEs, etc. Writing robust and efficient scripts is more like a never-ending journey, where you can pick up new techniques and ideas even after a lot of experience, and it might extend beyond the use of just bash, but into other languages or even workflow managers. You can compare the normal and the extended versions of the scripts in ./training/scripts for some ideas.\nFor now, knowing enough to get by, and being careful when trying things out, is most important. Howvever, if you decide in the future that really want to master bash, there are also several resources availabe that explain some of the more common beginner mistakes. Alternatively, you can look into workflow tools like Nextflow (and the nf-core community pipelines), Snakemake or Makefiles.\n\n\n\n14.6.1 More indices\nLike our reference FASTA files, BAM files can also be indexed to improve the performance of downstream tools that need to search for particular reads in this large file. For example, it is required for visualisation with IGV (Section 14.9) and variant calling with GATK (Chapter 15).\nsamtools index alignment.sort.bam\n\n\n\n\n\n\nExercise\n\n\n\nTry to create an index for one of your BAM files. Which new files are created?",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Alignment to reference genome</span>"
    ]
  },
  {
    "objectID": "content/genomics/mapping.html#sec-picard-duplicates",
    "href": "content/genomics/mapping.html#sec-picard-duplicates",
    "title": "14  Alignment to reference genome",
    "section": "14.7 Mark PCR duplicates - picard MarkDuplicates",
    "text": "14.7 Mark PCR duplicates - picard MarkDuplicates\n\n\n\n\n\n\nPCR duplicates tagging/removal should NOT be performed for AmpliSeq data\n\n\n\nThe library construction process for AmpliSeq results in identical fragments or duplicates for each of the targeted amplicons, hence we should not attempt to remove them.13\n\n\n13 As a side note, in RNA-seq this duplicate removal step is also not recommended (unless unique molecular identifiers (UMIs) are used), although the number of duplicates can be used for quality control.14 You can read more about how PCR duplicates arise in this excellent blog post by Eric Minikel.A final step in the mapping pipeline is the tagging (or removal) of PCR duplicates. These are identical sequences that can arise because of PCR steps during library preparation or because a cluster on the flow cell was erroneously detected as two separate ones14. The overall goal of PCR duplicate removal is to reduce bias during downstream analyses (like variant detection) due to their presence.\nPCR duplicates are polyclonal molecules that do not represent any true biological variation, nor are they independent observations of the DNA that was present in a sample. Thus, the observed abundance of these identical reads does not accurately reflect the true abundance of the different molecules in the sample(Rochette et al. 2023). If a particular PCR duplicate happens to be over-represented compared to other reads covering the same region of the genome, this could cause a bias in our analyses. Just imagine what would happen if during PCR amplification an error was introduced into one of the fragments. If we ended up with many PCR duplicates of this fragment, we might erroneously conclude that there is a SNP at this position because we see it occur in multiple reads. However, if we remove PCR duplicates, we will only observe the SNP in a single read, and variant detection will not mark this as a SNP due to lack of confidence (~agreement between different reads covering the same position, more on that in Chapter 15).\nOne of the tools that we use for marking PCR duplicates is Picard. Picard actually provides many different functionalities, but the one that we’re interested for now is MarkDuplicates. An in-depth explanation of how it works can be found here. After running the tool, we end up with a BAM file again, but this time with all suspected PCR duplicates marked as such. Our downstream analysis tools will take this information into account whenever relevant (e.g., when detecting variants).15\n15 Note that removal generally is not necessary, but it can be done by passing the -REMOVE_DUPLICATES true flag.\n\n\n\n\n\nExercises\n\n\n\n\nTry to mark all PCR duplicates for a single BAM file using the following command:\n\npicard MarkDuplicates\n    -I alignment.sort.bam\n    -O alignment.sort.dups.bam\n    -M alignment.markeddups_metrics.txt\n\nRun samtools flagstat again. What changed compared to the previous report?\nExamine and run the remove_dups.sh script to process all files.\n\nBonus exercise: Try to combine all the steps of the pipeline we have seen so far (including trimming) into a single script.\n\n\n\n\n\n\n\n\nRemember to use meaningful file names\n\n\n\nYou might have noticed in the example above that we continued to extend the suffix of the BAM file to indicate what has happened to it: sorting and PCR duplicate marking. This information is also kept track of in the BAM header, but it is a good habit to still look for appropriate file names whenever possible.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe pipeline that we’re introducing here deviates from the GATK4 best practices in this section, because we do not introduce the concept of base quality score recalibration (BQSR). The basic idea here is that we want to detect and correct systematic errors made by the sequencer, by using our knowledge of known variants to model them.\nThe above link by GATK gives an in-depth explanation on the topic, but you can also find some interesting discussions here, here and here as well as some hands-on walkthroughs here (section 6) and here.",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Alignment to reference genome</span>"
    ]
  },
  {
    "objectID": "content/genomics/mapping.html#sec-picard-metrics",
    "href": "content/genomics/mapping.html#sec-picard-metrics",
    "title": "14  Alignment to reference genome",
    "section": "14.8 Alignment statistics",
    "text": "14.8 Alignment statistics\nWe already introduced the samtools flagstat command to report on basic alignment statistics, but a more in-depth method of inspecting your alignment is provided by Picard’s CollectAlignmentSummaryMetrics, or the more extensive CollectMultipleMetrics (we will see Picard again in the next section on PCR duplicate removal):\npicard CollectAlignmentSummaryMetrics R=&lt;reference.fasta&gt; I=&lt;input.sort.bam&gt; O=&lt;output.bam&gt;\nThe output of these tools can again be aggregated using a tool like MulitQC. See here for an example.",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Alignment to reference genome</span>"
    ]
  },
  {
    "objectID": "content/genomics/mapping.html#sec-alignment-visualisation",
    "href": "content/genomics/mapping.html#sec-alignment-visualisation",
    "title": "14  Alignment to reference genome",
    "section": "14.9 Alignment visualisation",
    "text": "14.9 Alignment visualisation\nAfter creating and filtering our alignment files, we can visualise them in a tool like IGV (Integrative Genomics Viewer). We refer to this section of the Galaxy training materials (Hiltemann et al. 2023; Wolff, Batut, and Rasche).\n\n\n\n\nClevaLab. 2022. “A Step-by-Step Guide to NGS.” https://www.clevalab.com/post/a-step-by-step-guide-to-ngs.\n\n\nHiltemann, Saskia, Helena Rasche, Simon Gladman, Hans-Rudolf Hotz, Delphine Larivière, Daniel Blankenberg, Pratik D. Jagtap, et al. 2023. “Galaxy Training: A Powerful Framework for Teaching!” Edited by Francis Ouellette. PLoS Comput Biol Computational Biology 19 (1): e1010752. https://doi.org/10.1371/journal.pcbi.1010752.\n\n\nMalariaGEN, Muzamil Mahdi Abdel Hamid, Mohamed Hassan Abdelraheem, Desmond Omane Acheampong, Ambroise Ahouidi, Mozam Ali, Jacob Almagro-Garcia, et al. 2023. “Pf7: An Open Dataset of Plasmodium Falciparum Genome Variation in 20,000 Worldwide Samples.” Wellcome Open Research 8 (January): 22. https://doi.org/10.12688/wellcomeopenres.18681.1.\n\n\nRochette, Nicolas C., Angel G. Rivera‐Colón, Jessica Walsh, Thomas J. Sanger, Shane C. Campbell‐Staton, and Julian M. Catchen. 2023. “On the Causes, Consequences, and Avoidance of &lt;Span Style=\"font-Variant:small-Caps;\"&gt;PCR&lt;/Span&gt; Duplicates: Towards a Theory of Library Complexity.” Molecular Ecology Resources 23 (6): 1299–1318. https://doi.org/10.1111/1755-0998.13800.\n\n\nWolff, Joachim, Bérénice Batut, and Helena Rasche. “Mapping (Galaxy Training Materials).” https://training.galaxyproject.org/training-material/topics/sequence-analysis/tutorials/mapping/tutorial.html.",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Alignment to reference genome</span>"
    ]
  },
  {
    "objectID": "content/genomics/variant-calling.html",
    "href": "content/genomics/variant-calling.html",
    "title": "15  Variant calling",
    "section": "",
    "text": "15.1 Types of variants\nThe previous steps of our pipeline were concerned with 1) the quality control of our sequence reads and 2) where all these reads fit on our reference genome. In this final step we will assess the genetic differences between each of our samples and the reference genome. This process is called variant calling or variant detection. While conceptually simple, the main challenge is that it can be difficult to discern sequencing errors from real variants.\nThe tools and steps involved in the variant calling process are outlined in the diagram below.\nVariants can be identified by looking at all the reads at a given position in the genome and looking at the variation present in the different reads covering the position. The more reads that we see that agree on a specific variant compared to the reference genome, the more confident we can be that we are dealing with a real variant, in that particular sample. Of course, there are many considerations to make at this point; we need to take into account the possibility of sequencing errors, mapping errors, the ploidy of our organism, the possibility of complex infections (i.e., multiple genetically distinct parasites in a single sample, instead of a single clone), etc. Moreover, we can compare variants across different samples and perform various filtering steps to reduce the chance of false positives.\nWe will make use of GATK4 (Genomic Analysis ToolKit) for the variant detection components of our pipeline. This is a very extensive toolkit which offers many different utilities.1\nWe can identify several different types of genetic variants:\nThe GATK variant discovery pipeline that we introduce below focuses on short (germline, as opposed to somatic) variants, i.e. SNPs and indels.",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Variant calling</span>"
    ]
  },
  {
    "objectID": "content/genomics/variant-calling.html#types-of-variants",
    "href": "content/genomics/variant-calling.html#types-of-variants",
    "title": "15  Variant calling",
    "section": "",
    "text": "Single nucleotide polymorphism (SNPs)2\n\nIf a SNP results in an amino acid change, we refer to it as a nonsynonomous substitution (resulting in a &&missense mutation). If there is no change in the amino acid sequence (thanks to codon redundancy), it is referred to as a synonomous substitution.\nIf a SNP introduces a stop-codon and results in premature termination of mRNA translation, this is termed a nonsense mutation, if it removes a stop-codon, it is called a nonstop mutation.\n\nShort insertions and deletions (indels)\n\nIf the indel is not a multiple of three bases, this leads to a frameshift mutation.\n\nStructural variants (SVs): larger (chromosomal) rearrangements (&gt; 50 base pairs) like insertions, deletions, inversions, translocations or duplications.\nCopy number variation (CNVs): a particular type of SV caused by the duplication or deletion of a particular DNA segment (e.g., hrp2 and hrp3 genes in P. falciparum).\n\n2 A related term is the single nucleotide variation (SNV). It appears to be more commonly used for somatic rather than germline events, although others use it to describe single nucleotide changes below a certain frequency threshold.\n\n\n\n\n\n\nStructural variant discovery\n\n\n\nLarger structural variant discovery tends to be more challenging than short variant detection. Long-reads help quite a bit, as does paired-end sequencing. Similar considerations apply for calling haplotypes (~ combination of nearby variants, or more strictly: a physical grouping of genetic variants that is inherited together).\n\n\n\nDetecting structural variants. Source: Baker (2012). Structural Variation: The Genome’s Hidden Architecture. https://doi.org/10.1038/nmeth.1858\n\n\nFurther reading:\n\nhttps://gatk.broadinstitute.org/hc/en-us/articles/9022476791323-Structural-Variants\nMahmoud, et al. 2019. Structural variant calling: the long and the short of it\nCollins, et al. 2020. A structural variation reference for medical and population genetics\n\n\n\n\n15.1.1 Even more indices\nSeveral of the GATK tools require us to index the reference genome again(!), but this time using samtools faidx ref.fasta, resulting in a .fai indexed reference file. Like before, this creates a new file in the reference genome directory, with the extension .fai. Additionally, we also need a .dict dictionary file, which can be created using gatk CreateSequenceDictionary –R ref.fasta.\n\n\n\n\n\n\nExercises\n\n\n\n\nCreate the two required reference index files for both of your Plasmodium reference genomes.",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Variant calling</span>"
    ]
  },
  {
    "objectID": "content/genomics/variant-calling.html#calling-short-variants---gatk-haplotypecaller",
    "href": "content/genomics/variant-calling.html#calling-short-variants---gatk-haplotypecaller",
    "title": "15  Variant calling",
    "section": "15.2 Calling (short) variants - GATK HaplotypeCaller",
    "text": "15.2 Calling (short) variants - GATK HaplotypeCaller\n\n\n\n\n\n\nNote\n\n\n\nJust like the other steps in our pipeline, there exist many different algorithmic implementations of variant calling, each with their own strengths, weaknesses and intended use cases.\nAside from GATK HaplotypeCaller, you might encounter\n\nbcftools mpileup (position-based caller, made by the same developers as samtools)\nFreeBayes\n\n\n\nWe will use the GATK HaplotypeCaller tool to simultaneously detect both SNPs and indels. It uses local de novo assembly of haplotype in regions that show signs of variation. You can find out more about the underlying process in the GATK documentation or in this GATK article.\nThe tool requires two\ngatk HaplotypeCaller \\\n    -R reference.fasta \\\n    -I alignment.sort.dup.bam \\\n    -O output.g.vcf.gz\" \\\n    -ERC GVCF\n\n# optional arguments:\n# set number of threads: --native-pair-hmm-threads 2\n# set maximum and initial memory usage: --java-options \"-Xmx4g -Xms4g\"\nThe output of this process is a GVCF file. It is very similar to the VCF file format, except that it tracks each genomic coordinate instead of only those positions where variations were detected. Also note that the output is compressed with gzip by default.\n\n\n\n\n\n\nExercises\n\n\n\n\nRun gatk HaplotypeCaller for a single BAM file and inspect the resulting VCF file.\nCreate your own script to call variants for all for the Peruvian P. falciparum AmpliSeq alignments. Afterwards, compare your approach with the call_variants.sh script and call_variants-extended.sh scripts.",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Variant calling</span>"
    ]
  },
  {
    "objectID": "content/genomics/variant-calling.html#vcf-files-revisited",
    "href": "content/genomics/variant-calling.html#vcf-files-revisited",
    "title": "15  Variant calling",
    "section": "15.3 VCF files revisited",
    "text": "15.3 VCF files revisited\nThe output of variant calling is a file in the Variant Call Format (.vcf), which we already encountered in Section 7.1.1.1.\n\n\n\nOverview of Variant Call Format (VCF). Source: https://vcftools.sourceforge.net/VCF-poster.pdf\n\n\nMore info can be found on on Dave Tang’s Learning the VCF page",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Variant calling</span>"
    ]
  },
  {
    "objectID": "content/genomics/variant-calling.html#sec-vcf-visualisation",
    "href": "content/genomics/variant-calling.html#sec-vcf-visualisation",
    "title": "15  Variant calling",
    "section": "15.4 Visualisation using IGV",
    "text": "15.4 Visualisation using IGV\nSimilar to our BAM files, we can visualise VCF files in tools like IGV (Integrative Genomics Viewer). For a tutorial on this, we refer to this Data Carpentry workshop.",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Variant calling</span>"
    ]
  },
  {
    "objectID": "content/genomics/variant-calling.html#combine-samples",
    "href": "content/genomics/variant-calling.html#combine-samples",
    "title": "15  Variant calling",
    "section": "15.5 Combine samples",
    "text": "15.5 Combine samples\nAfter the previous step, we ended up with a GVCF file for each of our samples. Now we will merge these samples into a single multi-sample GVCF file using either GATK CombineGVCFs (docs) or GATK GenomicsDBImport (docs)3\n3 The latter is slightly faster, while the former makes it easier to work with multiple intervals. See this tutorial for an example on how to use CombineGVCFs.gatk GenomicsDBImport \\\n    --genomicsdb-workspace-path /path/to/store/database \\\n    --intervals &lt;genomic-range&gt; \\\n    --intervals &lt;genomic-range&gt; \\\n    # ...\n    --sample-name-map &lt;cohort.sample_map&gt; \\\n    --tmp-dir /path/to/large/tmp\n# optional arguments:\n# set maximum and initial memory usage: --java-options \"-Xmx4g -Xms4g\"\n# number of samples to process simultaneously (affects memory): --batch-size 50\nThe cohort.sample_map file should contain a list of sample and file names in tab-delimited format:\nsample1      sample1.vcf.gz\nsample2      sample2.vcf.gz\nsample3      sample3.vcf.gz\nFor a small number of samples, you can also provide each of them directly using multipe -V flags:\ngatk GenomicsDBImport \\\n    -V sample_1.g.vcf.gz \\\n    -V sample_2.g.vcf.gz \\\n    -V sample_3.g.vcf.gz \\\n    --genomicsdb-workspace-path /path/to/store/output/database \\\n    --tmp-dir=/path/to/large/tmp \\\n    -L &lt;genomic-interval&gt;\nThe -L (or --intervals) option is used to define the genomic intervals over which the tool operates. Unfortunately, these intervals need to be prepared and provided manually. For the Plasmodium falciparum Peru AmpliSeq panel, you can use:\n\n\nClick me to show the full list of intervals:\n\n-L Pf3D7_01_v3:179679-179985 \\\n-L Pf3D7_01_v3:180375-180467 \\\n-L Pf3D7_01_v3:190106-191218 \\\n-L Pf3D7_01_v3:191661-192230 \\\n-L Pf3D7_01_v3:192264-192925 \\\n-L Pf3D7_01_v3:192961-194344 \\\n-L Pf3D7_01_v3:194371-195570 \\\n-L Pf3D7_01_v3:195762-196800 \\\n-L Pf3D7_01_v3:196836-200312 \\\n-L Pf3D7_01_v3:200448-200978 \\\n-L Pf3D7_01_v3:204949-205193 \\\n-L Pf3D7_01_v3:339163-339466 \\\n-L Pf3D7_01_v3:464638-466535 \\\n-L Pf3D7_01_v3:466803-470193 \\\n-L Pf3D7_02_v3:519270-519567 \\\n-L Pf3D7_02_v3:694157-694456 \\\n-L Pf3D7_03_v3:361017-361320 \\\n-L Pf3D7_03_v3:849331-849578 \\\n-L Pf3D7_04_v3:532089-532395 \\\n-L Pf3D7_04_v3:691941-692036 \\\n-L Pf3D7_04_v3:747935-749935 \\\n-L Pf3D7_04_v3:770125-770439 \\\n-L Pf3D7_05_v3:921740-921999 \\\n-L Pf3D7_05_v3:957777-958361 \\\n-L Pf3D7_05_v3:958396-959096 \\\n-L Pf3D7_05_v3:959153-959988 \\\n-L Pf3D7_05_v3:960166-961735 \\\n-L Pf3D7_05_v3:962020-962193 \\\n-L Pf3D7_05_v3:1188236-1188508 \\\n-L Pf3D7_05_v3:1214304-1214615 \\\n-L Pf3D7_06_v3:148637-148858 \\\n-L Pf3D7_06_v3:635908-636217 \\\n-L Pf3D7_07_v3:403083-404022 \\\n-L Pf3D7_07_v3:404296-406440 \\\n-L Pf3D7_07_v3:455433-455668 \\\n-L Pf3D7_07_v3:782049-782219 \\\n-L Pf3D7_08_v3:500969-501114 \\\n-L Pf3D7_08_v3:548041-549952 \\\n-L Pf3D7_08_v3:549982-550332 \\\n-L Pf3D7_08_v3:803010-803329 \\\n-L Pf3D7_08_v3:1373986-1374244 \\\n-L Pf3D7_08_v3:1374249-1374474 \\\n-L Pf3D7_08_v3:1374486-1374705 \\\n-L Pf3D7_08_v3:1374711-1375419 \\\n-L Pf3D7_09_v3:230922-231210 \\\n-L Pf3D7_09_v3:1005112-1005419 \\\n-L Pf3D7_10_v3:340949-341230 \\\n-L Pf3D7_10_v3:1172615-1172823 \\\n-L Pf3D7_11_v3:416182-416488 \\\n-L Pf3D7_11_v3:874927-875071 \\\n-L Pf3D7_11_v3:1294408-1295115 \\\n-L Pf3D7_11_v3:1295124-1295390 \\\n-L Pf3D7_11_v3:1295423-1295672 \\\n-L Pf3D7_11_v3:1505334-1505635 \\\n-L Pf3D7_12_v3:717790-718460 \\\n-L Pf3D7_12_v3:718476-719820 \\\n-L Pf3D7_12_v3:1126866-1127122 \\\n-L Pf3D7_12_v3:1552012-1552292 \\\n-L Pf3D7_12_v3:1611167-1611448 \\\n-L Pf3D7_12_v3:2091976-2094267 \\\n-L Pf3D7_13_v3:1595960-1596123 \\\n-L Pf3D7_13_v3:1724599-1727164 \\\n-L Pf3D7_13_v3:1827453-1827763 \\\n-L Pf3D7_13_v3:2503123-2503359 \\\n-L Pf3D7_13_v3:2503390-2505580 \\\n-L Pf3D7_13_v3:2840639-2841793 \\\n-L Pf3D7_14_v3:293359-293982 \\\n-L Pf3D7_14_v3:294267-294832 \\\n-L Pf3D7_14_v3:832462-832742 \\\n-L Pf3D7_14_v3:1381895-1381990 \\\n-L Pf3D7_API_v3:24048-34146 \\\n-L Pf_M76611:3384-4634\n\nFor the other AmpliSeq panels, check out the pipelines by Eline Kattenberg on GitHub.\nFor whole-genome sequencing, the MalariaGEN Pf7 database uses 10 kilobase intervals (and -ip/--interval-padding 500 to add some padding) (see methods).\n\n\n\n\n\n\nImproving performance\n\n\n\nMerging GVCF files can be quite slow. GATK has written an article on how to address performance issues.\n\n\n\n\n\n\n\n\nExercises\n\n\n\nMerge the GVCFs for all Peruvian Plasmodium falciparum AmpliSeq samples.",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Variant calling</span>"
    ]
  },
  {
    "objectID": "content/genomics/variant-calling.html#joint-genotyping",
    "href": "content/genomics/variant-calling.html#joint-genotyping",
    "title": "15  Variant calling",
    "section": "15.6 Joint genotyping",
    "text": "15.6 Joint genotyping\nAfter combining the GVCF files for all our samples, it is time to perform joint genotyping using GATK GenotypeGVCFs (docs). The main idea behind this step is to improve the inference of variant detection by leveraging the information that is present across all samples in our cohort. For more information on the logic behind joint genotyping, we refer to this GATK article.\ngatk GenotypeGVCFs \\\n    -R reference.fasta \\\n    -V gendb://genomicsDB \\\n    -O variants.vcf.gz \\\n    --tmp-dir /path/to/large/tmp\n\n# optional arguments:\n# set maximum and initial memory usage: --java-options \"-Xmx4g -Xms4g\"\n\n\n\n\n\n\nExercises\n\n\n\n\nPerform joint-genotyping for the multi-sample VCF file with the Peruvian Plasmodium falciparum AmpliSeq samples.\nInspect and visualise the output VCF (optionally using IGV).\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nHave a look at GATK’s article on VCF files, in particular the section on the structure of variant call records and how to interpret them: https://gatk.broadinstitute.org/hc/en-us/articles/360035531692-VCF-Variant-Call-Format.",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Variant calling</span>"
    ]
  },
  {
    "objectID": "content/genomics/variant-calling.html#variant-filtering",
    "href": "content/genomics/variant-calling.html#variant-filtering",
    "title": "15  Variant calling",
    "section": "15.7 Variant filtering",
    "text": "15.7 Variant filtering\nNot all the variants present in our VCF file are likely to be real. Therefor, a final step in our pipeline is to filter out variants that are likely to be errors. We will discuss two different approaches: Variant Quality Score Recalibration (VQSR) (recommended by GATK) and hard-filtering.\nHard-filtering works by setting specific thresholds for one or multiple annotations (i.e. properties of a variant, like read coverage, proportion of forward/reverse reads, etc.) VQSR is a more powerful because it tries to combine all of these annotations to make a decision about good or bad variants, but to do this, it require a large number of samples and a database of well-curated known variants. Therefor, we do not recommend it for AmpliSeq data, but it is probably a good idea to use it for WGS data.\nThe GATK tool that performs hard-filtering is GATK VariantFiltration (docs). The two main options are --filter-name and --filter-expression, which allow you to set a particular threshold for an annotation. For example, the code below filters out variants which have a sequencing depth lower than 10 (DP &lt; 100):\ngatk --java-options \"-Xmx7g\" VariantFiltration \\\n    -R reference.fasta \\\n    -V variants.vcf.gz \\\n    -O variants.filter.vcf.gz \\\n    --filter-name \"DP100\" \\\n    --filter-expression \"DP &lt; 100\"\nRunning the above will create a VCF file where the filter column is said to PASS or not depending on the filters that were applied. In order to actually remove the suspected bad variants from the file, we still need to run the following:\ngatk/gatk SelectVariants \\\n    -R reference.fasta \\\n    -V variants.filter.vcf.gz \\\n    --exclude-filtered \\\n    -O variants.filter.pass.vcf.gz\n\n# or equivalently\nbcftools view -f 'PASS' -O vcf -o variants.filter.pass.vcf variants.filter.vcf\n\n\n\n\n\n\nHard-filtering should be done separately for SNPs and indels\n\n\n\nThe filtering steps should ideally be performed separately for both SNPs and indels. You can split your VCF file via:\n# create SNPs-only subset\ngatk SelectVariants \\\n    -V cohort.vcf.gz \\\n    -select-type SNP \\\n    -O snps.vcf.gz\n\n# create indels-only subset\ngatk SelectVariants \\\n    -V cohort.vcf.gz \\\n    -select-type INDEL \\\n    -O indels.vcf.gz\n    --select-type mixed\nAfter filtering, they can be merged back together.\nSee here (section 2) for more info on the --select-type flag.\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nFor hard-filtering AmpliSeq data, you can explore the P. falciparum AmpliSeq Peru pipeline.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFurther reading:\n\nOverview of annotations to filter on: https://gatk.broadinstitute.org/hc/en-us/articles/360035890471-Hard-filtering-germline-short-variants\nHands-on GATK filtering approaches: https://gatk.broadinstitute.org/hc/en-us/articles/360035531112–How-to-Filter-variants-either-with-VQSR-or-by-hard-filtering\nhttps://www.melbournebioinformatics.org.au/tutorials/tutorials/variant_calling_gatk1/variant_calling_gatk1/#section-4-filter-and-prepare-analysis-ready-variants\nhttps://speciationgenomics.github.io/filtering_vcfs/\nhttps://sib-swiss.github.io/NGS-variants-training/2021.9/day2/filtering_evaluation/\nhttps://qcb.ucla.edu/wp-content/uploads/sites/14/2016/03/IntroductiontoVariantCallsetEvaluationandFilteringTutorialAppendix-LA2016.pdf\nhttps://app.terra.bio/#workspaces/help-gatk/GATKTutorials-Germline",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Variant calling</span>"
    ]
  },
  {
    "objectID": "content/genomics/variant-calling.html#annotation-of-variants",
    "href": "content/genomics/variant-calling.html#annotation-of-variants",
    "title": "15  Variant calling",
    "section": "15.8 Annotation of variants",
    "text": "15.8 Annotation of variants\nThe final step in our pipeline is to annotate variants. For this, a tool like snpEff can be used. You can see it being used in the AmpliSeq pipeline here.",
    "crumbs": [
      "Processing genomic data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Variant calling</span>"
    ]
  },
  {
    "objectID": "content/references.html",
    "href": "content/references.html",
    "title": "References",
    "section": "",
    "text": "Batut, Bérénice, Maria Doyle, Alexandre Cormier, Anthony Bretaudeau,\nLaura Leroi, Erwan Corre, Stéphanie Robin, gallantries, and Cameron\nHyde. “Quality Control (Galaxy Training Materials).” https://training.galaxyproject.org/training-material/topics/sequence-analysis/tutorials/quality-control/tutorial.html.\n\n\nClevaLab. 2022a. “A Step-by-Step Guide to NGS.” https://www.clevalab.com/post/a-step-by-step-guide-to-ngs.\n\n\n———. 2022b. “Next Generation Sequencing - a Step-by-Step Guide to\nDNA Sequencing.” YouTube. https://www.youtube.com/watch?v=WKAUtJQ69n8.\n\n\nHiltemann, Saskia, Helena Rasche, Simon Gladman, Hans-Rudolf Hotz,\nDelphine Larivière, Daniel Blankenberg, Pratik D. Jagtap, et al. 2023.\n“Galaxy Training: A Powerful Framework for Teaching!”\nEdited by Francis Ouellette. PLoS Comput Biol\nComputational Biology 19 (1): e1010752. https://doi.org/10.1371/journal.pcbi.1010752.\n\n\nIllumina. 2008. “Multiplexed Sequencing with the Illumina Genome\nAnalyzer System.” https://www.illumina.com/documents/products/datasheets/datasheet_sequencing_multiplex.pdf.\n\n\n———. 2016. “Overview of Illumina Sequencing by Synthesis\nWorkflow.” YouTube. https://www.youtube.com/watch?v=fCd6B5HRaZ8.\n\n\n———. 2017. “An Introduction to Next-Generation Sequencing\nTechnology.” https://www.illumina.com/content/dam/illumina-marketing/documents/products/illumina_sequencing_introduction.pdf.\n\n\n———. 2020. “How Short Inserts Affect Sequencing\nPerformance.” https://support.illumina.com/ko-kr/bulletins/2020/12/how-short-inserts-affect-sequencing-performance.html.\n\n\n———. 2021a. “Learn about Illumina’s Next-Generation Sequencing\nWorkflow.” YouTube. https://www.youtube.com/watch?v=oIJaA6h2bFM.\n\n\n———. 2021b. “Indexed Sequencing on Illumina Systems.” https://support-docs.illumina.com/SHARE/IndexedSeq/indexed-sequencing.pdf.\n\n\n———. n.d. “Next-Generation Sequencing Glossary.” NGS\nTerminology. https://www.illumina.com/science/technology/next-generation-sequencing/beginners/glossary.html.\n\n\nKattenberg, Johanna Helena, Carlos Fernandez-Miñope, Norbert J. Van\nDijk, Lidia Llacsahuanga Allcca, Pieter Guetens, Hugo O. Valdivia,\nJean-Pierre Van Geertruyden, et al. 2023. “Malaria Molecular\nSurveillance in the Peruvian Amazon with a Novel Highly Multiplexed\nPlasmodium Falciparum AmpliSeq Assay.” Edited by\nGemma Moncunill. Microbiology Spectrum 11 (2): e00960–22. https://doi.org/10.1128/spectrum.00960-22.\n\n\nKattenberg, Johanna Helena, Hong Van Nguyen, Hieu Luong Nguyen, Erin\nSauve, Ngoc Thi Hong Nguyen, Ana Chopo-Pizarro, Hidayat Trimarsanto, et\nal. 2022. “Novel Highly-Multiplexed AmpliSeq Targeted\nAssay for Plasmodium Vivax Genetic Surveillance Use Cases at Multiple\nGeographical Scales.” Frontiers in Cellular and Infection\nMicrobiology 12 (August): 953187. https://doi.org/10.3389/fcimb.2022.953187.\n\n\nLaunen, Loren. 2017. “Illumina Sequencing (for Dummies).”\nhttps://kscbioinformatics.wordpress.com/2017/02/13/illumina-sequencing-for-dummies-samples-are-sequenced/.\n\n\nLedergerber, C., and C. Dessimoz. 2011. “Base-Calling for\nNext-Generation Sequencing Platforms.” Briefings in\nBioinformatics 12 (5): 489–97. https://doi.org/10.1093/bib/bbq077.\n\n\nMalariaGEN, Muzamil Mahdi Abdel Hamid, Mohamed Hassan Abdelraheem,\nDesmond Omane Acheampong, Ambroise Ahouidi, Mozam Ali, Jacob\nAlmagro-Garcia, et al. 2023. “Pf7: An Open Dataset of Plasmodium\nFalciparum Genome Variation in 20,000 Worldwide Samples.”\nWellcome Open Research 8 (January): 22. https://doi.org/10.12688/wellcomeopenres.18681.1.\n\n\nPiper, Mary E., Meeta Mistry, Jihe Liu, William J. Gammerdinger, and\nRadhika S. Khetani. 2022.\n“Hbctraining/Intro-to-Rnaseq-Hpc-Salmon-Flipped: Introduction to\nRNA-Seq Using Salmon Lessons from HCBC (First\nRelease),” January. https://doi.org/10.5281/ZENODO.5833880.\n\n\nRausch, Tobias. 2022. “Methods in Genomic Variant Calling.”\nEMBL-EBI. https://doi.org/10.6019/TOL.GenomicVariantCalling-w.2022.00001.1\n.\n\n\nRochette, Nicolas C., Angel G. Rivera‐Colón, Jessica Walsh, Thomas J.\nSanger, Shane C. Campbell‐Staton, and Julian M. Catchen. 2023. “On\nthe Causes, Consequences, and Avoidance of &lt;Span\nStyle=\"font-Variant:small-Caps;\"&gt;PCR&lt;/Span&gt;\nDuplicates: Towards a Theory of Library Complexity.”\nMolecular Ecology Resources 23 (6): 1299–1318. https://doi.org/10.1111/1755-0998.13800.\n\n\nRodriguez, Raphaël, and Yamuna Krishnan. 2023. “The Chemistry of\nNext-Generation Sequencing.” Nature Biotechnology 41\n(12): 1709–15. https://doi.org/10.1038/s41587-023-01986-3.\n\n\nShendure, Jay, Shankar Balasubramanian, George M. Church, Walter\nGilbert, Jane Rogers, Jeffery A. Schloss, and Robert H. Waterston. 2017.\n“DNA Sequencing at 40: Past, Present and\nFuture.” Nature 550 (7676): 345–53. https://doi.org/10.1038/nature24286.\n\n\nUniversity of Exeter DNA sequencing service. “Introduction to\nGenomics.” https://biomedicalhub.github.io/genomics/01-part1-introduction.html.\n\n\nWolff, Joachim, Bérénice Batut, and Helena Rasche. “Mapping\n(Galaxy Training Materials).” https://training.galaxyproject.org/training-material/topics/sequence-analysis/tutorials/mapping/tutorial.html.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "content/unix/appendix-unix.html",
    "href": "content/unix/appendix-unix.html",
    "title": "Appendix A — Various Unix topics",
    "section": "",
    "text": "A.1 Tips and hints",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Various Unix topics</span>"
    ]
  },
  {
    "objectID": "content/unix/appendix-unix.html#sec-unix-tips",
    "href": "content/unix/appendix-unix.html#sec-unix-tips",
    "title": "Appendix A — Various Unix topics",
    "section": "",
    "text": "Naming conventions and cases\n\n\n\nNever (never!) use spaces in your file or directory names. This will only lead to pain… Instead, use hyphens (-) or underscores (_) to separate words. E.g., my_first_script and 3B207-2_S92_L001_R1_001.fastq.gz.\nAdditionally, unlike in Windows, in Unix everything is case-sensitive. Thus, /home/documents != /home/Documents. Be mindful of this when naming or pointing to files/directories.\n\n\n\n\n\n\n\n\nAutocompletion and command history\n\n\n\nAvoid unnecessary typing and just make things easy for yourself!\nWhile typing commands in the shell, you can almost always use the tab key for auto-completion. This will automatically type out paths, file names or known commands. If there exist multiple matches, a single press of tab will not appear to do anything, but if you press the button twice, a list of possible options will appear on your screen. This is incredibly useful, not only for speeding things up, but also for avoiding typos when dealing with long or complex file names.\nAn equally useful tool is your command history. While on the shell prompt, pressing the up arrow (↑) will bring up your most recent previous command. Pressing it again will cycle through the entire history, in reverse order. You can also search through your history by pressing ctrl+r allows you to search through your command history. Just start typing and you will see the search try to narrow down on the command that you are looking for. Once you find it, just press enter to run it directly or tab to copy it to your prompt (in case you still want to change it). The search form will look like this: (reverse-i-search)`world': echo \"Hello world!\"\n\n\n\n\n\n\n\n\nCopying and pasting\n\n\n\nCopying and pasting might work slightly different to what you are used to, depending on the terminal application that you are using. If ctrl+c and ctrl+v do not appear to work, you can tryctrl+shift+c and ctrl+shift+v instead. Often times, the mouse middle or right click can also be used for pasting.\nFor the native WSL terminal specifically, you can refer to this site for more info: https://devblogs.microsoft.com/commandline/copy-and-paste-arrives-for-linuxwsl-consoles/\n\n\n\n\n\n\n\n\nDon’t panic when you lose control of your shell!\n\n\n\nIf a command seems to hang or get stuck, your terminal becomes unresponsive, or if you tried to print a very large text file to your screen, you can use CTRL+C to interrupt almost any operation and regain control.\nSimilarly, CTRL+D is an often used shortcut for exiting/logging out (e.g., when dealing with remote servers of nested shells).\nIn some cases, like when using an interactive terminal program such as the text editors nano and vim or a text viewer like less, you will only be able to exit them using that particular program’s shortcut keys (CTRL+X, : followed by q and enter, and Q, for these applications respectively). For more info on terminal programs, check out Fantastic terminal programs and how to quit them.\n\n\n\n\n\n\n\n\nWatch out…\n\n\n\nBe careful while learning your way around the command-line. The Unix shell will do exactly what you tell it to, often without hesitation or asking for confirmation. This means that you might accidentally move, overwrite or delete files without intending to do so. For example, when creating, copying or moving files, they can overwrite existing ones if you give them the same name. Similarly, when a file is deleted, it will be removed completely, without first passing by a recycle bin.\nNo matter how much experience you have, it is a good idea to remain cautious when performing these types of operations.\nFor the purposes of learning, if you are using your own device instead of a cloud environment, we recommend that you work in a dedicated playground directory or even create a new user profile to be extra safe. And like always, backups of your important files are invaluable regardless of what you are doing.\n\n\n\n\n\n\n\n\nGoogle, -h/--help and comments are your friends.\n\n\n\nAt the beginning things will be awkward, so don’t worry about having to search for the same information multiple times. That is all part of the learning process. Moreover, being able to retrieve information when you are in need of a particular command is more useful than memorizing everything.\nIt can still be a good idea though to keep a list of commands that you often use, but have a difficult time committing to memory.\nMany commands will also display a short help text when called with the -h/--help flag. For some tools, you will need to call them without any arguments to display the help. Lastly, for some tools, you can use the man &lt;tool_name&gt; to open up an even more in-depth manual.\n\n\n\n\n\n\n\n\nMake your scripts easier to read by using comments and breaking up long lines\n\n\n\nRemember that you can always write comments inside of your scripts by starting a line with #. That way you can add a short explainer or extra info to the different sections of a script. Code that seems clear while you are writing it, has the unfortunate tendency of becoming much more confusing when you refer back to it at a later time.\n#!/usr/bin/env bash\n\n##########################################################\n# Script to map fastq files to reference genome with bwa #\n##########################################################\n\n# make sure to run this script from within the directory where it is stored!\n\n# move to the directory containing the fastq files\ncd ../data/fastq\n\n# create a directory to store the results and store the path as a variable\noutput_dir=\"../../results/bwa\"\nmkdir -p ${output_dir}\n\n...\nAdditionally, you can break up long commands using a \\ to make them easier to read. You can do this both in scripts or on the command line. E.g.,\nbwa mem \\\n  ../reference/PlasmoDB-65_Pfalciparum3D7_Genome.fasta \\\n  ${read_1} \\\n  ${sample_name}_R2_001.fastq.gz\n\n\n\n\n\n\n\n\nhttps://explainshell.com/ & https://tldr.inbrowser.app\n\n\n\nThe first website is tremendously useful for figuring out what a command and all of its options mean. Whereas the second shows you a quick summary of the most command usages of a particular command.\nUse both of these to your advantage! But do not forget that most commands also have a built-in help page that can be accessed using the --help flag (in some cases just typing the command without any arguments also shows some help information).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Various Unix topics</span>"
    ]
  },
  {
    "objectID": "content/unix/appendix-unix.html#sec-table-special-syntax",
    "href": "content/unix/appendix-unix.html#sec-table-special-syntax",
    "title": "Appendix A — Various Unix topics",
    "section": "A.2 Overview of special syntax",
    "text": "A.2 Overview of special syntax\nThe table below gives you an overview of some of the special characters that we will encounter. You do not need to memorize them, but you can always refer back to this section if you see a symbol later on and are not quite sure what its purpose is.\n\n\n\n\n\n\n\n\nSymbol\nName\nUses\n\n\n\n\n/\nForward slash\nFile path separator or root location/file path\n\n\n\\\nBack slash\nSplit long command to a new line and escape special characters (+ file path separator in Windows)\n\n\n~\nTilde\nShortcut for home directory in file paths\n\n\n|\nPipe or vertical bar\nChains the output of one command to the input of another one (piping)\n\n\n#\nHash\nPart of the shebang at the top of scripts #! and used for comments in shell scripts\n\n\n$\nDollar sign\nUsed to access variables in bash\n\n\n*\nAsterisk or wildcard\nGlobbing operator\n\n\n&gt;\nGreater than symbol\nRedirect output of a command (&gt;&gt; redirect and append instead of overwriting)\n\n\n&lt;\nLess than symbol\nRedirect input to a command\n\n\n.\nDot\nIn the context of a path, it represents the current working directory\n\n\n..\nDouble dot\nIn the context of a path, it represents the parent directory of the working directory",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Various Unix topics</span>"
    ]
  },
  {
    "objectID": "content/unix/appendix-unix.html#sec-unix-sudo",
    "href": "content/unix/appendix-unix.html#sec-unix-sudo",
    "title": "Appendix A — Various Unix topics",
    "section": "A.3 Understanding superusers, root and sudo",
    "text": "A.3 Understanding superusers, root and sudo\nSuperusers are special users on Unix systems with additional privileges (cf. Windows administrator). By convention, the default name for a superuser on Linux is root (don’t confuse this with the root of the filesystem /). Superusers have access to all files and directories on the file system, including any critical components that make the system work and any files owned by other users. Moreover, several tasks like software installation and modifying system configuration require administrative privileges. It would be a security risk to run as a superuser constantly, but how can regular users install software then? Or what about modifying the permissions of a file that you (accidentally) removed your own access to? This is where the sudo command comes into play.\nsudo stands for “superuser do” and it allows regular users, who have been added to the sudo user group, to run individual commands as if they were the root user. A common use-case is using sudo to install new software via apt (on Debian-like systems) or dnf (on Fedora/CentOS): sudo apt install ncbi-blast+.\nWith great power comes great responsibility, so exercise caution and think twice before running a command with sudo. Only do so when you are sure you know what the end result will be.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Various Unix topics</span>"
    ]
  },
  {
    "objectID": "content/unix/appendix-unix.html#sec-unix-path",
    "href": "content/unix/appendix-unix.html#sec-unix-path",
    "title": "Appendix A — Various Unix topics",
    "section": "A.4 What is $PATH?",
    "text": "A.4 What is $PATH?\nThe $PATH is a way of letting your computer know where specific tools or other special locations are stored on your file system. Unless you tell it explicitly, it won’t know where to find any new software you install. Fortunately, most methods of installing software automatically take care of this for you, but every now and then you will need to manually add things to your $PATH. If you don’t, you will be greeted by messages like Command 'python' not found, did you mean:.\nThe $PATH is nothing more than a list of locations on your computer. Everything that is found in those locations, will become available to use directly on the CLI without having to type out its full location. Even the basic Unix commands, like ls and cd are only known to your shell because they are in a location that is indexed by your path.\nIn the following example, we will demonstrate how you can add a custom directory with scripts to your $PATH, making them callable from anywhere.\n# show the contents of PATH\necho $PATH\n/home/pmoris/miniforge3/bin:/home/pmoris/miniforge3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Program Files (x86)/Common Files/Oracle/Java/javapath:/mnt/c/windows/system32:/mnt/c/windows:/mnt/c/windows/System32/Wbem:/mnt/c/windows/System32/WindowsPowerShell/v1.0:/mnt/c/windows/System32/OpenSSH:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/dotnet:/mnt/c/Users/pmoris/AppData/Local/Programs/Quarto/bin:/mnt/c/Users/pmoris/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/pmoris/AppData/Local/Programs/Microsoft VS Code/bin:/snap/bin\n\n# temporarily add directory of scripts to PATH\n$ ls ~/itg/FiMAB-bioinformatics/training/scripts\ncall_variants.sh  download_reference.sh  map.sh  remove_dups.sh  trim.sh\n$ export PATH=\"$PATH:~/itg/FiMAB-bioinformatics/training/scripts\"\n\n# now these scripts can be invoked directly without having to type out their full location\n# e.g., map.sh works just as well as ~/itg/FiMAB-bioinformatics/training/scripts/map.sh\nTo make these changes permanent, you’d have to add that export statement to your .bashrc file (stored in your home directory). This file is run every time you launch a new shell, so that will allow the $PATH to be modified every time during startup.\nYou can find more information on modifying the PATH here.\nLastly, be careful when modifying your PATH. If you mess it up, it can cause all kinds of havoc.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Various Unix topics</span>"
    ]
  },
  {
    "objectID": "content/unix/appendix-unix.html#sec-permissions",
    "href": "content/unix/appendix-unix.html#sec-permissions",
    "title": "Appendix A — Various Unix topics",
    "section": "A.5 Dealing with file permissions",
    "text": "A.5 Dealing with file permissions\nYou can find an excellent explanation on file permissions here.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Various Unix topics</span>"
    ]
  },
  {
    "objectID": "content/unix/appendix-unix.html#sec-ssh",
    "href": "content/unix/appendix-unix.html#sec-ssh",
    "title": "Appendix A — Various Unix topics",
    "section": "A.6 Working with remote machines via SSH",
    "text": "A.6 Working with remote machines via SSH\nIn some cases, you will need to work on a Linux machine that is physically located somewhere else, i.e. a remote server. Access to these is usually managed via a command-line tool called SSH (or a stand-alone GUI tool like Putty in Windows). The syntax of the ssh command is as follows:\nssh username@domain\nWhere username is a name given to you by the admin of the system and domain is the address of the server (can be a URL or an IP address).\nThe connection is secured via SSH keys: a pair of files used for authentication stored in ~/.ssh.\n\nPublic key: e.g., id_rsa.pub or id_ed25519.pub, located on the remote server.\nPrivate file: e.g., id_rsa or id_ed25519.pub, located on your own machine. *Never share this file with anyone else!**\n\nInstructions to generate new SSH keys can be found https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent.\nA common problem when connecting is that the file permissions of your keys or credential files are messed up. This can happen if you generate them in Windows and later move them to a Linux file system. To fix this, check https://superuser.com/a/215506.\nMore info on remote servers can be found here.\nLastly, keep in mind that when working on remote servers, it is essential to use screen or tmux for long-running jobs. Otherwise, they will be interrupted when you disconnect or even when the network briefly fails.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Various Unix topics</span>"
    ]
  },
  {
    "objectID": "content/unix/appendix-unix.html#further-reading",
    "href": "content/unix/appendix-unix.html#further-reading",
    "title": "Appendix A — Various Unix topics",
    "section": "A.7 Further reading",
    "text": "A.7 Further reading\nThere are still many facets of Unix that we have not covered here yet, but we hope that you can pick these up on your own if were to ever need them. For now though, the basics that we covered here, will hopefully already go a long way in helping you use Unix, read scripts written by others and teaching yourself new concepts.\nA selection of more advanced topics to explore at a later time could be:\n\nAn superb workshop on various intermediate concepts: https://genomicsaotearoa.github.io/shell-for-bioinformatics/\nInstalling software on Linux machines, from source or via apt (Ubuntu), dnf (Fedora) or pacman (Arch), or through tools like conda/Miniforge and bioconda.\nThe find command: useful for finding files in your file system.\nIf/else conditions: allow you to execute parts of scripts if certain conditions are fulfilled.\nCommand substitution and process substitution: run commands in a subshell to allow more complex ways of redirecting and piping outputs/inputs.\nsed/awk are commands that let you do things like search and replace, calculations or other manipulations based on all the lines of a file.\nthe join command: combining columns of (multiple) tabular data files in particular ways.\nEnvironment variables: variables that are always available, like PATH.\nUsing screen or tmux to spawn persistent background terminal sessions. This allows you to run commands on a remote server and shutdown your own machine, without the remote process being interrupted.\nLearn about the difference in line endings on Unix (\\n) and Windows (\\r\\n), how to switch between them (dos2unix) and how to set a preference in your editors like RStudio.\nLearn about sudo, allowing you to perform actions as an administrator.\nRegular expressions, to power up your grep searches and sed/awk commands.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Various Unix topics</span>"
    ]
  }
]